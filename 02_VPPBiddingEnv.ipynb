{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a32f4a0-96ea-48fe-a8ac-fa6adbef7f3e",
   "metadata": {},
   "source": [
    "# Run Agent in Environment with different Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37105a1-c06c-445d-b9f9-afc70b8764e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data \n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import numpy as np\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# Algorithms \n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import TD3\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from rl_zoo3 import linear_schedule\n",
    "\n",
    "from gym import make\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "\n",
    "import subprocess\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "from vpp_gym.vpp_gym.utils.register_env import register_env\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f10a6-c266-47b0-80cc-53cbbf95fe0e",
   "metadata": {},
   "source": [
    "## Seed Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c3eda0-ce42-4c02-b13a-f4aefdebead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEED = 44\n",
    "\n",
    "#torch.manual_seed(SEED)\n",
    "#torch.cuda.manual_seed(SEED)\n",
    "#torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db6c67-ec11-492b-b29d-773764e7dc09",
   "metadata": {},
   "source": [
    "## Register the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de7d899-f638-44cf-8c5b-3f80b0723019",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_NAME = \"vpp_config_1_training.json\"\n",
    "\n",
    "#register_env(config=CONFIG_NAME, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17256e7-0240-435d-a641-55d444a43406",
   "metadata": {},
   "source": [
    "## Test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72671bc0-33dd-4453-81b9-d86c8979a5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It will check your custom environment and output additional warnings if needed\n",
    "#env_to_check = make('VPPBiddingEnv-TEST-v1', render_mode=None)\n",
    "#check_env(env_to_check)\n",
    "#env_to_check.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b9778-4b51-48b3-9b93-8fee865a49ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff546b2-3a85-4377-bc6d-75d3d60b41aa",
   "metadata": {},
   "source": [
    "### Offline Training and later sync logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5787c994-9554-4c38-94df-ee1adafe2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"0cea1eee5f42654eca0de365f0acca116367c9b4\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc579db-a0a3-474c-bc1b-d139ac256d18",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1fcf70-09cd-497e-9d99-f04cbe27ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_algorithm(algorithm, seed): \n",
    "    env = make('VPPBiddingEnv-TRAIN-FAST-v1', render_mode=\"fast_training\")\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env)\n",
    "    \n",
    "    if algorithm == \"R_PPO\": \n",
    "        policy = 'MultiInputLstmPolicy'\n",
    "    else: \n",
    "        policy = 'MultiInputPolicy'\n",
    "\n",
    "    wandb.init(\n",
    "        sync_tensorboard=True, \n",
    "        project=\"RL-VPP-Training\",\n",
    "        save_code=True,\n",
    "        entity=\"jlu237\", \n",
    "        tags=[algorithm] + EXPERIMENT_TAGS, \n",
    "        job_type=\"training\"\n",
    "    )\n",
    "    \n",
    "    model_params = HYPERPARAMS[algorithm]\n",
    "    model = ALGORITHMS[algorithm](policy, env, verbose=0, seed=seed, **model_params)\n",
    "    model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                log_interval=1,\n",
    "                progress_bar = True,\n",
    "                callback=WandbCallback(\n",
    "                    gradient_save_freq=1,\n",
    "                    verbose=0))\n",
    "    wandb.finish()\n",
    "    return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "\n",
    "    return return_code, model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b85f6d-5c58-48e7-a799-cebc8f392eca",
   "metadata": {},
   "source": [
    "## Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3282c7-e8c2-4a16-a327-c6e3a7da08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_algorithm(algorithm, model):\n",
    "    eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "    eval_env = Monitor(eval_env) \n",
    "    eval_env = RecordEpisodeStatistics(eval_env) \n",
    "\n",
    "    wandb.init(\n",
    "        sync_tensorboard=True, \n",
    "        project=\"RL-VPP-Evaluation\",\n",
    "        save_code=True,\n",
    "        entity=\"jlu237\", \n",
    "        tags=[algorithm] + EXPERIMENT_TAGS, \n",
    "        job_type=\"eval\",\n",
    "        config={\"algo\": algorithm}\n",
    "    )\n",
    "\n",
    "    episodes = 70\n",
    "    for i_episode in range(episodes):\n",
    "        observation = eval_env.reset()\n",
    "        if algorithm == \"R_PPO\":\n",
    "            lstm_states = None\n",
    "            num_envs = 1\n",
    "            # Episode start signals are used to reset the lstm states\n",
    "            episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "            for t in range(1):\n",
    "                action, lstm_states = model.predict(observation, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "                observation, reward, dones, info = eval_env.step(action)\n",
    "                episode_starts = dones\n",
    "        else: \n",
    "            for t in range(1):\n",
    "                action, _states = model.predict(observation, deterministic = True)\n",
    "                observation, reward, done, info = eval_env.step(action)\n",
    "\n",
    "    eval_env.close()\n",
    "    wandb.finish()\n",
    "    return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "    return return_code\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07c2f8b5-4790-4032-ab9e-6d9faaab712c",
   "metadata": {},
   "source": [
    "\n",
    "HYPERPARAMS = {\n",
    "    \n",
    "    \"A2C\": {'n_steps': 5,\n",
    "            'gamma': 0.9999,\n",
    "            'gae_lambda': 0.8,\n",
    "            'learning_rate': 0.00002253262628450229,\n",
    "            'ent_coef': 0.0001,\n",
    "            'normalize_advantage': True,\n",
    "            'max_grad_norm': 3,\n",
    "            'use_rms_prop': False,\n",
    "            'use_sde': True,\n",
    "            'vf_coef': 0.9623826424096591,\n",
    "            'policy_kwargs': {'net_arch': [{'pi': [400, 400], 'vf': [400, 400]}],\n",
    "                              'full_std': True,\n",
    "                              'activation_fn': nn.ReLU,\n",
    "                              'sde_net_arch': [64],\n",
    "                              'ortho_init': False,\n",
    "                              'log_std_init': -2.88684575170293},\n",
    "            'sde_sample_freq': 1},\n",
    "    \n",
    "    \"DDPG\": {'learning_rate': 0.00016202400413457016,\n",
    "             'buffer_size': 100000,\n",
    "             'learning_starts': 200,\n",
    "             'batch_size': 200,\n",
    "             'tau': 0.01,\n",
    "             'gamma': 0.999,\n",
    "             'train_freq': 1,\n",
    "             'gradient_steps': 8,\n",
    "             'policy_kwargs': {'net_arch': [400, 300],\n",
    "                               'activation_fn': nn.Tanh,\n",
    "                               'n_critics': 1},\n",
    "             'action_noise': OrnsteinUhlenbeckActionNoise(mean=np.array([0.0] * 12), sigma=np.array([0.02549087]*12))},\n",
    "    \n",
    "    \"SAC\": {'learning_rate': 0.0004340027182463646,\n",
    "            'batch_size': 16,\n",
    "            'buffer_size': 1000000,\n",
    "            'learning_starts': 0,\n",
    "            'train_freq': 16,\n",
    "            'gradient_steps': 32,\n",
    "            'ent_coef': 'auto_0.1',\n",
    "            'tau': 0.05,\n",
    "            'gamma': 0.99,\n",
    "            'policy_kwargs': {'net_arch': [400, 300],\n",
    "                              'activation_fn': nn.ReLU,\n",
    "                              'log_std_init': 0.24980902087899004,\n",
    "                              'use_sde': False},\n",
    "            'sde_sample_freq': 2,\n",
    "            'target_entropy': -10},\n",
    "    \n",
    "    \"PPO\": {'learning_rate': 0.00001637489428901879,\n",
    "            'n_steps': 3,\n",
    "            'batch_size': 3,\n",
    "            'n_epochs': 10,\n",
    "            'gamma': 0.99,\n",
    "            'gae_lambda': 0.99,\n",
    "            'clip_range': 0.2,\n",
    "            'normalize_advantage': False,\n",
    "            'ent_coef': 0.0000016283826579654475,\n",
    "            'vf_coef': 0.5370544952687742,\n",
    "            'max_grad_norm': 2,\n",
    "            'use_sde': True,\n",
    "            'target_kl': 0.001,\n",
    "            'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}],\n",
    "                              'ortho_init': False,\n",
    "                              'activation_fn': nn.ELU,\n",
    "                              'log_std_init': -3.0232833854071797,\n",
    "                              'full_std': False},\n",
    "            'sde_sample_freq': 3},\n",
    "    \n",
    "    \"TD3\": {'learning_rate': 0.0002925979906993544,\n",
    "            'batch_size': 64,\n",
    "            'buffer_size': 100000,\n",
    "            'tau': 0.05,\n",
    "            'gamma': 0.99,\n",
    "            'train_freq': 2,\n",
    "            'gradient_steps': 8,\n",
    "            'learning_starts': 0,\n",
    "            'policy_delay': 5,\n",
    "            'target_policy_noise': 0.1,\n",
    "            'policy_kwargs': {'net_arch': [64, 64],\n",
    "                              'activation_fn': nn.ELU},\n",
    "            'action_noise': NormalActionNoise(mean=np.array([0.0] * 12), sigma=np.array([0.85406018]*12))},\n",
    "    \n",
    "    \"TQC\": {'learning_rate': 0.000012766113197846353,\n",
    "            'batch_size': 100,\n",
    "            'buffer_size': 10000,\n",
    "            'learning_starts': 10,\n",
    "            'train_freq': 2,\n",
    "            'gradient_steps': 1,\n",
    "            'ent_coef': 0.0001,\n",
    "            'tau': 0.005,\n",
    "            'gamma': 0.95,\n",
    "            'policy_kwargs': {'net_arch': [400, 300],\n",
    "                              'activation_fn': nn.ReLU,\n",
    "                              'n_quantiles': 31,\n",
    "                              'use_sde': False},\n",
    "            'target_entropy': 1,\n",
    "            'action_noise': OrnsteinUhlenbeckActionNoise(mean=np.array([0.0] * 12), sigma=np.array([0.4542673]*12)),\n",
    "            'top_quantiles_to_drop_per_net': 26},\n",
    "    \n",
    "    \"TRPO\": {'learning_rate':linear_schedule(0.00015090267665771766),\n",
    "             'n_steps': 4,\n",
    "             'batch_size': 4,\n",
    "             'gamma': 0.9999,\n",
    "             'cg_max_steps': 10,\n",
    "             'cg_damping': 0.2,\n",
    "             'line_search_shrinking_factor': 0.8,\n",
    "             'line_search_max_iter': 10,\n",
    "             'n_critic_updates': 5,\n",
    "             'gae_lambda': 0.99,\n",
    "             'normalize_advantage': True,\n",
    "             'use_sde': False,\n",
    "             'target_kl': 0.001,\n",
    "             'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}],\n",
    "                               'ortho_init': False,\n",
    "                               'activation_fn': nn.LeakyReLU}},\n",
    "    \n",
    "    \"R_PPO\": {'learning_rate': 0.00008816185810075235,\n",
    "              'n_steps': 9, \n",
    "              'batch_size': 9,\n",
    "              'n_epochs': 5,\n",
    "              'gamma': 0.999,\n",
    "              'gae_lambda': 0.8,\n",
    "              'clip_range': 0.3,\n",
    "              'normalize_advantage': True,\n",
    "              'ent_coef': 0.0000011091115338848158,\n",
    "              'vf_coef': 0.1367524569844577,\n",
    "              'max_grad_norm': 0.3,\n",
    "              'target_kl': 0.005, \n",
    "              'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}],\n",
    "                                'full_std': True,\n",
    "                                'activation_fn': nn.ELU,\n",
    "                                'ortho_init': False,\n",
    "                                'log_std_init': -1.7739650671729592},\n",
    "              'sde_sample_freq': 3}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26af9ef6-e6b1-46b6-9d63-66025b448e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TD 3 Alternative 1\n",
    "\n",
    "HYPERPARAMS = {\n",
    "\n",
    "        \"TD3\": {'learning_rate': 0.0001811374579441975,\n",
    "                'batch_size': 100,\n",
    "                'buffer_size': 1000000,\n",
    "                'tau': 0.08,\n",
    "                'gamma': 0.99,\n",
    "                'train_freq': 2,\n",
    "                'gradient_steps': 1,\n",
    "                'learning_starts': 10,\n",
    "                'policy_delay': 1,\n",
    "                'target_policy_noise': 0.3,\n",
    "                'policy_kwargs': {'net_arch': [256, 256],\n",
    "                                  'activation_fn': nn.ReLU}},\n",
    "\n",
    "\n",
    "# TRPO Alternative 1\n",
    "\n",
    "    \"TRPO\":{'learning_rate': linear_schedule(0.000017406994884083613),\n",
    "             'n_steps': 10,\n",
    "             'batch_size': 10,\n",
    "             'gamma': 0.9999,\n",
    "             'cg_max_steps': 20,\n",
    "             'cg_damping': 0.5,\n",
    "             'line_search_shrinking_factor': 0.7,\n",
    "             'line_search_max_iter': 5,\n",
    "             'n_critic_updates': 1,\n",
    "             'gae_lambda': 0.9,\n",
    "             'normalize_advantage': False,\n",
    "             'use_sde': False,\n",
    "             'target_kl': 0.02,\n",
    "             'policy_kwargs': {'net_arch': [{'pi': [400, 400], 'vf': [400, 400]}],\n",
    "                               'ortho_init': False,\n",
    "                               'activation_fn': nn.ReLU}},\n",
    "\n",
    "\n",
    "# RPPO Alternative 1 \n",
    "\n",
    "    \"R_PPO\":{'learning_rate': linear_schedule(0.002677762425612007),\n",
    "             'n_steps': 9,\n",
    "             'batch_size': 9,\n",
    "             'n_epochs': 10,\n",
    "             'gamma': 0.99,\n",
    "             'gae_lambda': 0.9,\n",
    "             'clip_range': 0.4,\n",
    "             'normalize_advantage': True,\n",
    "             'ent_coef': 0.000002637536156684848,\n",
    "             'vf_coef': 0.5221493086134156,\n",
    "             'max_grad_norm': 0.6,\n",
    "             'target_kl': 0.001,\n",
    "             'policy_kwargs': {'net_arch': [{'pi': [64, 64], 'vf': [64, 64]}],\n",
    "                               'full_std': True,\n",
    "                               'activation_fn': nn.Tanh,\n",
    "                               'ortho_init': True,\n",
    "                               'log_std_init': -2.3151490699717723},\n",
    "             'sde_sample_freq': 3}\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99483e8f-fb49-4efe-b58f-4f68e9b9b808",
   "metadata": {},
   "source": [
    "# TD 3 Alternative 2\n",
    "\n",
    "        \"TD3\": {'learning_rate': 0.0007732991861165822,\n",
    "                'batch_size': 200,\n",
    "                'buffer_size': 100000,\n",
    "                'tau': 0.05,\n",
    "                'gamma': 0.99,\n",
    "                'train_freq': 1,\n",
    "                'gradient_steps': -1,\n",
    "                'learning_starts': 100,\n",
    "                'policy_delay': 2,\n",
    "                'target_policy_noise': 0.1,\n",
    "                'policy_kwargs': {'net_arch': [64, 64],\n",
    "                                  'activation_fn': nn.ELU},\n",
    "                'action_noise': OrnsteinUhlenbeckActionNoise(mean=np.array([0.0] * 12), sigma=np.array([0.79326878]*12))},\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc088d8b-c4fe-42a0-b2a5-ab31fa51c43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALGORITHMS = {\n",
    "    \"A2C\": A2C,\n",
    "    \"DDPG\": DDPG,\n",
    "    \"PPO\": PPO,\n",
    "    \"SAC\": SAC,\n",
    "    \"TD3\": TD3,\n",
    "    \"TQC\": TQC,\n",
    "    \"TRPO\": TRPO,\n",
    "    \"R_PPO\": RecurrentPPO,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f4386e0-1c92-4777-b298-3e1e35b01648",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_TAGS = [\"config1_training\", \n",
    "                   \"11140\",\n",
    "                  ]\n",
    "\n",
    "EXPERIMENT_TIMESTEPS = 11140  #2785 #5570  #11140 #22280\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da23b29c-4008-4b4a-9405-679982f2bb93",
   "metadata": {},
   "source": [
    "## Train all Algorithms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee2bd362-e1c6-4f94-a57d-88e96933fa2a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#algorithm_list = [\"R_PPO\", \"TRPO\", \"PPO\", \"A2C\", \"DDPG\", \"TD3\", \"TQC\", \"SAC\"]\n",
    "\n",
    "#algorithm_list = [\"R_PPO\", \"TRPO\", \"TD3\"]\n",
    "\n",
    "algorithm_list = [\"R_PPO\"]\n",
    "\n",
    "for algorithm in algorithm_list:\n",
    "    for seed in [44, 45, 46, 47, 48, 49, 50, 51, 52, 53]: \n",
    "        EXPERIMENT_TAGS = [\"config1_training\",\n",
    "                           (str(EXPERIMENT_TIMESTEPS) + \" ep.\"),\n",
    "                           (\"S\" + str(seed))\n",
    "                        ]\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        register_env(config=CONFIG_NAME, seed=seed)\n",
    "        \n",
    "        try:\n",
    "            print(\"now training \" + algorithm)\n",
    "            return_code, model = train_algorithm(algorithm, seed) \n",
    "            print(\"training finished with : \" + str(return_code))\n",
    "            return_code = evaluate_algorithm(algorithm, model)\n",
    "            print(\"evaluation finished with : \" + str(return_code))        \n",
    "        except:\n",
    "            print(\"error occurred, next algo\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b318ebe-e815-47f8-908c-17182fab63da",
   "metadata": {},
   "source": [
    "# run test set \n",
    "\n",
    "return_code = evaluate_algorithm(algorithm, model)\n",
    "print(\"evaluation finished with : \" + str(return_code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0771ced6-2c02-4761-ba5b-8686e2dd6617",
   "metadata": {},
   "source": [
    "# upload wandb logs \n",
    "\n",
    "!wandb sync wandb/latest-run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be238c86-e64c-4bc6-996d-d62fd1c08ea2",
   "metadata": {},
   "source": [
    "## Single Algorithm Training (FINAL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d67e528c-9f22-4760-92f4-1ee19a503c2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# seed = [44, 45, 46, 47, 48, 49, 50, 51, 52, 53]: \n",
    "# open = [44, 45, 46, 47, 48, 49, 50, 51, 52, 53]: \n",
    "# done = \n",
    "\n",
    "#algorithm_list = [\"R_PPO\", \"TRPO\", \"PPO\", \"A2C\", \"DDPG\", \"TD3\", \"TQC\", \"SAC\"]\n",
    "#open  = [\"R_PPO\", \"TRPO\", \"PPO\", \"A2C\", \"DDPG\", \"TD3\", \"TQC\", \"SAC\"]\n",
    "# done = \n",
    "# do again: \"R_PPO\", \"TRPO\", \"TD3\"\n",
    "\n",
    "seed = 46\n",
    "algorithm = \"R_PPO\"\n",
    "\n",
    "EXPERIMENT_TAGS = [\"config1_training\",\n",
    "                   (str(EXPERIMENT_TIMESTEPS) + \" ep.\"),\n",
    "                   (\"S\" + str(seed))\n",
    "                ]\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "register_env(config=CONFIG_NAME, seed=seed)\n",
    "\n",
    "print(\"now training \" + algorithm)\n",
    "return_code, model = train_algorithm(algorithm, seed) \n",
    "print(\"training finished with : \" + str(return_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc135d30-71db-40de-b70c-a886913614e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload wandb logs \n",
    "\n",
    "#!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a694a995-fad5-4d06-990b-a5f0d1ed5168",
   "metadata": {},
   "source": [
    "## Single Algorithm Eval (FINAL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f711641-cfcb-4ce5-9479-6f964fa07176",
   "metadata": {
    "tags": []
   },
   "source": [
    "return_code = evaluate_algorithm(algorithm, model)\n",
    "print(\"evaluation finished with : \" + str(return_code))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbe3f1d8-8ae1-4dd8-a9d3-0720c1393617",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "#algorithm_list = [\"R_PPO\", \"TRPO\", \"PPO\", \"A2C\", \"DDPG\", \"TD3\", \"TQC\", \"SAC\"]\n",
    "\n",
    "#algorithm_list = [\"R_PPO\", \"TRPO\", \"TD3\"]\n",
    "\n",
    "algorithm_list = [ \"SAC\"]\n",
    "\n",
    "for algorithm in algorithm_list:\n",
    "    if algorithm == \"SAC\": \n",
    "        seed_list = [48, 49, 50, 51, 52, 53]\n",
    "    else: \n",
    "        seed_list = [44, 45, 46, 47, 48, 49, 50, 51, 52, 53]\n",
    "    for seed in seed_list: \n",
    "        EXPERIMENT_TAGS = [\"config1_training\",\n",
    "                           (str(EXPERIMENT_TIMESTEPS) + \" ep.\"),\n",
    "                           (\"S\" + str(seed))\n",
    "                        ]\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        register_env(config=CONFIG_NAME, seed=seed)\n",
    "        \n",
    "        try:\n",
    "            print(\"now training \" + algorithm)\n",
    "            return_code, model = train_algorithm(algorithm, seed) \n",
    "            print(\"training finished with : \" + str(return_code))\n",
    "            return_code = evaluate_algorithm(algorithm, model)\n",
    "            print(\"evaluation finished with : \" + str(return_code))        \n",
    "        except:\n",
    "            print(\"error occurred, next algo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc34a3c-438e-4efb-ac7a-653482b1affe",
   "metadata": {},
   "source": [
    "## Single Algorithm Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ea1de1-b50d-44d7-92fa-1fc6fc15a4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now training R_PPO\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365f14f4b60545e98307cf9086a6aeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>step_activ_count</td><td>▁▁▂▁▇▁▆▆▁▃▁▁▂▆▇█▅▇▃▂▅▂▃▆▂▃▆▆▁▂▃▂▂▇▇▇▇▇▁▇</td></tr><tr><td>step_activ_ratio</td><td>▁▁█▁█▁██▁█▁▁████████████████▁█████████▁█</td></tr><tr><td>step_lost_count</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_not_activ_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_not_part_count</td><td>▁▁▁▁▁▁▁▁▁▅▅▁▁▁▁▁▅▅▅▅█▅▅▅█▅██▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>step_not_res_count</td><td>▁▇▇█▂█▃▃█▅▇█▇▃▂▁▃▁▅▆▂▆▅▂▅▅▁▁▇▆▅▆▆▁▁▁▁▁▇▁</td></tr><tr><td>step_penalties</td><td>█▅▆▆▆▃▄▇▁▆▃▇▆▆████▇▆█▇███▇██▆█▇█████████</td></tr><tr><td>step_profit</td><td>▁▂▁▁▅▁▂▂▃▁▁▁▁█▃▆▁▇▁▁▁▁▂▂▇▂▆▂▁▁▃▁▁▄▂▃▁▃▁▃</td></tr><tr><td>step_res_count</td><td>▁▁▂▁▇▁▆▆▁▃▁▁▂▆▇█▅▇▃▂▅▂▃▆▂▃▆▆▁▂▃▂▂▇▇▇▇▇▁▇</td></tr><tr><td>step_res_ratio</td><td>▁▁▂▁▇▁▆▆▁▄▁▁▂▆▇█▅█▄▂▆▂▄▇▃▄██▁▂▄▂▂█████▁█</td></tr><tr><td>step_revenue</td><td>▁▃▂▃▅▅▃▂█▃▄▃▂█▂▃▁▄▂▃▁▂▂▂▅▄▄▂▃▁▃▂▂▃▁▂▁▂▁▂</td></tr><tr><td>step_reward</td><td>▁▃▄▄▇▃▆▇▄▄▃▄▄▇▇█▅▇▅▄▅▄▅▆▄▅▅▅▄▄▅▄▄▇▇▇▇▇▄▇</td></tr><tr><td>step_won_count</td><td>▁▇███████▇▇█████▇▇▇▇▆▇▇▇▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>step_won_ratio</td><td>▁▇███████▇▇█████▇▇▇▇▆▇▇▇▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>total_activ_count</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>total_lost_count</td><td>▁▆██████████████████████████████████████</td></tr><tr><td>total_not_activ_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_not_part_count</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>total_not_res_count</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>total_penalties</td><td>██▇▇▇▆▆▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_profit</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>total_res_count</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>total_revenue</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>total_reward</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>total_won_count</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>11141</td></tr><tr><td>step_activ_count</td><td>5</td></tr><tr><td>step_activ_ratio</td><td>1.0</td></tr><tr><td>step_lost_count</td><td>0</td></tr><tr><td>step_not_activ_count</td><td>0</td></tr><tr><td>step_not_part_count</td><td>1</td></tr><tr><td>step_not_res_count</td><td>0</td></tr><tr><td>step_penalties</td><td>0.0</td></tr><tr><td>step_profit</td><td>1037.41</td></tr><tr><td>step_res_count</td><td>5</td></tr><tr><td>step_res_ratio</td><td>1.0</td></tr><tr><td>step_revenue</td><td>1037.41</td></tr><tr><td>step_reward</td><td>0.83548</td></tr><tr><td>step_won_count</td><td>5</td></tr><tr><td>step_won_ratio</td><td>0.83333</td></tr><tr><td>total_activ_count</td><td>27461</td></tr><tr><td>total_lost_count</td><td>1456</td></tr><tr><td>total_not_activ_count</td><td>0</td></tr><tr><td>total_not_part_count</td><td>9692</td></tr><tr><td>total_not_res_count</td><td>28243</td></tr><tr><td>total_penalties</td><td>-30536659.65744</td></tr><tr><td>total_profit</td><td>4767308.3274</td></tr><tr><td>total_res_count</td><td>27461</td></tr><tr><td>total_revenue</td><td>12218985.91</td></tr><tr><td>total_reward</td><td>7226.40406</td></tr><tr><td>total_won_count</td><td>55704</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/offline-run-20221220_151738-exb0vmi2<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20221220_151738-exb0vmi2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/debug-cli.Jan-Lukas.Pflaum.log\n",
      "Syncing: https://wandb.ai/jlu237/RL-VPP-Training/runs/exb0vmi2 ... done.\n",
      "training finished with : CompletedProcess(args='wandb sync wandb/latest-run', returncode=0)\n"
     ]
    }
   ],
   "source": [
    "#algorithm_list = [\"R_PPO\", \"TRPO\", \"TD3\"]\n",
    "\n",
    "\n",
    "ALGORITHM = \"R_PPO\"\n",
    "seed = 44\n",
    "\n",
    "EXPERIMENT_TAGS = [\"config1_training\",\n",
    "                       (str(EXPERIMENT_TIMESTEPS) + \" ep.\"),\n",
    "                       (\"S\" + str(seed))\n",
    "                    ]\n",
    "\n",
    " \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "register_env(config=CONFIG_NAME, seed=seed)\n",
    "\n",
    "print(\"now training \" + ALGORITHM)\n",
    "return_code, model = train_algorithm(ALGORITHM, seed) \n",
    "print(\"training finished with : \" + str(return_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ceb116-2855-4038-82d7-e495aad110aa",
   "metadata": {},
   "source": [
    "## Single Algorithm Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71ad3931-7715-4cb2-b54c-8fcb5272b633",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>step_activ_count</td><td>▁▁▁█▁▁▂▇████████████▇▅▁▁▁▂▁▁▁█▁▁▁▇██████</td></tr><tr><td>step_activ_ratio</td><td>▁▁▁█▁▁████████████████▁▁▁█▁▁▁█▁▁▁███████</td></tr><tr><td>step_lost_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_not_activ_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_not_part_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_not_res_count</td><td>███▁██▇▂▁▁▁▁▁▁▁▁▁▁▁▁▂▄███▇███▁███▂▁▁▁▁▁▁</td></tr><tr><td>step_penalties</td><td>▆▆██▇▆▇███████████████▇███▄▅▇█▅▁▄███████</td></tr><tr><td>step_profit</td><td>▁▁▂▃▂▁▂▄▂▃▃▃▂▂▂▂▂▃▄▅▇▆▁▄▃▄▁▂▄▆▃▁▁█▇▇▄▇▅▄</td></tr><tr><td>step_res_count</td><td>▁▁▁█▁▁▂▇████████████▇▅▁▁▁▂▁▁▁█▁▁▁▇██████</td></tr><tr><td>step_res_ratio</td><td>▁▁▁█▁▁▂▇████████████▇▅▁▁▁▂▁▁▁█▁▁▁▇██████</td></tr><tr><td>step_revenue</td><td>▁▁▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▂▄▅▄▃▃▃▃▃▃▄▄▅▆█▆▅▅▂▅▃▃</td></tr><tr><td>step_reward</td><td>▁▁▂█▁▁▃▆████████████▇▆▂▁▂▃▁▁▂█▁▁▁▆██████</td></tr><tr><td>step_won_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step_won_ratio</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_activ_count</td><td>▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>total_lost_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_not_activ_count</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_not_part_count</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>total_not_res_count</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▅▅▆▆▆▆▇▇████████</td></tr><tr><td>total_penalties</td><td>██████████████████████▇▇▇▇▇▇▇▇▆▆▁▁▁▁▁▁▁▁</td></tr><tr><td>total_profit</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇██</td></tr><tr><td>total_res_count</td><td>▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>total_revenue</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▇▇▇▇████</td></tr><tr><td>total_reward</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>total_won_count</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>69</td></tr><tr><td>step_activ_count</td><td>5</td></tr><tr><td>step_activ_ratio</td><td>1.0</td></tr><tr><td>step_lost_count</td><td>0</td></tr><tr><td>step_not_activ_count</td><td>0</td></tr><tr><td>step_not_part_count</td><td>1</td></tr><tr><td>step_not_res_count</td><td>0</td></tr><tr><td>step_penalties</td><td>0.0</td></tr><tr><td>step_profit</td><td>505.76</td></tr><tr><td>step_res_count</td><td>5</td></tr><tr><td>step_res_ratio</td><td>1.0</td></tr><tr><td>step_revenue</td><td>505.76</td></tr><tr><td>step_reward</td><td>0.84992</td></tr><tr><td>step_won_count</td><td>5</td></tr><tr><td>step_won_ratio</td><td>0.83333</td></tr><tr><td>total_activ_count</td><td>208</td></tr><tr><td>total_lost_count</td><td>0</td></tr><tr><td>total_not_activ_count</td><td>0</td></tr><tr><td>total_not_part_count</td><td>70</td></tr><tr><td>total_not_res_count</td><td>142</td></tr><tr><td>total_penalties</td><td>-75222.11445</td></tr><tr><td>total_profit</td><td>26358.00595</td></tr><tr><td>total_res_count</td><td>208</td></tr><tr><td>total_revenue</td><td>57144.28</td></tr><tr><td>total_reward</td><td>49.62974</td></tr><tr><td>total_won_count</td><td>350</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/offline-run-20221220_153348-38h8st8l<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20221220_153348-38h8st8l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/debug-cli.Jan-Lukas.Pflaum.log\n",
      "Syncing: https://wandb.ai/jlu237/RL-VPP-Evaluation/runs/38h8st8l ... done.\n",
      "evaluation finished with : CompletedProcess(args='wandb sync wandb/latest-run', returncode=0)\n"
     ]
    }
   ],
   "source": [
    "return_code = evaluate_algorithm(ALGORITHM, model)\n",
    "print(\"evaluation finished with : \" + str(return_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34849d-7a52-4e06-ac99-c204f1cccf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
