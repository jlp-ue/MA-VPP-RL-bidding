{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a32f4a0-96ea-48fe-a8ac-fa6adbef7f3e",
   "metadata": {},
   "source": [
    "# Run Agent in Environment with different Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b37105a1-c06c-445d-b9f9-afc70b8764e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Data \n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import numpy as np\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "from gym import make"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db6c67-ec11-492b-b29d-773764e7dc09",
   "metadata": {},
   "source": [
    "## Register the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de7d899-f638-44cf-8c5b-3f80b0723019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "   \n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TRAIN-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"training\",\n",
    "            'render_mode' :\"human\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-EVAL-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"eval\",\n",
    "            'render_mode' :\"human\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TEST-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"INFO\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"test\",\n",
    "            'render_mode' :\"human\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17256e7-0240-435d-a641-55d444a43406",
   "metadata": {},
   "source": [
    "## Test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72671bc0-33dd-4453-81b9-d86c8979a5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log_step: initial // slot: initial  log level = info\n",
      "log_step: initial // slot: initial  log level = warning\n",
      "log_step: 0 slot: None Bid Submission time (D-1) = 2020-07-02 05:00:00+00:00\n",
      "log_step: 0 slot: None Gate Closure time (D-1) = 2020-07-02 06:00:00+00:00\n",
      "log_step: 0 slot: None Historic Data Window: from 2020-07-01 05:00:00+00:00 to 2020-07-02 04:45:00+00:00 \n",
      "log_step: 0 slot: None Forecast Data Window: from 2020-07-02 22:00:00+00:00 to 2020-07-03 21:45:00+00:00 \n",
      "log_step: 0 slot: 0 Current Slot Time: (D) = 2020-07-02 22:00:00+00:00\n",
      "log_step: 0 slot: 0 agents_bid_size = 44\n",
      "log_step: 0 slot: 0 agents_bid_price = 637.98444\n",
      "log_step: 0 slot: 0 settlement_price_DE : 16.67\n",
      "log_step: 0 slot: 0 self.activation_results['slots_won'] = \n",
      "log_step: 0 slot: 0\n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 0 slot: 0      agents bid_size = \n",
      "log_step: 0 slot: 0\n",
      "size: \t44 \n",
      "size: \t108 \n",
      "size: \t36 \n",
      "size: \t112 \n",
      "size: \t64 \n",
      "size: \t31\n",
      "log_step: 0 slot: 0 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 0 slot: 0\n",
      "price: \t16.67 \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 0 slot: 1 Current Slot Time: (D) = 2020-07-03 02:00:00+00:00\n",
      "log_step: 0 slot: 1 agents_bid_size = 108\n",
      "log_step: 0 slot: 1 agents_bid_price = 1882.6656\n",
      "log_step: 0 slot: 1 settlement_price_DE : 19.06\n",
      "log_step: 0 slot: 1 self.activation_results['slots_won'] = \n",
      "log_step: 0 slot: 1\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 0 slot: 1      agents bid_size = \n",
      "log_step: 0 slot: 1\n",
      "size: \t44 \n",
      "size: \t108 \n",
      "size: \t36 \n",
      "size: \t112 \n",
      "size: \t64 \n",
      "size: \t31\n",
      "log_step: 0 slot: 1 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 0 slot: 1\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 0 slot: 2 Current Slot Time: (D) = 2020-07-03 06:00:00+00:00\n",
      "log_step: 0 slot: 2 agents_bid_size = 36\n",
      "log_step: 0 slot: 2 agents_bid_price = 1081.6606\n",
      "log_step: 0 slot: 2 settlement_price_DE : 17.72\n",
      "log_step: 0 slot: 2 self.activation_results['slots_won'] = \n",
      "log_step: 0 slot: 2\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 0 slot: 2      agents bid_size = \n",
      "log_step: 0 slot: 2\n",
      "size: \t44 \n",
      "size: \t108 \n",
      "size: \t36 \n",
      "size: \t112 \n",
      "size: \t64 \n",
      "size: \t31\n",
      "log_step: 0 slot: 2 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 0 slot: 2\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 0 slot: 3 Current Slot Time: (D) = 2020-07-03 10:00:00+00:00\n",
      "log_step: 0 slot: 3 agents_bid_size = 112\n",
      "log_step: 0 slot: 3 agents_bid_price = 1569.6455\n",
      "log_step: 0 slot: 3 settlement_price_DE : 16.67\n",
      "log_step: 0 slot: 3 self.activation_results['slots_won'] = \n",
      "log_step: 0 slot: 3\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 0 slot: 3      agents bid_size = \n",
      "log_step: 0 slot: 3\n",
      "size: \t44 \n",
      "size: \t108 \n",
      "size: \t36 \n",
      "size: \t112 \n",
      "size: \t64 \n",
      "size: \t31\n",
      "log_step: 0 slot: 3 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 0 slot: 3\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \t16.67 \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 0 slot: 4 Current Slot Time: (D) = 2020-07-03 14:00:00+00:00\n",
      "log_step: 0 slot: 4 agents_bid_size = 64\n",
      "log_step: 0 slot: 4 agents_bid_price = 2180.204\n",
      "log_step: 0 slot: 4 settlement_price_DE : 18.0\n",
      "log_step: 0 slot: 4 self.activation_results['slots_won'] = \n",
      "log_step: 0 slot: 4\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone\n",
      "log_step: 0 slot: 4      agents bid_size = \n",
      "log_step: 0 slot: 4\n",
      "size: \t44 \n",
      "size: \t108 \n",
      "size: \t36 \n",
      "size: \t112 \n",
      "size: \t64 \n",
      "size: \t31\n",
      "log_step: 0 slot: 4 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 0 slot: 4\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \t16.67 \n",
      "price: \t18.0 \n",
      "price: \tNone\n",
      "log_step: 0 slot: 5 Current Slot Time: (D) = 2020-07-03 18:00:00+00:00\n",
      "log_step: 0 slot: 5 agents_bid_size = 31\n",
      "log_step: 0 slot: 5 agents_bid_price = 2811.0718\n",
      "log_step: 0 slot: 5 settlement_price_DE : 16.67\n",
      "log_step: 0 slot: 5 self.activation_results['slots_won'] = \n",
      "log_step: 0 slot: 5\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1\n",
      "log_step: 0 slot: 5      agents bid_size = \n",
      "log_step: 0 slot: 5\n",
      "size: \t44 \n",
      "size: \t108 \n",
      "size: \t36 \n",
      "size: \t112 \n",
      "size: \t64 \n",
      "size: \t31\n",
      "log_step: 0 slot: 5 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 0 slot: 5\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \t16.67 \n",
      "price: \t18.0 \n",
      "price: \t16.67\n",
      "log_step: 0 slot: None Reward Overview:\n",
      "log_step: 0 slot: None self.activation_results['slots_won']: [-1, -1, -1, -1, -1, -1]\n",
      "log_step: 0 slot: None len(self.activation_results['slots_won']) : 6\n",
      "log_step: 0 slot: 0 slot no. 0\n",
      "log_step: 0 slot: 0 slot no 0 was lost\n",
      "log_step: 0 slot: 0 distance_to_settlement_price = 621.3144360351563\n",
      "log_step: 0 slot: 0 step_reward = 0.536894156280356\n",
      "log_step: 0 slot: 0 for slot no : 0\n",
      "log_step: 0 slot: 0 self.activation_results['slot_settlement_prices_DE'][slot]: 16.67\n",
      "log_step: 0 slot: 0 step_profit: 0\n",
      "log_step: 0 slot: 0 step_reward: 0.536894156280356\n",
      "log_step: 0 slot: 0 weighted_step_reward (step_reward/3) = 0.17896471876011866\n",
      "log_step: 0 slot: 0 total_step_reward = 0.17896471876011866\n",
      "log_step: 0 slot: 1 slot no. 1\n",
      "log_step: 0 slot: 1 slot no 1 was lost\n",
      "log_step: 0 slot: 1 distance_to_settlement_price = 1863.6056494140626\n",
      "log_step: 0 slot: 1 step_reward = 0.2813833245644244\n",
      "log_step: 0 slot: 1 for slot no : 1\n",
      "log_step: 0 slot: 1 self.activation_results['slot_settlement_prices_DE'][slot]: 19.06\n",
      "log_step: 0 slot: 1 step_profit: 0\n",
      "log_step: 0 slot: 1 step_reward: 0.2813833245644244\n",
      "log_step: 0 slot: 1 weighted_step_reward (step_reward/3) = 0.0937944415214748\n",
      "log_step: 0 slot: 1 total_step_reward = 0.27275916028159347\n",
      "log_step: 0 slot: 2 slot no. 2\n",
      "log_step: 0 slot: 2 slot no 2 was lost\n",
      "log_step: 0 slot: 2 distance_to_settlement_price = 1063.94064453125\n",
      "log_step: 0 slot: 2 step_reward = 0.42572138614040245\n",
      "log_step: 0 slot: 2 for slot no : 2\n",
      "log_step: 0 slot: 2 self.activation_results['slot_settlement_prices_DE'][slot]: 17.72\n",
      "log_step: 0 slot: 2 step_profit: 0\n",
      "log_step: 0 slot: 2 step_reward: 0.42572138614040245\n",
      "log_step: 0 slot: 2 weighted_step_reward (step_reward/3) = 0.14190712871346747\n",
      "log_step: 0 slot: 2 total_step_reward = 0.41466628899506097\n",
      "log_step: 0 slot: 3 slot no. 3\n",
      "log_step: 0 slot: 3 slot no 3 was lost\n",
      "log_step: 0 slot: 3 distance_to_settlement_price = 1552.9755078125\n",
      "log_step: 0 slot: 3 step_reward = 0.33193068297276607\n",
      "log_step: 0 slot: 3 for slot no : 3\n",
      "log_step: 0 slot: 3 self.activation_results['slot_settlement_prices_DE'][slot]: 16.67\n",
      "log_step: 0 slot: 3 step_profit: 0\n",
      "log_step: 0 slot: 3 step_reward: 0.33193068297276607\n",
      "log_step: 0 slot: 3 weighted_step_reward (step_reward/3) = 0.11064356099092203\n",
      "log_step: 0 slot: 3 total_step_reward = 0.525309849985983\n",
      "log_step: 0 slot: 4 slot no. 4\n",
      "log_step: 0 slot: 4 slot no 4 was lost\n",
      "log_step: 0 slot: 4 distance_to_settlement_price = 2162.2041015625\n",
      "log_step: 0 slot: 4 step_reward = 0.23736916974806865\n",
      "log_step: 0 slot: 4 for slot no : 4\n",
      "log_step: 0 slot: 4 self.activation_results['slot_settlement_prices_DE'][slot]: 18.0\n",
      "log_step: 0 slot: 4 step_profit: 0\n",
      "log_step: 0 slot: 4 step_reward: 0.23736916974806865\n",
      "log_step: 0 slot: 4 weighted_step_reward (step_reward/3) = 0.07912305658268955\n",
      "log_step: 0 slot: 4 total_step_reward = 0.6044329065686725\n",
      "log_step: 0 slot: 5 slot no. 5\n",
      "log_step: 0 slot: 5 slot no 5 was lost\n",
      "log_step: 0 slot: 5 distance_to_settlement_price = 2794.40177734375\n",
      "log_step: 0 slot: 5 step_reward = 0.15497175983268618\n",
      "log_step: 0 slot: 5 for slot no : 5\n",
      "log_step: 0 slot: 5 self.activation_results['slot_settlement_prices_DE'][slot]: 16.67\n",
      "log_step: 0 slot: 5 step_profit: 0\n",
      "log_step: 0 slot: 5 step_reward: 0.15497175983268618\n",
      "log_step: 0 slot: 5 weighted_step_reward (step_reward/3) = 0.051657253277562064\n",
      "log_step: 0 slot: 5 total_step_reward = 0.6560901598462346\n",
      "log_step: 0 slot: 5  total_weighted_step_reward for all 6 slots : 0.10934835997437244\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "env_to_check = make('VPPBiddingEnv-TEST-v1', render_mode=None)\n",
    "check_env(env_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b9778-4b51-48b3-9b93-8fee865a49ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stable Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4386e0-1c92-4777-b298-3e1e35b01648",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_tags = [\"3step_activation_check\", \"new_logging\", \"no_render_in_reset\", \"distance_reward_slots\", \"not_participated_reward\", \"new_slot_viz\" , \"weighted_step_reward_out_of_loop\"]\n",
    "experiment_timesteps = 2785"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff546b2-3a85-4377-bc6d-75d3d60b41aa",
   "metadata": {},
   "source": [
    "### Offline Training and later sync logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5787c994-9554-4c38-94df-ee1adafe2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"0cea1eee5f42654eca0de365f0acca116367c9b4\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf65e36-b792-4eee-9dd3-5ddcae5b91cb",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdce13-d9e6-49c4-8405-241856fbe544",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a05d95-6c52-4826-9484-bcc9e8306ffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"SAC\"] + experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = SAC(config['policy'], env, verbose=0)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29214f-d5c9-4a8e-9bd1-ce2fb5bfc118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22814aa5-3352-4191-be75-438089e4c506",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bbc69a-b0b1-436c-afbb-3d4016a8b197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"SAC\"] + experiment_tags, \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45654f7a-0f3d-43a1-9376-6dd09f6b5834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a8d29d-30ce-4197-b48c-b724b2c8c829",
   "metadata": {},
   "source": [
    "## A2C "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ef72a-bb38-4d04-87b9-485b7fd82235",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44de4ea7-31ab-4c61-98e7-33317d79d7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/nbformat/validator.py:315: DeprecationWarning:\n",
      "\n",
      "Passing a schema to Validator.iter_errors is deprecated and will be removed in a future release. Call validator.evolve(schema=new_schema).iter_errors(...) instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/wandb/sdk/lib/ipython.py:59: DeprecationWarning:\n",
      "\n",
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>step_profit</td><td>▇███▇▇▆█▇▇█▆▆▆▇▂▇▇▇▇▅▇▇▇███▇▆▆▁▆▇▇▇▆▄██▇</td></tr><tr><td>step_reward</td><td>▂▂▃▃▃▃▃▁▃▃▁▃▅▃▃▄▃▄▄▄▃▆▆▄▅▆▆▃▄▄▃▄▄▃▅▅▃▇█▃</td></tr><tr><td>total_profit</td><td>██████▇▇▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>total_reward</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>2784</td></tr><tr><td>step_profit</td><td>-72115.36678</td></tr><tr><td>step_reward</td><td>0.27524</td></tr><tr><td>total_profit</td><td>-739307213.42046</td></tr><tr><td>total_reward</td><td>995.5646</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/offline-run-20221007_140230-35igvy7g<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20221007_140230-35igvy7g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"A2C\"] + experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = A2C(config['policy'], env, verbose=0)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38c6fa2-75d3-46f5-8746-27e933e218ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/debug-cli.Jan-Lukas.Pflaum.log\n",
      "Syncing: https://wandb.ai/jlu237/RL-VPP-Training/runs/35igvy7g ... done.\n"
     ]
    }
   ],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b5aad3-fa4d-4d9d-84e5-ca95d4b71666",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f255eea2-dd07-4ade-90bc-7bba9d975afb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:297: UserWarning:\n",
      "\n",
      "\u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Run Reward: 0.4377142857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/nbformat/validator.py:315: DeprecationWarning:\n",
      "\n",
      "Passing a schema to Validator.iter_errors is deprecated and will be removed in a future release. Call validator.evolve(schema=new_schema).iter_errors(...) instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/wandb/sdk/lib/ipython.py:59: DeprecationWarning:\n",
      "\n",
      "Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>episode_reward</td><td>▂▂▃▂▂▂▂▂▅▆▇▆▁▄▂▅▅█▃▃▂▂▂▁▂▃▂▂▃▄▂▂▃▃▂▆▅▂▂▄</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>step_profit</td><td>▆▆▆▇▅▆▇▆▇███▇▆▆███▇█▅▆▄▇▃▄▁▅▅▆▄▁▅▆██▂▇▃▂</td></tr><tr><td>step_reward</td><td>▂▂▃▂▂▂▂▂▅▆▇▆▁▄▂▅▅█▃▃▂▂▂▁▂▃▂▂▃▄▂▂▃▃▂▆▅▂▂▄</td></tr><tr><td>total_profit</td><td>████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▄▄▄▃▃▂▂▂▂▂▁▁</td></tr><tr><td>total_reward</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>139</td></tr><tr><td>episode_reward</td><td>0.55556</td></tr><tr><td>global_step</td><td>139</td></tr><tr><td>mean_run_reward</td><td>0.43771</td></tr><tr><td>step_profit</td><td>-227068.5129</td></tr><tr><td>step_reward</td><td>0.55556</td></tr><tr><td>total_profit</td><td>-31753770.57914</td></tr><tr><td>total_reward</td><td>61.28493</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/offline-run-20221007_142723-37baiomn<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20221007_142723-37baiomn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"A2C\"] + experiment_tags, \n",
    "    job_type=\"eval\",\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb20b6a8-4d95-4100-a184-69788ce3c759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find logs at: /Users/Jan-Lukas.Pflaum/Dev/masterthesis/wandb/debug-cli.Jan-Lukas.Pflaum.log\n",
      "Syncing: https://wandb.ai/jlu237/RL-VPP-Evaluation/runs/37baiomn ... done.\n"
     ]
    }
   ],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114da99c-13b3-45e9-8ec0-f8448d7cd69a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contrib packages: TQC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb52fea-bf32-4cad-904c-cff38159c74f",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a5179-c965-42e1-93b7-a2149e18149a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sb3_contrib import TQC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TQC\"] + experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "\n",
    "policy_kwargs = dict(n_critics=2, n_quantiles=25)\n",
    "model = TQC(config['policy'], env, top_quantiles_to_drop_per_net=2, verbose=0, policy_kwargs=policy_kwargs)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e92660-7228-4dd5-abc0-c47242ffa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b85cc0-264e-4deb-84bb-365ec5c75853",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76cd545-bb27-414e-a0a3-e63bee99d996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TQC\"] + experiment_tags, \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, _states = model.predict(observation, deterministic=True)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de50e223-07a5-42bc-bf1b-1648649d4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76662589-ed24-4f6c-aefe-d74d8c80eea7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contrib packages: TRPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b475d-2c52-448f-9dc3-507a7cd1eaa6",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86432aa7-ab9c-4648-ba3f-4b1b2ab64857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sb3_contrib import TRPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TRPO\"] + experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = TRPO(config['policy'], env, verbose=0)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38878b19-3c6a-48e6-b9dd-e256bdb943e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b76e47-53a1-440b-972e-4a3ac0d5d968",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf863e-4168-4822-b8ea-ac462a895697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TRPO\"] + experiment_tags, \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, _states = model.predict(observation,  deterministic=True)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fe4ec-fa34-4313-ba19-97d95939dc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8041122-244a-49b6-ad93-f484ad79652d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Contrib packages: RecurrentPPO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995ab4eb-728d-4949-9729-3640b794f936",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579e0f5-5eb0-4cbe-9f30-82049f9805ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "import numpy as np\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputLstmPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"RecurrentPPO\"] + experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = RecurrentPPO(config['policy'], env, verbose=0)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830f213-74e2-43a7-a24f-07514a3a2ff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5a8cf-6a51-435b-9e40-92c238674217",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93dee6-1e6a-459d-8c0c-8e893955777c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"RecurrentPPO\"] + experiment_tags, \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    lstm_states = None\n",
    "    num_envs = 1\n",
    "    # Episode start signals are used to reset the lstm states\n",
    "    episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, lstm_states = model.predict(observation, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "        observation, reward, dones, info = eval_env.step(action)\n",
    "        episode_starts = dones\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a7104-af25-4ec4-a6f1-ea595ec3e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fb446-9ec5-4eb4-aebb-7f67edf5945e",
   "metadata": {},
   "source": [
    "## TD3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92227e40-1ef0-46a9-8b79-3c474b102033",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08083015-cb57-4510-a84c-9dd52124994c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps # 1 iter = 557\n",
    "}\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "#action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=0.34709686 * np.ones(n_actions))\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\"] + experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = TD3(config['policy'],\n",
    "            env,\n",
    "            verbose=0,\n",
    "            tensorboard_log=f\"runs/ddpg\",\n",
    "            gamma=0.99,\n",
    "            batch_size=200, \n",
    "            buffer_size=1000000,\n",
    "            learning_rate=0.2456,\n",
    "            tau=0.001,\n",
    "            action_noise=action_noise,\n",
    "            policy_kwargs = {'net_arch': [64, 64]}\n",
    "           )\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b3913-c4a1-4521-a962-03b36acac482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05bbeb-906b-4f7c-9473-52a5bce82c43",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3694d5-33a3-4c75-94e8-5d2f0869a038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3\"] + experiment_tags, \n",
    "    job_type=\"eval\",\n",
    "    #settings=wandb.Settings(start_method=\"thread\")\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eddf0a-4b86-4199-af87-8a02afa8d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e96e410-f147-424c-b274-5cb91d58f63c",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa6126-c250-4d39-b21f-fc32fa04005e",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281b68f-d7e7-4fff-879c-d558da2f94fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps #557\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"PPO\"] + experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = PPO(config['policy'], env, verbose=0)\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957fc3e-b248-4f0f-a497-47643af02c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29272f27-6dd0-492e-a187-0373f68e526b",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65dbc2-a534-4141-9e52-46e77090aa2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"PPO\"] + experiment_tags, \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7010536-1fb3-41a3-9735-b73a25cd5f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d3eb4-1556-46f9-81e1-af6956e07e0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DDPG: Deep Deterministic Policy Gradient (DDPG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f86ac-d5b2-498d-922f-50f64b6c5843",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2970c20-6c2e-4392-8972-b657b734e7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": experiment_timesteps #557\n",
    "}\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma= 0.1 * np.ones(n_actions))\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"DDPG\"]+ experiment_tags, \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = DDPG(config['policy'], env, action_noise=action_noise, verbose=0, tensorboard_log=f\"runs/ddpg\")\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=0))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37a144-afe9-4c1f-99be-16c7fb0171a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dcac2-e0b7-4251-9e97-bba435f4ee23",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce413bca-a219-4562-aed3-5852fc79aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1', render_mode=\"human\")\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"DDPG\"]+ experiment_tags, \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 140\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render()\n",
    "        logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            #print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5043e517-9145-4271-9bc3-2f4e6bbf5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb sync wandb/latest-run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
