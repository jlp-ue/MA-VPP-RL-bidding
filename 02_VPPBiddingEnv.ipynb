{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a32f4a0-96ea-48fe-a8ac-fa6adbef7f3e",
   "metadata": {},
   "source": [
    "# gym Environment and Testing of Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37105a1-c06c-445d-b9f9-afc70b8764e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# Data \n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from functools import reduce\n",
    "import json\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# Plotting\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.io import to_html\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# RL\n",
    "from gym import Env\n",
    "from gym import make\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiDiscrete, MultiBinary\n",
    "from gym.spaces import flatdim, flatten_space, unflatten, flatten\n",
    "\n",
    "# Auxiliary \n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43da0f2d-1bd2-4c1c-a8f7-7f92664c98b8",
   "metadata": {},
   "source": [
    "## Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba0636-6a8f-42e0-ae18-157e7d1ae880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPPBiddingEnv(Env):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config_path,\n",
    "                 log_level, \n",
    "                 env_type\n",
    "                ):\n",
    "        \n",
    "        logger = logging.getLogger()\n",
    "        while logger.hasHandlers():\n",
    "                logger.removeHandler(logger.handlers[0])\n",
    "        \n",
    "        if env_type == \"training\":\n",
    "            self.env_type = \"training\"\n",
    "            os.remove(\"logs/training.log\")\n",
    "            fhandler = logging.FileHandler(filename='logs/training.log', mode='w')\n",
    "        if env_type == \"eval\":\n",
    "            self.env_type = \"eval\"\n",
    "            os.remove(\"logs/eval.log\")\n",
    "            fhandler = logging.FileHandler(filename='logs/eval.log', mode='w')\n",
    "        if env_type == \"test\":\n",
    "            self.env_type = \"test\"\n",
    "            fhandler = logging.StreamHandler()\n",
    "            \n",
    "        logger.addHandler(fhandler)           \n",
    "        logger.setLevel(log_level)\n",
    "        \n",
    "        logging.debug(\"log level = debug\")\n",
    "        logging.info(\"log level = info\")\n",
    "        logging.warning(\"log level = warning\")\n",
    "        \n",
    "\n",
    "        # data \n",
    "        self.config = self._load_config(config_path)\n",
    "            \n",
    "        self.renewables_df = self._load_data(\"renewables\")\n",
    "        self.tenders_df = self._load_data(\"tenders\")\n",
    "        self.market_results = self._load_data(\"market_results\") \n",
    "        self.bids_df = self._load_data(\"bids\") \n",
    "        self.time_features_df = self._load_data(\"time_features\") \n",
    "        \n",
    "        self.asset_data , self.asset_data_FCR = self._configure_vpp()\n",
    "        self.asset_data_total = self.asset_data.loc[:,\"Total\"]\n",
    "        self.asset_data_FCR_total = self.asset_data_FCR.loc[:,\"Total\"]\n",
    "        self.maximum_possible_VPP_capacity = round(self.asset_data_total.max(),2) + 0.01\n",
    "        \n",
    "        self.total_slot_FCR_demand = None\n",
    "        \n",
    "        # window_size\n",
    "        self.hist_window_size = self.config[\"config\"][\"time\"][\"hist_window_size\"]\n",
    "        self.forecast_window_size = self.config[\"config\"][\"time\"][\"forecast_window_size\"]\n",
    "        \n",
    "        # episode        \n",
    "        self.first_slot_date_start = pd.to_datetime(self.config[\"config\"][\"time\"][\"first_slot_date_start\"])\n",
    "        self.last_slot_date_end = pd.to_datetime(self.config[\"config\"][\"time\"][\"last_slot_date_end\"])\n",
    "        \n",
    "        # Timeselection of Dataframes\n",
    "        self.renewables_df = self.renewables_df[self.first_slot_date_start:self.last_slot_date_end]\n",
    "        self.tenders_df = self.tenders_df[self.first_slot_date_start:self.last_slot_date_end]\n",
    "        self.market_results = self.market_results[:self.last_slot_date_end] # start prior to first_slot_date_start as data is needed for historic market results\n",
    "        self.bids_df = self.bids_df[self.first_slot_date_start:self.last_slot_date_end]\n",
    "        self.time_features_df = self.time_features_df[self.first_slot_date_start:self.last_slot_date_end]\n",
    "        \n",
    "        logging.debug(\"selection self.renewables_df\" + str(self.renewables_df))\n",
    "        logging.debug(\"selection self.tenders_df\" + str(self.tenders_df))\n",
    "        logging.debug(\"selection self.market_results\" + str(self.market_results))\n",
    "        logging.debug(\"selection self.bids_df\" + str(self.bids_df))\n",
    "        logging.debug(\"selection self.time_features_df\" + str(self.time_features_df))\n",
    "\n",
    "        # slot start , gate closure, auction time \n",
    "        self.lower_slot_start_boundary = self.first_slot_date_start\n",
    "        self.gate_closure = pd.to_datetime(self.tenders_df[self.lower_slot_start_boundary:][\"GATE_CLOSURE_TIME\"][0])\n",
    "        self.slot_start = self.tenders_df[self.lower_slot_start_boundary:].index[0]\n",
    "        self.bid_submission_time = self.gate_closure - pd.offsets.DateOffset(hours = 1)\n",
    "        \n",
    "        self.initial = True\n",
    "        self.done = None\n",
    "        self.total_reward = 0.\n",
    "        self.total_profit = 0.\n",
    "        self.history = None\n",
    "        \n",
    "        # Slots \n",
    "        #self.slots_won = [0, 0, 0, 0, 0, 0]\n",
    "        #self.slot_prices_DE = [0., 0., 0., 0., 0., 0.]\n",
    "        \n",
    "        self.delivery_results = {}\n",
    "        self.previous_delivery_results  = {}\n",
    "        \n",
    "        self.logging_step = -1\n",
    "                \n",
    "        # Spaces\n",
    "        \n",
    "        # Observation Space\n",
    "        obs_low = np.float32(np.array([0.0] * 96)) #96 timesteps to min 0.0\n",
    "        #obs_high = np.float32(np.array([1.0] * 96)) #96 timesteps to max 1.0\n",
    "       \n",
    "        obs_high = np.float32(np.array([self.maximum_possible_VPP_capacity] * 96)) #96 timesteps to max 1.0\n",
    "\n",
    "        # Create a observation space with all observations inside\n",
    "        self.observation_space = Dict({\n",
    "            \"asset_data_historic\": Box(obs_low, obs_high, dtype=np.float32),\n",
    "            \"asset_data_forecast\": Box(obs_low, obs_high, dtype=np.float32),\n",
    "            \"predicted_market_prices\":  Box(low=0.0, high=np.float32(4257.07), shape=(6,), dtype=np.float32), # for each slot, can be prices of same day last week \n",
    "            \"weekday\": Discrete(7), # for the days of the week\n",
    "            \"week\": Discrete(53),  # for week of the year\n",
    "            \"month\": Discrete(12),\n",
    "            \"isHoliday\": Discrete(2), # holiday = 1, no holiday = 0\n",
    "            \"followsHoliday\": Discrete(2), # followsHoliday = 1, no followsHoliday = 0\n",
    "            \"priorHoliday\": Discrete(2), # priorHoliday = 1, no priorHoliday = 0\n",
    "            \"slots_won\": MultiBinary(6), #boolean for each slot, 0 if loss , 1 if won \n",
    "            \"slot_prices_DE\": Box(low=0.0, high=np.float32(4257.07), shape=(6,), dtype=np.float32)\n",
    "            })\n",
    "        \n",
    "        self.observation = None\n",
    "        \n",
    "        \n",
    "        # Action Space\n",
    "        \n",
    "        # VERSION 3\n",
    "        # Convert complex action space to flattended space\n",
    "        \n",
    "        # maximum possible FCR capacity \n",
    "        maximum_possible_FCR_capacity = round(self.asset_data_FCR_total.max(),2)\n",
    "        maximum_possible_market_price = self.bids_df[\"settlement_price\"].max()\n",
    "        \n",
    "        #TODO: DELETE NEXT LINE \n",
    "        #maximum_possible_market_price = 100.0\n",
    "        \n",
    "        # 12 values from  min 0.0\n",
    "        action_low = np.float32(np.array([0.0] * 12)) \n",
    "        # 6 values to max maximum_possible_FCR_capacity = the bid sizes \n",
    "        # 6 values to max maximum_possible_market_price = the bid prices\n",
    "        #action_high = np.float32(np.array([maximum_possible_FCR_capacity] * 6 + [maximum_possible_market_price] *6 )) \n",
    "        action_high = np.float32(np.array([self.maximum_possible_VPP_capacity] * 6 + [maximum_possible_market_price] *6 )) \n",
    "        \n",
    "        self.action_space = Box(low=action_low, high=action_high, shape=(12,), dtype=np.float32)\n",
    "        \n",
    "        # VERSION 2 \n",
    "        \n",
    "        '''# Convert complex action space to flattended space\n",
    "        # bid sizes =  6 DISCRETE slots from 0 to 25  = [ 25, 25, 25, 25, 25 , 25]  = in flattened = 150 values [0,1]\n",
    "        # bid prizes = 6 CONTINUOUS slots from 0 to 100  = [ 100., 100., 100., 100., 100. , 100.]  = in flattened = 150 values [0,1]\n",
    "\n",
    "        # 156 values from  min 0.0\n",
    "        action_low = np.float32(np.array([0.0] * 156)) \n",
    "        #150 values to max 1.0 = the bid sizes \n",
    "        # +6 values to max 100. = the bid prices\n",
    "        action_high = np.float32(np.array([1.0] * 150 + [100.0]*6)) \n",
    "        self.action_space = Box(low=action_low, high=action_high, shape=(156,), dtype=np.float32)'''\n",
    "\n",
    "        # VERSION 1\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        self.complex_action_space = Tuple((\n",
    "            # INFO: TSOs allow divisible and indivisible bids. Biggest divisible bid was 188 MW , maximum price was 4257.07 \n",
    "            #MultiDiscrete([ 188, 188, 188, 188, 188 , 188]),\n",
    "            MultiDiscrete([ 25, 25, 25, 25, 25 , 25]),\n",
    "            #Box(low=0.0, high=np.float32(4257.07), shape=(6,), dtype=np.float32)))\n",
    "            Box(low=0.0, high=np.float32(100.), shape=(6,), dtype=np.float32)))\n",
    "        \n",
    "        #flatten_action_space_64 = flatten_space(self.complex_action_space)\n",
    "        #self.action_space = flatten_action_space_64\n",
    "\n",
    "        \n",
    "        #logging.debug(flatten_action_space_64)\n",
    "        #logging.debug(type(flatten_action_space_64))\n",
    "        #logging.debug(\"#\" *42)\n",
    "        \n",
    "        #flattened_action = flatten(self.complex_action_space, self.complex_action_space.sample())\n",
    "        #logging.debug(flattened_action)\n",
    "\n",
    "        #unflattened_action = unflatten(self.complex_action_space, flattened_action)\n",
    "        #logging.debug(unflattened_action)'''\n",
    "    \n",
    "    def _load_config(self, config_path):        \n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        return config\n",
    "            \n",
    "        \n",
    "    def _load_data(self, data_source):\n",
    "        df = pd.read_csv(self.config[\"config\"][\"csv_paths\"][data_source], sep = \";\", index_col = 0)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def _configure_vpp(self):\n",
    "        # list to concat all dfs later on\n",
    "        asset_frames_total = []\n",
    "        asset_frames_FCR = []\n",
    "        # for each asset type defined in the config (e.g.: \"hydro\", \"wind\")\n",
    "        for asset_type in self.config[\"assets\"].keys():\n",
    "            # for every plant configuration there is per asset type\n",
    "            for plant_config in range(len(self.config[\"assets\"][asset_type])):\n",
    "                # get the qunatity of plants\n",
    "                quantity = self.config[\"assets\"][asset_type][plant_config][\"quantity\"]\n",
    "                # get the maximum capacity of these plants \n",
    "                max_capacity_MW = self.config[\"assets\"][asset_type][plant_config][\"max_capacity_MW\"]\n",
    "                max_FCR_capacity_share = self.config[\"assets\"][asset_type][plant_config][\"max_FCR_capacity_share\"]\n",
    "                # get the name of the column in the renewables csv\n",
    "                asset_column_names = self.config[\"assets\"][asset_type][plant_config][\"asset_column_names\"]\n",
    "                # initialize a array with zeros and the length of the renewables dataframe\n",
    "                total_asset_capacity = np.array([0.0] * len(self.renewables_df))\n",
    "                total_asset_FCR_capacity = np.array([0.0] * len(self.renewables_df))\n",
    "\n",
    "                i = 1\n",
    "                while i < quantity: \n",
    "                    for asset_column_name in (asset_column_names):\n",
    "                        asset_data = self.renewables_df[[asset_column_name]].values.flatten()\n",
    "                        asset_data *= max_capacity_MW\n",
    "                        asset_FCR_capacity = asset_data * max_FCR_capacity_share\n",
    "                        \n",
    "                        total_asset_FCR_capacity += asset_FCR_capacity \n",
    "                        total_asset_capacity += asset_data\n",
    "                        \n",
    "                        i += 1\n",
    "                        if i < quantity:\n",
    "                            continue\n",
    "                        else: \n",
    "                            break                        \n",
    "                    \n",
    "                total_df = pd.DataFrame(index=self.renewables_df.index)\n",
    "                FCR_df = pd.DataFrame(index=self.renewables_df.index)\n",
    "                \n",
    "                total_df[asset_type + \"_class_\" + str(plant_config)] = total_asset_capacity\n",
    "                FCR_df[asset_type + \"_class_\" + str(plant_config)] = total_asset_FCR_capacity\n",
    "                asset_frames_total.append(total_df)\n",
    "                asset_frames_FCR.append(FCR_df)\n",
    "\n",
    "        if not asset_frames_total: \n",
    "            logging.error(\"No asset data found\")\n",
    "        all_asset_data = reduce(lambda x, y: pd.merge(x, y, on = \"time\"), asset_frames_total)\n",
    "        all_asset_data_FCR = reduce(lambda x, y: pd.merge(x, y, on = \"time\"), asset_frames_FCR)\n",
    "\n",
    "        all_asset_data['Total'] = all_asset_data.iloc[:,:].sum(axis=1)\n",
    "        all_asset_data_FCR['Total'] = all_asset_data_FCR.iloc[:,:].sum(axis=1)\n",
    "        \n",
    "        return all_asset_data, all_asset_data_FCR\n",
    "    \n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        if self.initial is False: \n",
    "            self.lower_slot_start_boundary = self.lower_slot_start_boundary  + pd.offsets.DateOffset(days=1)\n",
    "            self.gate_closure = pd.to_datetime(self.tenders_df[self.lower_slot_start_boundary:][\"GATE_CLOSURE_TIME\"][0])\n",
    "            self.slot_start = self.tenders_df[self.lower_slot_start_boundary:].index[0]\n",
    "            self.bid_submission_time = self.gate_closure - pd.offsets.DateOffset(hours = 1)\n",
    "            \n",
    "            logging.info(\"new self.lower_slot_start_boundary = \" + str(self.lower_slot_start_boundary))\n",
    "            logging.info(\"self.gate_closure = \" + str(self.gate_closure))\n",
    "            logging.info(\"self.slot_start = \" + str(self.slot_start))\n",
    "            logging.info(\"self.bid_submission_time = \" + str(self.bid_submission_time))\n",
    "\n",
    "        self.total_slot_FCR_demand = self.tenders_df[str(self.slot_start):][\"total\"][0] \n",
    "        self.done = False\n",
    "\n",
    "        self.previous_delivery_results = self.delivery_results.copy()\n",
    "        logging.debug(\"self.delivery_results = \" + str(self.delivery_results))\n",
    "        self.delivery_results.clear()\n",
    "        logging.debug(\"self.delivery_results after clearing\")\n",
    "        logging.debug(\"self.delivery_results = \" + str(self.delivery_results))\n",
    "        logging.debug(\"self.previous_delivery_results after clearing\")\n",
    "        logging.debug(\"self.previous_delivery_results = \" + str(self.previous_delivery_results))\n",
    "        \n",
    "        self.delivery_results[\"slots_won\"] = [0, 0, 0, 0, 0, 0]\n",
    "        self.delivery_results[\"slot_prices_DE\"] = [0., 0., 0., 0., 0., 0.]\n",
    "        \n",
    "        # reset for each episode \n",
    "        self._get_new_timestamps()\n",
    "        \n",
    "        # get new observation\n",
    "        self._get_observation()\n",
    "        \n",
    "        # when first Episode is finished, set boolean.  \n",
    "        self.initial = False\n",
    "        \n",
    "        self.logging_step += 1\n",
    "        logging.debug(\"logging_step: \" + str(self.logging_step))\n",
    "        \n",
    "        return self.observation\n",
    "                \n",
    "    \n",
    "    def _get_new_timestamps(self):\n",
    "                \n",
    "        self.historic_data_start = self.bid_submission_time - pd.offsets.DateOffset(days=self.hist_window_size)\n",
    "        self.historic_data_end =  self.bid_submission_time - pd.offsets.DateOffset(minutes = 15)\n",
    "        logging.debug(\"self.historic_data_start = \" + str(self.historic_data_start))\n",
    "        logging.debug(\"self.historic_data_end = \" + str(self.historic_data_end))\n",
    "        \n",
    "        self.forecast_start = self.slot_start\n",
    "        self.forecast_end = self.forecast_start + pd.offsets.DateOffset(days=self.forecast_window_size) - pd.offsets.DateOffset(minutes=15) \n",
    "        logging.debug(\"self.forecast_start = \" + str(self.forecast_start))\n",
    "        logging.debug(\"self.forecast_end = \" + str(self.forecast_end))\n",
    "\n",
    "        self.market_start = self.slot_start\n",
    "        self.market_end = self.market_start + pd.offsets.DateOffset(hours=24) - pd.offsets.DateOffset(minutes = 15)\n",
    "        logging.debug(\"self.market_start = \" + str(self.market_start))\n",
    "        logging.debug(\"self.market_end = \" + str(self.market_end))\n",
    "\n",
    "        self.slot_date_list = self.tenders_df[self.market_start:][0:6].index\n",
    "        \n",
    "        '''self.slot_date_list = []\n",
    "        slot_date = self.market_start \n",
    "        for i in range(0,6):\n",
    "            self.slot_date_list.append(str(slot_date))\n",
    "            slot_date = slot_date + pd.offsets.DateOffset(hours=4)  '''\n",
    "            \n",
    "        logging.debug(\"self.slot_date_list = \" + str( self.slot_date_list))\n",
    "    \n",
    "    \n",
    "    def _add_gaussian_noise(self, data, whole_data):\n",
    "        mean = 0.0\n",
    "        standard_deviation = np.std(whole_data)\n",
    "        standard_deviation_gaussian = standard_deviation *  0.2 # for 20% Gaussian noise\n",
    "        noise = np.random.normal(mean, standard_deviation_gaussian, size = data.shape)\n",
    "        data_noisy = data + noise\n",
    "        # Set negative values to 0 \n",
    "        data_noisy = data_noisy.clip(min=0)\n",
    "\n",
    "        return data_noisy \n",
    "    \n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \n",
    "        '''if (self.done is False) and (self.initial is False):\n",
    "            print(\"if schleife 1 \")\n",
    "            print(\"done = \" + str(self.done))\n",
    "            print(\"initial = \" + str(self.initial))\n",
    "            \n",
    "            self.observation[\"slots_won\"] = np.array(self.delivery_results[\"slots_won\"], dtype=np.int32)\n",
    "            self.observation[\"slot_prices_DE\"] = np.array(self.delivery_results[\"slot_prices_DE\"], dtype=np.float32)\n",
    "            \n",
    "            \n",
    "        if (self.done is True) or (self.initial is True):\n",
    "            print(\"if schleife 2 \")\n",
    "            print(\"done = \" + str(self.done))\n",
    "            print(\"initial = \" + str(self.initial))'''\n",
    "\n",
    "        asset_data_historic = self.asset_data_total[str(self.historic_data_start) : str(self.historic_data_end)].to_numpy(dtype=np.float32)\n",
    "        logging.debug(\"asset_data_historic = \" + str(self.asset_data_total[str(self.historic_data_start) : str(self.historic_data_end)]) )\n",
    "\n",
    "        asset_data_forecast = self.asset_data_total[str(self.forecast_start) : str(self.forecast_end)].to_numpy(dtype=np.float32)\n",
    "        logging.debug(\"asset_data_forecast = \"  + str(self.asset_data_total[str(self.forecast_start) : str(self.forecast_end)]))\n",
    "\n",
    "        # add gaussian noise to data\n",
    "        noisy_asset_data_forecast = self._add_gaussian_noise(asset_data_forecast, self.asset_data_total)\n",
    "        noisy_asset_data_forecast = noisy_asset_data_forecast.astype(np.float32)\n",
    "        logging.debug(\"noisy_asset_data_forecast = \"  + str(noisy_asset_data_forecast))\n",
    "\n",
    "        # for predicted market Prices try naive prediction: retrieve price of same day last week \n",
    "        market_start_last_week = self.market_start - pd.offsets.DateOffset(days=7) \n",
    "        market_end_last_week = self.market_end - pd.offsets.DateOffset(days=7)\n",
    "        logging.debug(\"market_start_last_week = \"  + str(market_start_last_week))\n",
    "        logging.debug(\"market_end_last_week = \"  + str(market_end_last_week))\n",
    "        predicted_market_prices = self.market_results[\"DE_SETTLEMENTCAPACITY_PRICE_[EUR/MW]\"][str(market_start_last_week) : str(market_end_last_week)].to_numpy(dtype=np.float32)\n",
    "        logging.debug(\"predicted_market_prices = \"  + str(predicted_market_prices))\n",
    "        if len(predicted_market_prices) < 6:\n",
    "            # predicted_market_prices list is smaller than 6 so fake is generated mean of first week\n",
    "            predicted_market_prices = np.array([ 17.48, 17.48, 17.48, 17.48, 17.48, 17.48], dtype=np.float32) \n",
    "            logging.debug(\"predicted_market_prices list is smaller than 6 so fake is generated: \"  + str(predicted_market_prices))\n",
    "        \n",
    "        time_features = self.time_features_df[str(self.market_start) : str(self.market_end)]\n",
    "        logging.debug(self.time_features_df[str(self.market_start) : str(self.market_end)])\n",
    "\n",
    "        weekday = int(time_features[\"weekday\"][0])\n",
    "        week = int(time_features[\"week\"][0])\n",
    "        month = int(time_features[\"month\"][0])\n",
    "        isHoliday = int(time_features[\"is_holiday\"][0])\n",
    "        followsHoliday = int(time_features[\"followsHoliday\"][0])\n",
    "        priorHoliday = int(time_features[\"priorHoliday\"][0])\n",
    "\n",
    "        slots_won =  np.array(self.delivery_results[\"slots_won\"], dtype=np.int32)\n",
    "        slot_prices_DE = np.array(self.delivery_results[\"slot_prices_DE\"], dtype=np.float32)\n",
    "\n",
    "        self.observation = OrderedDict({\n",
    "            \"asset_data_historic\": asset_data_historic,\n",
    "            \"asset_data_forecast\": noisy_asset_data_forecast,\n",
    "            \"predicted_market_prices\": predicted_market_prices,\n",
    "            \"weekday\": weekday, \n",
    "            \"week\": week, \n",
    "            \"month\": month,\n",
    "            \"isHoliday\": isHoliday, \n",
    "            \"followsHoliday\": followsHoliday,\n",
    "            \"priorHoliday\": priorHoliday,\n",
    "            \"slots_won\": slots_won,\n",
    "            \"slot_prices_DE\": slot_prices_DE\n",
    "            })\n",
    "        logging.debug(\"NEW Observation = \"  + str(self.observation))\n",
    "            \n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        # convert action list with shape (12,) into dict\n",
    "        action_dict = {\n",
    "            \"size\": action[0:6], \n",
    "            \"price\": action[6:]\n",
    "        }\n",
    "        \n",
    "        # Simulate VPP \n",
    "        self._simulate_vpp()\n",
    "        \n",
    "        # Simulate Market \n",
    "        # take the bid out of the action of the agent and resimulate the market clearing algorithm\n",
    "        self._simulate_market(action_dict)\n",
    "        \n",
    "        # Prepare the data for the delivery simulation and reward calculation\n",
    "        self._prepare_delivery()\n",
    "        \n",
    "        # calculate reward from state and action \n",
    "        step_reward = self._calculate_reward(action_dict)\n",
    "        \n",
    "        self.total_reward += step_reward\n",
    "            \n",
    "        info = dict(\n",
    "            bid_submission_time = str(self.bid_submission_time),\n",
    "            step_reward = round(step_reward,2),\n",
    "            total_reward = round(self.total_reward,2),\n",
    "            total_profit = round(self.total_profit,2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self._update_history(info)\n",
    "                \n",
    "        self.done = True\n",
    "        self._get_observation()\n",
    "        \n",
    "        \n",
    "        if self.env_type != \"test\":\n",
    "            \n",
    "            if self.env_type == \"training\":\n",
    "                wandb.log({\n",
    "                    \"global_step\": self.logging_step,\n",
    "                    \"total_reward\": self.total_reward,\n",
    "                    \"total_profit\": self.total_profit,\n",
    "                    \"step_reward\": step_reward},\n",
    "                    #step=self.logging_step,\n",
    "                    commit=False)\n",
    "                \n",
    "                self.render()\n",
    "            \n",
    "            if self.env_type == \"eval\":\n",
    "                wandb.log({\n",
    "                    \"global_step\": self.logging_step,\n",
    "                    \"total_reward\": self.total_reward,\n",
    "                    \"total_profit\": self.total_profit,\n",
    "                    \"step_reward\": step_reward},\n",
    "                    step=self.logging_step,\n",
    "                    commit=False\n",
    "                )\n",
    "        \n",
    "        return self.observation, step_reward, self.done, info\n",
    "    \n",
    "    \n",
    "    def _calculate_reward(self, action_dict):        \n",
    "        # Step 1 of Reward Function: The Auction\n",
    "        # did the agent win the auction? \n",
    "        # what was the revenue ?\n",
    "        \n",
    "        step_reward = 0\n",
    "        \n",
    "        # per slot won: + 100\n",
    "        # per slot won: + (bid size *  marginal prize)\n",
    "        # per slot lost: -100\n",
    "\n",
    "        logging.info(\"Reward Overview:\")\n",
    "        logging.debug(\"self.delivery_results['slots_won']: \" + str(self.delivery_results[\"slots_won\"]))\n",
    "        logging.debug(\"len(self.delivery_results['slots_won']) : \"  + str(len(self.delivery_results[\"slots_won\"])))       \n",
    "        \n",
    "        for slot in range(0, len(self.delivery_results[\"slots_won\"])):\n",
    "            \n",
    "            logging.debug(\"slot no. \" + str(slot))\n",
    "            \n",
    "            if self.delivery_results[\"slots_won\"][slot] == 0:\n",
    "                logging.debug(\"slot no \" + str(slot) + \" was lost\")\n",
    "                step_reward -= 100\n",
    "\n",
    "            if self.delivery_results[\"slots_won\"][slot] == 1:\n",
    "                logging.debug(\"slot no. \" + str(slot)+  \" was won\")\n",
    "\n",
    "                # Approach 1 : first reward the won slot, then check if it could be delivered and give huge negative reward (-1000)\n",
    "                # Approach 2 : first check if won slot could be delivered and then calculate partial reward (60 minutes - penalty minutes / 60 ) * price * size \n",
    "                # we try Approach 1 \n",
    "    \n",
    "                \n",
    "                # Step 1: award the agent for a won slot\n",
    "                step_reward += 100\n",
    "                \n",
    "                # Step 2: Calculate the Profit of the bid if won \n",
    "                \n",
    "                # extract the bid size of the agent \n",
    "                agents_bid_size = self.delivery_results[\"agents_bid_sizes_round\"][slot]\n",
    "                # and calculate the reward by multiplying the bid size with the settlement price of the slot\n",
    "                step_profit = (agents_bid_size * self.delivery_results[\"slot_prices_DE\"][slot])\n",
    "                \n",
    "                # Step 3: validate if the VPP can deliver the traded capacity\n",
    "                self._simulate_delivery(slot, action_dict)\n",
    "                logging.debug(\"self.delivery_results['delivered_slots']\")\n",
    "                logging.debug(self.delivery_results[\"delivered_slots\"])\n",
    "\n",
    "                # Step 4: if the capacity can not be delivered give a high Penalty\n",
    "                if self.delivery_results[\"delivered_slots\"][slot] == False:\n",
    "                    step_reward -= 5000\n",
    "                \n",
    "                # Update the total profit and Step Reward. \n",
    "                self._update_profit(step_profit)\n",
    "                step_reward +=  step_profit\n",
    "                \n",
    "                logging.debug(\"agents_bid_size: \" + str(agents_bid_size))\n",
    "                logging.debug(\"self.delivery_results['slot_prices_DE'][slot]: \" + str(self.delivery_results[\"slot_prices_DE\"][slot]))\n",
    "                logging.debug(\"step_profit: \" + str(step_profit))\n",
    "                \n",
    "            logging.info(\"step_reward Slot \" + str(slot) +\" = \" + str(step_reward))\n",
    "        \n",
    "    \n",
    "        \n",
    "        # further rewards? \n",
    "        # diff to the settlement price\n",
    "        # diff to the max. forecasted capacity of the VPP\n",
    "        # incentive to go nearer to settlement price or forecasted capacity can be: 1- (abs(diff_to_capacity)/max_diff_to_capacity)^0.5\n",
    "        # idea: reward for positive and negative reward separate. \n",
    "        \n",
    "        # Alternative solution: \n",
    "        # A reward function, that combines penalty and delivered FCR: \n",
    "        # compensation = (60 minutes - penalty minutes / 60 ) * price * size \n",
    "        # penalty  = (penalty minutes / 60 ) * price * size \n",
    "        # reputation_damage = reputation_factor *  penalty_min/ 60 * size\n",
    "            # penalty_min = number of minutes where capacity could not be provided\n",
    "        # in total: r = compensation − penalty − reputation_damage,\n",
    "        \n",
    "        return step_reward\n",
    "    \n",
    "    \n",
    "    def _update_profit(self, step_profit):\n",
    "        \n",
    "        self.total_profit += step_profit\n",
    "        \n",
    "    \n",
    "    def _update_history(self, info):\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "            \n",
    "    def render(self, mode=\"human\"):\n",
    "        if not self.delivery_results:\n",
    "            logging.debug(\"self.delivery_results is empty, not plotting it \")\n",
    "        else:\n",
    "            # only plot to wandb when not in test mode\n",
    "            if self.env_type != \"test\":\n",
    "                if self.logging_step > 0: \n",
    "                    logging.debug(\" now in render()\")        \n",
    "                    logging.debug(\" self.previous_delivery_results \" + str(self.previous_delivery_results))      \n",
    "                    logging.debug(\" self.delivery_results['slots_won'] \" + str(self.delivery_results[\"slots_won\"]))      \n",
    "\n",
    "                    # Render Won / Lost Slots \n",
    "                    slots_won = self.previous_delivery_results[\"slots_won\"]\n",
    "                    logging.debug(\" slots_won \" + str(slots_won))      \n",
    "                    slots_lost = [None,None,None,None,None,None]\n",
    "                    for x in range(len(slots_won)):\n",
    "                        if slots_won[x] == 1:\n",
    "                            slots_lost[x] = 0\n",
    "                        else:\n",
    "                            slots_lost[x] = 1\n",
    "\n",
    "                    data = {'Slot Won': slots_won, 'Slot Lost': slots_lost}\n",
    "                    slots_df = pd.DataFrame(data=data, index=[1, 2, 3, 4, 5, 6])\n",
    "                    logging.debug(\" slots_df \" + str(slots_df))\n",
    "                    slots_won_plot = px.bar(slots_df,  x= slots_df.index, y=['Slot Won', 'Slot Lost'], color_discrete_sequence=[ \"green\", \"gainsboro\"] )\n",
    "\n",
    "                    # Render Delivery for Capacity \n",
    "                    delivery_plot = go.Figure()\n",
    "                    delivery_plot.add_trace(go.Scatter(x=list(range(1, 97)), y=self.previous_delivery_results[\"vpp_total\"], fill='tozeroy', fillcolor='rgba(0, 85, 255, 0.4)',  line_color=\"blue\", name=\"VPP Cap.\"))\n",
    "                    delivery_plot.add_trace(go.Scatter(x=list(range(1, 97)), y=self.previous_delivery_results[\"vpp_total_FCR\"], fill='tozeroy', line_color=\"green\", name=\"VPP FCR Cap.\" )) \n",
    "                    delivery_plot.add_trace(go.Scatter(x=list(range(1, 97)), y=self.previous_delivery_results[\"bid_sizes_all_slots\"], fill='tozeroy', fillcolor='rgba(255, 0, 0, 0.5)', line_color=\"red\", name=\"Agents Bid\" )) \n",
    "\n",
    "                    # Render Delivery for each Slot \n",
    "                    slots_delivered = [None,None,None,None,None,None]\n",
    "                    for slot in range(6):\n",
    "                        if self.previous_delivery_results[\"delivered_slots\"][slot] == True:\n",
    "                            slots_delivered[slot] = 1\n",
    "                        else: \n",
    "                            slots_delivered[slot] = 0\n",
    "                    slots_not_delivered = [None,None,None,None,None,None]\n",
    "                    for x in range(len(slots_delivered)):\n",
    "                        if slots_delivered[x] == 1:\n",
    "                            slots_not_delivered[x] = 0\n",
    "                        else:\n",
    "                            slots_not_delivered[x] = 1\n",
    "                            \n",
    "                    data = {'delivered': slots_delivered, 'NOT deliv.': slots_not_delivered}\n",
    "                    slots_delivered_df = pd.DataFrame(data=data, index=[1, 2, 3, 4, 5, 6])\n",
    "                    logging.debug(\" slots_delivered_df \" + str(slots_delivered_df))\n",
    "                    slots_delivered_plot = px.bar(slots_delivered_df,  x= slots_delivered_df.index, y=['delivered', 'NOT deliv.'], color_discrete_sequence=[ \"lawngreen\", \"red\"] )\n",
    "\n",
    "    \n",
    "                    # Render Agents Slot Prices and Settlement Prices \n",
    "\n",
    "                    price_plot = go.Figure()\n",
    "                    price_plot.add_trace(go.Scatter(x=list(range(1,7)), y=self.previous_delivery_results[\"settlement_price_DE\"], line_color=\"blue\", name=\"Market Price\"))\n",
    "                    price_plot.add_trace(go.Scatter(x=list(range(1,7)), y=self.previous_delivery_results[\"agents_bid_prices\"] , line_color=\"red\", name=\"Agents Price\" )) \n",
    "\n",
    "\n",
    "                    if self.env_type != \"test\":\n",
    "                        \n",
    "                        if self.env_type == \"training\":\n",
    "                            wandb.log({\n",
    "                                \"Won / Loss of Slots\": slots_won_plot,\n",
    "                                \"Sold and Available Capacity\" : delivery_plot,\n",
    "                                \"Agents and Settlement Prices per Slot\" : price_plot,\n",
    "                                \"Delivery per Slot\": slots_delivered_plot},\n",
    "                                #step=self.logging_step,\n",
    "                                commit=True\n",
    "                            )\n",
    "                            \n",
    "                        if self.env_type == \"eval\":\n",
    "                            wandb.log({\n",
    "                                \"global_step\": self.logging_step,\n",
    "                                \"Won / Loss of Slots\": slots_won_plot,\n",
    "                                \"Sold and Available Capacity\" : delivery_plot,\n",
    "                                \"Agents and Settlement Prices per Slot\" : price_plot,\n",
    "                                \"Delivery per Slot\": slots_delivered_plot},\n",
    "                                step=self.logging_step,\n",
    "                                commit=False\n",
    "                            )\n",
    "\n",
    "    \n",
    "    def _simulate_vpp(self):\n",
    "        \n",
    "        vpp_total = self.asset_data_total[str(self.market_start) : str(self.market_end)].to_numpy(dtype=np.float32)\n",
    "        vpp_total_FCR = self.asset_data_FCR_total[str(self.market_start) : str(self.market_end)].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        self.delivery_results[\"vpp_total\"] = vpp_total\n",
    "        self.delivery_results[\"vpp_total_FCR\"] = vpp_total_FCR\n",
    "        self.delivery_results[\"bid_sizes_all_slots\"] = [0] * 96\n",
    "        \n",
    "    \n",
    "\n",
    "    def _simulate_market(self, action_dict):\n",
    "        \n",
    "        auction_bids = self.bids_df[self.market_start : self.market_end]\n",
    "        logging.debug(\"auction_bids = \")        \n",
    "        logging.debug(self.bids_df[self.market_start : self.market_end])\n",
    "        \n",
    "        logging.info(\"Bid Submission time (D-1) = %s\" % (self.bid_submission_time))\n",
    "        logging.info(\"Gate Closure time (D-1) = %s\" % (self.gate_closure))\n",
    "        logging.info(\"Historic Data Window: from %s to %s \" % (self.historic_data_start, self.historic_data_end))\n",
    "        logging.info(\"Forecast Data Window: from %s to %s \" % (self.forecast_start, self.forecast_end))\n",
    "\n",
    "        self.delivery_results[\"agents_bid_prices\"] = [None,None,None,None,None,None]\n",
    "        self.delivery_results[\"settlement_price_DE\"] = [None,None,None,None,None,None]\n",
    "        self.delivery_results[\"agents_bid_sizes_round\"] = [None,None,None,None,None,None]\n",
    "        self.delivery_results[\"slots_won\"] = [None,None,None,None,None,None]\n",
    "\n",
    "        for slot in range(0, len(self.slot_date_list)):\n",
    "            slot_date = self.slot_date_list[slot]\n",
    "            logging.info(\"Current Slot Time: (D) = %s\" % (slot_date)) \n",
    "            slot_bids = auction_bids[slot_date : slot_date].reset_index(drop=True).reset_index(drop=False)\n",
    "            logging.debug(\"slot_bids = \" + str(slot_bids))\n",
    "            slot_bids_list = slot_bids.to_dict('records')\n",
    "            logging.debug(\"slot_bids_list = \" + str(slot_bids_list))\n",
    "            # extract the bid size out of the agents action\n",
    "            # ROUND TO FULL INTEGER\n",
    "            agents_bid_size = round(action_dict[\"size\"][slot])\n",
    "            self.delivery_results[\"agents_bid_sizes_round\"][slot] = agents_bid_size\n",
    "            # extract the bid price out of the agents action\n",
    "            agents_bid_price = action_dict[\"price\"][slot]\n",
    "            # TODO: add to delivery results\n",
    "            self.delivery_results[\"agents_bid_prices\"][slot] = agents_bid_price\n",
    "            logging.info(\"agents_bid_size = %s\" % (agents_bid_size))\n",
    "            logging.info(\"agents_bid_price = %s\" % (agents_bid_price))            \n",
    "            # get settlement price\n",
    "            settlement_price_DE = [bid['settlement_price'] for bid in slot_bids_list if bid['country']== \"DE\"][0] \n",
    "            logging.info( \"settlement_price_DE : \" + str(settlement_price_DE))\n",
    "            self.delivery_results[\"settlement_price_DE\"][slot] = settlement_price_DE\n",
    "\n",
    "            \n",
    "            # First check if agents bid price is higher than the settlement price of Germany \n",
    "            # OR if agents bid size is 0 \n",
    "            if (agents_bid_price > settlement_price_DE) or (agents_bid_size == 0):\n",
    "                # if it is higher, the slot is lost. \n",
    "                self.delivery_results[\"slots_won\"][slot] = 0\n",
    "                # set settlement price for the current auctioned slot in slot_prices_DE list\n",
    "                self.delivery_results[\"slot_prices_DE\"][slot] = settlement_price_DE\n",
    "            else: \n",
    "                # If agents bid price is lower than settlement price (bid could be in awarded bids)\n",
    "                # get CBMP of countries without LMP\n",
    "                unique_country_bids = list({v['country']:v for v in slot_bids_list}.values())\n",
    "                grouped_prices = [x['settlement_price'] for x in unique_country_bids]\n",
    "                cbmp = max(set(grouped_prices), key = grouped_prices.count)\n",
    "                logging.info( \"cbmp : \" + str(cbmp))\n",
    "                # check if settlement_price_DE is same as CBMP (no limit constraints where hit)\n",
    "                if cbmp == settlement_price_DE:\n",
    "                    price_filter = cbmp\n",
    "                    logging.debug(\"DE has CBMP\")\n",
    "                else: \n",
    "                    # if Germany has a price based on limit constraints\n",
    "                    price_filter = settlement_price_DE\n",
    "                    logging.debug(\"DE has LMP\")\n",
    "                                \n",
    "                # as the probability is high that the agents bid moved the last bid out of the list, \n",
    "                # we have to check which bids moved out of the list and what is the new settlement price\n",
    "                \n",
    "                # sort the bid list based on the price\n",
    "                slot_bids_list_sorted_by_price = sorted(slot_bids_list, key=lambda x: x['price'])\n",
    "                # filter the bid list by the settlement price of either the CBMP or the LMP of germany \n",
    "                #slot_bids_prices_filtered = [bid['price'] for bid in slot_bids_list_sorted_by_price if bid['settlement_price']== price_filter]\n",
    "                #logging.debug(slot_bids_prices_filtered)\n",
    "                slot_bids_filtered = [bid for bid in slot_bids_list_sorted_by_price if bid['settlement_price']== price_filter]\n",
    "                accumulated_replaced_capacity = 0\n",
    "                \n",
    "                slot_bids_filtered_size_sum = sum([bid['size'] for bid in slot_bids_filtered])\n",
    "                    # for the case the action_dict space is not dynamic and agent can choose any bid size,\n",
    "                    # it needs to be checked here if \n",
    "                if agents_bid_size >= slot_bids_filtered_size_sum:\n",
    "                    logging.debug(\"unrealistic bid size\")\n",
    "                    # set auction won to false\n",
    "                    self.delivery_results[\"slots_won\"][slot] = 0\n",
    "                    # set settlement price to zero as it is an unrealistic auciton\n",
    "                    self.delivery_results[\"slot_prices_DE\"][slot] = 0\n",
    "                else:\n",
    "                    for bid in range(0, len(slot_bids_filtered)): \n",
    "                        logging.debug(\"bid size = \" + str(slot_bids_filtered[-(bid+1)][\"size\"]))\n",
    "                        logging.debug(\"bid price = \" + str(slot_bids_filtered[-(bid+1)][\"price\"]))\n",
    "                        bid_capacity = slot_bids_filtered[-(bid+1)][\"size\"]\n",
    "                        accumulated_replaced_capacity += bid_capacity\n",
    "                        logging.debug(\"accumulated_replaced_capacity = \" + str( accumulated_replaced_capacity))\n",
    "                            \n",
    "                        if accumulated_replaced_capacity >= agents_bid_size:\n",
    "                            logging.debug(\"realistic bid size\")\n",
    "                            if slot_bids_filtered[-(bid+1)][\"indivisible\"] is False:\n",
    "                                logging.debug(\"bid is divisible, so current bids price is new settlement price\")\n",
    "                                new_settlement_price_DE = slot_bids_filtered[-(bid+1)][\"price\"]\n",
    "                            else:\n",
    "                                logging.debug(\"bid is INDIVISIBLE, so move one bids further is new settlement price\")\n",
    "                                accumulated_replaced_capacity -= bid_capacity\n",
    "                                continue\n",
    "                            logging.info(\"new_settlement_price_DE = \" + str( new_settlement_price_DE))\n",
    "                            # set boolean for auction win\n",
    "                            self.delivery_results[\"slots_won\"][slot] = 1\n",
    "                            # set settlement price for the current auctioned slot in slot_prices_DE list\n",
    "                            self.delivery_results[\"slot_prices_DE\"][slot] = new_settlement_price_DE\n",
    "                            break\n",
    "\n",
    "            logging.info(\"self.delivery_results['slots_won'] = \")\n",
    "            logging.info(\"\\n\".join(\"slot won: \\t{}\".format(k) for k in self.delivery_results[\"slots_won\"]))\n",
    "            logging.info(\"     agents bid_size = \")\n",
    "            logging.info(\"\\n\".join(\"size: \\t{}\".format(round(k) )for k in action_dict[\"size\"]))            \n",
    "            logging.info(\"self.delivery_results['slot_prices_DE'] = \")\n",
    "            logging.info(\"\\n\".join(\"price: \\t{}\".format(k) for k in self.delivery_results[\"slot_prices_DE\"]))\n",
    "            \n",
    "            \n",
    "    def _prepare_delivery(self):\n",
    "        '''\n",
    "        Was macht die funktion? \n",
    "        \n",
    "        parameter\n",
    "        \n",
    "        return\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # extend slot bid size format from 6 slots to 96 time steps\n",
    "        bid_sizes_list = []\n",
    "        for slot_x in range (0,6): \n",
    "            for time_step in range(0,16):\n",
    "                #bid_sizes_list.append(action_dict[\"size\"][slot_x])\n",
    "                bid_sizes_list.append(self.delivery_results[\"agents_bid_sizes_round\"][slot_x])\n",
    "        bid_sizes_all_slots = np.array(bid_sizes_list)\n",
    "        self.delivery_results[\"agents_bid_sizes_round_all_slots\"] = bid_sizes_all_slots\n",
    "        self.delivery_results[\"bid_sizes_all_slots\"] = bid_sizes_all_slots\n",
    "        logging.debug(\"self.delivery_results['bid_sizes_all_slots'] : \"  + str(self.delivery_results['bid_sizes_all_slots']))\n",
    "        \n",
    "        # initialize slots dict\n",
    "        self.delivery_results[\"delivered_slots\"] = {}\n",
    "        # initialize slots in dict \n",
    "        for slot in range (0,6):\n",
    "            self.delivery_results[\"delivered_slots\"][slot] = None\n",
    "            \n",
    "            \n",
    "    def _check_delivery_possible(self, agent_bid_size, vpp_total_FCR_slot):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "                \n",
    "        # 2. Probability of maximum Delivery Amount (74 % :  0 - 5 %  Capacity , 18 % : 5 - 10 % , 5% : 10-15% ( 97%: max 15 %)\n",
    "\n",
    "        max_delivery_share = random.choices(\n",
    "             population=[0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0],\n",
    "             weights=   [0.74, 0.18, 0.05, 0.02, 0.007, 0.001, 0.001, 0.001],\n",
    "             k=1\n",
    "         )\n",
    "\n",
    "        capacity_to_deliver = max_delivery_share[0] * agent_bid_size\n",
    "\n",
    "        logging.debug(\"agent_bid_size : \" + str(agent_bid_size))\n",
    "        logging.debug(\"max_delivery_share : \" + str(max_delivery_share[0]))\n",
    "        logging.debug(\"capacity_to_deliver : \" + str(capacity_to_deliver))\n",
    "        \n",
    "        # 3. Probability of successfull Delivery (100%: 10% of HPP Capacity = Probability Curve)\n",
    "\n",
    "        mean = 0 # symmetrical normal distribution at 0 \n",
    "        sd = self.maximum_possible_VPP_capacity/7\n",
    "\n",
    "        max_at_10_percent = norm.pdf(self.maximum_possible_VPP_capacity*0.1,mean,sd)\n",
    "        scale_factor = 1 / max_at_10_percent\n",
    "\n",
    "        logging.debug(\"max_at_10_percent = \" + str(max_at_10_percent))\n",
    "        logging.debug(\"scale_factor = \" + str(scale_factor))\n",
    "\n",
    "        # Plot between -max_power and max_power with .001 steps.\n",
    "        #x_axis = np.arange(-max_power, max_power, 0.001)\n",
    "        #plt.plot(x_axis, (norm.pdf(x_axis, mean, sd)) * scale_factor + shift_to_100)\n",
    "        #plt.show()\n",
    "\n",
    "        propab_of_delivery = round((norm.pdf(capacity_to_deliver, mean,sd) * scale_factor),3)\n",
    "\n",
    "        if propab_of_delivery > 1.0: \n",
    "            propab_of_delivery =  1.0\n",
    "\n",
    "        logging.debug(\"propab_of_delivery = \" + str(propab_of_delivery))\n",
    "\n",
    "        delivery_possible = random.choices(\n",
    "             population=[True, False],\n",
    "             weights=   [propab_of_delivery , (1-propab_of_delivery)],\n",
    "             k=1\n",
    "         )\n",
    "        delivery_possible = delivery_possible[0] # as bool is in list \n",
    "        \n",
    "        # 4. Check VPP Boundaries: In case of a very high or low operating point (nearly 100% or 0% power output of the HPP): \n",
    "        # then the delivery is not possible. \n",
    "        \n",
    "        # check if probability of delivery is high and capacity could be deliverd\n",
    "        if delivery_possible == True:\n",
    "            # when negative FCR :\n",
    "            if capacity_to_deliver < 0:\n",
    "                if (vpp_total_FCR_slot - abs(capacity_to_deliver)) < 0:\n",
    "                    logging.error(\"Error, FCR is smaller than vpp_total_FCR_slot\")\n",
    "                    delivery_possible = False\n",
    "            # if positive FCR\n",
    "            else:\n",
    "                if (vpp_total_FCR_slot + capacity_to_deliver) > self.maximum_possible_VPP_capacity: \n",
    "                    delivery_possible = False\n",
    "      \n",
    "        return delivery_possible\n",
    "\n",
    "    def _simulate_delivery(self, slot, action_dict): \n",
    "        logging.debug(\"Delivery Simulation for Slot No. \" + str(slot))\n",
    "        \n",
    "        #vpp_total_slot = self.delivery_results[\"vpp_total\"][slot *16 : (slot+1)*16]\n",
    "        vpp_total_FCR_slot = self.delivery_results[\"vpp_total_FCR\"][slot *16 : (slot+1)*16]\n",
    "        bid_sizes_per_slot = self.delivery_results[\"bid_sizes_all_slots\"][slot *16 : (slot+1)*16]\n",
    "        \n",
    "        logging.debug(\"vpp_total_FCR_slot \" + str(vpp_total_FCR_slot))\n",
    "        logging.debug(\"bid_sizes_per_slot \" + str(bid_sizes_per_slot))\n",
    "\n",
    "\n",
    "        delivery_possible = None\n",
    "        delivery_possible_list = []\n",
    "\n",
    "        # check for every timestep\n",
    "        for time_step in range(0, 16):\n",
    "        \n",
    "            agent_bid_size = bid_sizes_per_slot[time_step]\n",
    "           \n",
    "            logging.debug(\"vpp_total_FCR_slot[time_step] : \" + str(vpp_total_FCR_slot[time_step]))\n",
    "            logging.debug(\"bid_sizes_per_slot[time_step] : \" + str(bid_sizes_per_slot[time_step]))\n",
    "\n",
    "            # check if positive FCR could be provided \n",
    "            delivery_possible = self._check_delivery_possible(agent_bid_size, vpp_total_FCR_slot[time_step])\n",
    "            delivery_possible_list.append(delivery_possible)\n",
    "            # check if negative FCR could be provided \n",
    "            delivery_possible = self._check_delivery_possible(-agent_bid_size, vpp_total_FCR_slot[time_step])\n",
    "            delivery_possible_list.append(delivery_possible)\n",
    "\n",
    "        if all(delivery_possible_list): \n",
    "            total_delivery_possible = True \n",
    "        else: \n",
    "            total_delivery_possible = False \n",
    "            \n",
    "        logging.debug(\"total_delivery_possible for slot \" + str(slot) + \" : \" + str(total_delivery_possible))\n",
    "        self.delivery_results[\"delivered_slots\"][slot] = total_delivery_possible\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95eedd-db5c-4793-987a-3ff0c01b9b65",
   "metadata": {},
   "source": [
    "Idee\n",
    "\n",
    "1. Not considered: \n",
    "    - Probability of Delivery Length (60%: Maximum of 10sec, 15% 11-20sec, 5% 21-30, 5% 31-60, 5% 1-5 min)\n",
    "2. Considered\n",
    "    - Probability of Delivery Amount (50 % :  0 - 5 %  Capacity , 27,5 % : 5 - 10 % , 12% : 10-15% ( 90%: max 15 %)\n",
    "    - Probability of successfull Delivery (100%: 10% of HPP Capacity = Probability Curve)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "378a6ab2-4d0e-4d7d-bde3-87e7100c8406",
   "metadata": {},
   "source": [
    "\n",
    "def check_delivery_possible(capacity):\n",
    "\n",
    "    # 2. Probability of maximum Delivery Amount (74 % :  0 - 5 %  Capacity , 18 % : 5 - 10 % , 5% : 10-15% ( 97%: max 15 %)\n",
    "\n",
    "    max_delivery_share = random.choices(\n",
    "         population=[0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0],\n",
    "         weights=   [0.74, 0.18, 0.05, 0.02, 0.007, 0.001, 0.001, 0.001],\n",
    "         k=1\n",
    "     )\n",
    "\n",
    "    capacity_to_deliver = max_delivery_share[0] * agent_bid_size\n",
    "\n",
    "    print(\"agent_bid_size : \" + str(agent_bid_size))\n",
    "    print(\"max_delivery_share : \" + str(max_delivery_share[0]))\n",
    "    print(\"capacity_to_deliver : \" + str(capacity_to_deliver))\n",
    "\n",
    "    mean = 0 # symmetrical normal distribution at 0 \n",
    "    sd = 500 /7\n",
    "\n",
    "    max_at_10_percent = norm.pdf(500*0.1,mean,sd)\n",
    "    scale_factor = 1 / max_at_10_percent\n",
    "\n",
    "    print(\"max_at_10_percent = \" + str(max_at_10_percent))\n",
    "    print(\"scale_factor = \" + str(scale_factor))\n",
    "\n",
    "    # Plot between -max_power and max_power with .001 steps.\n",
    "    #x_axis = np.arange(-max_power, max_power, 0.001)\n",
    "    #plt.plot(x_axis, (norm.pdf(x_axis, mean, sd)) * scale_factor + shift_to_100)\n",
    "    #plt.show()\n",
    "\n",
    "    propab_of_delivery = round((norm.pdf(capacity, mean,sd) * scale_factor),3)\n",
    "\n",
    "    if propab_of_delivery > 1.0: \n",
    "        propab_of_delivery =  1.0\n",
    "\n",
    "    print(\"propab_of_delivery = \" + str(propab_of_delivery))\n",
    "\n",
    "    # 3. Probability of successfull Delivery (100%: 10% of HPP Capacity = Probability Curve)\n",
    "\n",
    "    delivery_possible = random.choices(\n",
    "         population=[True, False],\n",
    "         weights=   [propab_of_delivery , (1-propab_of_delivery)],\n",
    "         k=1\n",
    "     )\n",
    "    return delivery_possible[0]\n",
    "\n",
    "\n",
    "def simulate_delivery(): \n",
    "        delivery_possible = None\n",
    "        delivery_possible_list = []\n",
    "        bid_sizes_per_slot = [50,100,50, 50]\n",
    "        # check for every timestep\n",
    "        for time_step in range(0, 4):\n",
    "\n",
    "            agent_bid_size = bid_sizes_per_slot[time_step]\n",
    "\n",
    "            # check if positive FCR could be provided \n",
    "            delivery_possible = check_delivery_possible(agent_bid_size)\n",
    "            delivery_possible_list.append(delivery_possible)\n",
    "            # check if negative FCR could be provided \n",
    "            delivery_possible = check_delivery_possible(-agent_bid_size)\n",
    "            delivery_possible_list.append(delivery_possible)\n",
    "        print(delivery_possible_list)\n",
    "       \n",
    "        if all(delivery_possible_list):\n",
    "            total_delivery_possible = True \n",
    "        else: \n",
    "            total_delivery_possible = False \n",
    "        return total_delivery_possible\n",
    "            \n",
    "total_delivery_possible = simulate_delivery()\n",
    "print(\"total_delivery_possible for slot \" +  str(total_delivery_possible))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32db6c67-ec11-492b-b29d-773764e7dc09",
   "metadata": {},
   "source": [
    "## Register the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de7d899-f638-44cf-8c5b-3f80b0723019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "   \n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TRAIN-v1\",\n",
    "    entry_point=\"__main__:VPPBiddingEnv\",\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"training\"\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-EVAL-v1\",\n",
    "    entry_point=\"__main__:VPPBiddingEnv\",\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"eval\"\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TEST-v1\",\n",
    "    entry_point=\"__main__:VPPBiddingEnv\",\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"INFO\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"test\",\n",
    "           }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72671bc0-33dd-4453-81b9-d86c8979a5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "env_to_check = make('VPPBiddingEnv-TEST-v1')\n",
    "check_env(env_to_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b9778-4b51-48b3-9b93-8fee865a49ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stable Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105d3eb4-1556-46f9-81e1-af6956e07e0c",
   "metadata": {},
   "source": [
    "## DDPG: Deep Deterministic Policy Gradient (DDPG) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f86ac-d5b2-498d-922f-50f64b6c5843",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2970c20-6c2e-4392-8972-b657b734e7b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 697\n",
    "}\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"new_action_high_price\", \"new_action_high_size\", \"delivery_simulation_4.0\", \"DDPG\",\"training\", \"wind_config\", \"single vpp obs\", \"vpp config\", \"13MW config\", \"4kprice\", \"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = DDPG(config['policy'], env, action_noise=action_noise, verbose=1, tensorboard_log=f\"runs/ddpg\")\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4dcac2-e0b7-4251-9e97-bba435f4ee23",
   "metadata": {},
   "source": [
    "### Eval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce413bca-a219-4562-aed3-5852fc79aa8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=False,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"delivery_simulation_4.0\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 697\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\n",
    "                #\"global_step\": eval_env.logging_step\n",
    "                \"episode_reward\": reward,\n",
    "                \"episode\": i_episode},\n",
    "                step=eval_env.logging_step,\n",
    "                commit=True)\n",
    "            break\n",
    "            \n",
    "'''wandb.log({\"bid_submission_time\" : tbl,\n",
    "          \"global_step\": eval_env.logging_step},\n",
    "         commit=True)'''\n",
    "\n",
    "wandb.run.summary[\"bid_submission_time_table\"] = tbl\n",
    "\n",
    "eval_env.close()\n",
    "mean_run_reward = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_run_reward\"] = mean_run_reward\n",
    "print(\"Mean Run Reward: \" + str(mean_run_reward))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaefe16b-26ab-4e90-98ab-1e88c5e1ead8",
   "metadata": {},
   "source": [
    ".predict() parameters: \n",
    "    \n",
    "- deterministic (bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188eb313-1b3d-41ad-a23f-47c483933c7d",
   "metadata": {},
   "source": [
    "## Tuning DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba0c26-38dd-4452-b4d6-41b4ab31c641",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a56c13b-497b-4c6f-9def-83275da802b5",
   "metadata": {},
   "source": [
    "- policy = \"MlpPolicy\" , \"CnnPolicy\" , \"MultiInputPolicy\"\n",
    "- **learning_rate** = staic or range(1,0)\n",
    "- buffer_size (int) – size of the replay buffer\n",
    "- **learning_starts (int)** – how many steps of the model to collect transitions for before learning starts\n",
    "    -  For a fixed number of steps at the beginning (set with the start_steps keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration.\n",
    "- batch_size (int) – Minibatch size for each gradient update\n",
    "- **tau (float)** – the soft update coefficient (“Polyak update”, between 0 and 1)\n",
    "- gamma (float) – the discount factor\n",
    "- train_freq (Union[int, Tuple[int, str]]) – Update the model every train_freq steps. Alternatively pass a tuple of frequency and unit like (5, \"step\") or (2, \"episode\").\n",
    "- gradient_steps (int) – How many gradient steps to do after each rollout (see train_freq) Set to -1 means to do as many gradient steps as steps done in the environment during the rollout.\n",
    "- action_noise (Optional[ActionNoise]) – the action noise type (None by default), this can help for hard exploration problem. Cf common.noise for the different action noise type.\n",
    "    -  uncorrelated, mean-zero Gaussian noise works perfectly well. \n",
    "    -  To facilitate getting higher-quality training data, you may reduce the scale of the noise over the course of training. (We do not do this in our implementation, and keep noise scale fixed throughout.)\n",
    "\n",
    "\n",
    "- replay_buffer_class (Optional[ReplayBuffer]) – Replay buffer class to use (for instance HerReplayBuffer). If None, it will be automatically selected.\n",
    "- optimize_memory_usage (bool) – Enable a memory efficient variant of the replay buffer at a cost of more complexity. See https://github.com/DLR-RM/stable-baselines3/issues/37#issuecomment-637501195\n",
    "- create_eval_env (bool) – Whether to create a second environment that will be used for evaluating the agent periodically. (Only available when passing string for the environment)\n",
    "\n",
    "- seed (Optional[int]) – Seed for the pseudo random generators\n",
    "- _init_setup_model (bool) – Whether or not to build the network at the creation of the instance\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500fb8a-f122-4b69-ab72-bde202b131a5",
   "metadata": {},
   "source": [
    "stable_baselines3.ddpg.MlpPolicy Parameters\n",
    "- lr_schedule (Callable[[float], float]) – Learning rate schedule (could be constant)\n",
    "- n_critics (int) – Number of critic networks to create.\n",
    "\n",
    "stable_baselines3.ddpg.MlpPolicy.set_training_mode()\n",
    "- mode (bool) – if true, set to training mode, else set to evaluation mode\n",
    "\n",
    "stable_baselines3.ddpg.CnnPolicy\n",
    "\n",
    "stable_baselines3.ddpg.MultiInputPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698996b7-a960-43d1-82db-9d631368740b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hide all deprecation warnings from tensorflow\n",
    "#import tensorflow as tf\n",
    "#tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import optuna\n",
    "\n",
    "#from stable_baselines import PPO2\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import HerReplayBuffer\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "#from stable_baselines.common.evaluation import evaluate_policy\n",
    "#from stable_baselines.common.cmd_util import make_vec_env\n",
    "\n",
    "# https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb\n",
    "#from custom_env import GoLeftEnv\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "normal_action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "\n",
    "def optimize_ddpg(trial):\n",
    "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\n",
    "    \n",
    "    replay_buffer_class = trial.suggest_categorical(\"replay_buffer_class\", [\"HER\", \"None\"])\n",
    "    replay_buffer_class = {\"HER\": HerReplayBuffer, \"None\": None}[replay_buffer_class]\n",
    "    \n",
    "    action_noise = trial.suggest_categorical(\"action_noise\", [\"action_noise\", \"None\"])\n",
    "    action_noise = {\"action_noise\": normal_action_noise, \"None\": None}[action_noise]\n",
    "    \n",
    "    params =  {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.0001, 1.0), #default: 0.001\n",
    "        'learning_starts': int(trial.suggest_int('learning_starts', 0, 200, 10)),  #default: 100\n",
    "        'batch_size': int(trial.suggest_int('batch_size', 0, 200,10)),  #default: 100\n",
    "        'tau': trial.suggest_loguniform('tau', 0.001, 1.0), #default: 0.005\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.9, 0.9999), # default: gamma=0.99\n",
    "        'replay_buffer_class' : replay_buffer_class,\n",
    "        'action_noise' : action_noise\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return params\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def optimize_agent(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    \n",
    "    model_params = optimize_ddpg(trial)\n",
    "    \n",
    "    # init tracking experiment.\n",
    "    # hyper-parameters, trial id are stored.\n",
    "    config = dict(trial.params)\n",
    "    config[\"trial.number\"] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"RL-optuna\",\n",
    "        entity=\"jlu237\", \n",
    "        sync_tensorboard=True,\n",
    "        config=config,\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "    model = DDPG('MultiInputPolicy', env, verbose=0, tensorboard_log=f\"runs/ddpg\", seed = 1, **model_params)\n",
    "    model.learn(total_timesteps=697, log_interval=1)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "study = optuna.create_study()\n",
    "try:\n",
    "    study.optimize(optimize_agent, n_trials=20)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8736a02-dde8-40db-8cff-9cd88419d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env.observation_space.spaces[\"observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e45aee-535f-4356-89b5-879c3a94d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6b94a-ff9e-4216-b56e-68057b2c9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameters()[\"critic.optimizer\"][\"param_groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eb8130-edfd-4631-8607-9cc2be042a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_parameters()[\"actor.optimizer\"][\"param_groups\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530625de-8a8d-471f-9d11-ae2d3f3028e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt-get install swig cmake ffmpeg freeglut3-dev xvfb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef058b-4411-4126-bbde-8fc93e588a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative from araffin for optuna from: https://github.com/optuna/optuna-examples/blob/52ed3aff3e3e936be3873b5acc6ee3ccdadea914/rl/sb3_simple.py#L60\n",
    "\n",
    "\"\"\" Optuna example that optimizes the hyperparameters of\n",
    "a reinforcement learning agent using A2C implementation from Stable-Baselines3\n",
    "on a OpenAI Gym environment.\n",
    "\n",
    "This is a simplified version of what can be found in https://github.com/DLR-RM/rl-baselines3-zoo.\n",
    "\n",
    "You can run this example as follows:\n",
    "    $ python sb3_simple.py\n",
    "\n",
    "\"\"\"\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "\n",
    "import gym\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "N_TRIALS = 100\n",
    "N_STARTUP_TRIALS = 5\n",
    "N_EVALUATIONS = 2\n",
    "N_TIMESTEPS = int(2e4)\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_EPISODES = 3\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}\n",
    "\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"Sampler for A2C hyperparameters.\"\"\"\n",
    "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
    "    gae_lambda = 1.0 - trial.suggest_float(\"gae_lambda\", 0.001, 0.2, log=True)\n",
    "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"tiny\", \"small\"])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # Display true values\n",
    "    trial.set_user_attr(\"gamma_\", gamma)\n",
    "    trial.set_user_attr(\"gae_lambda_\", gae_lambda)\n",
    "    trial.set_user_attr(\"n_steps\", n_steps)\n",
    "\n",
    "    net_arch = [\n",
    "        {\"pi\": [64], \"vf\": [64]} if net_arch == \"tiny\" else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
    "    ]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
    "\n",
    "    return {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"policy_kwargs\": {\n",
    "            \"net_arch\": net_arch,\n",
    "            \"activation_fn\": activation_fn,\n",
    "            \"ortho_init\": ortho_init,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"Callback used for evaluating and reporting a trial.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    # Sample hyperparameters\n",
    "    kwargs.update(sample_a2c_params(trial))\n",
    "    # Create the RL model\n",
    "    model = A2C(**kwargs)\n",
    "    # Create env used for evaluation\n",
    "    eval_env = gym.make(ENV_ID)\n",
    "    # Create the callback that will periodically evaluate\n",
    "    # and report the performance\n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env, trial, n_eval_episodes=N_EVAL_EPISODES, eval_freq=EVAL_FREQ, deterministic=True\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set pytorch num threads to 1 for faster training\n",
    "    torch.set_num_threads(1)\n",
    "\n",
    "    sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "    # Do not prune before 1/3 of the max budget is used\n",
    "    pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3)\n",
    "\n",
    "    study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_TRIALS, timeout=600)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    print(\"  User attrs:\")\n",
    "    for key, value in trial.user_attrs.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7392f04-fc0f-4226-bf3a-aa3c85930d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py#L340\n",
    "\n",
    "def sample_ddpg_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sampler for DDPG hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 256, 512, 1024, 2048])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "    # Polyak coeff\n",
    "    tau = trial.suggest_categorical(\"tau\", [0.001, 0.005, 0.01, 0.02, 0.05, 0.08])\n",
    "\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 32, 64, 128, 256, 512])\n",
    "    gradient_steps = train_freq\n",
    "\n",
    "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
    "    noise_std = trial.suggest_uniform(\"noise_std\", 0, 1)\n",
    "\n",
    "    # NOTE: Add \"verybig\" to net_arch when tuning HER (see TD3)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    # activation_fn = trial.suggest_categorical('activation_fn', [nn.Tanh, nn.ReLU, nn.ELU, nn.LeakyReLU])\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [64, 64],\n",
    "        \"medium\": [256, 256],\n",
    "        \"big\": [400, 300],\n",
    "    }[net_arch]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"gamma\": gamma,\n",
    "        \"tau\": tau,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "    }\n",
    "\n",
    "    if noise_type == \"normal\":\n",
    "        hyperparams[\"action_noise\"] = NormalActionNoise(\n",
    "            mean=np.zeros(trial.n_actions), sigma=noise_std * np.ones(trial.n_actions)\n",
    "        )\n",
    "    elif noise_type == \"ornstein-uhlenbeck\":\n",
    "        hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(\n",
    "            mean=np.zeros(trial.n_actions), sigma=noise_std * np.ones(trial.n_actions)\n",
    "        )\n",
    "\n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "\n",
    "    return hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a014b4-6236-47aa-bc41-4d85774c679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552ea1dc-86b5-416b-8eae-7577b79a5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd rl-baselines3-zoo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf98dd4-0eb6-46aa-bd76-d20d304272b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r rl-baselines3-zoo/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925e8dd-b86e-49d7-ba0a-4cb548b03dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rl-baselines3-zoo/train.py --algo ddpg --env VPPBiddingEnv-TRAIN-v1 -n 697 -optimize --n-trials 5 --n-jobs -1 \\\n",
    "  --sampler tpe --pruner median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b70fb-2003-4c1c-a6da-83212073b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rl-baselines3-zoo/scripts/parse_study.py -i path/to/study.pkl --print-n-best-trials 10 --save-n-best-hyperparameters 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fb446-9ec5-4eb4-aebb-7f67edf5945e",
   "metadata": {},
   "source": [
    "## TD3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92227e40-1ef0-46a9-8b79-3c474b102033",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08083015-cb57-4510-a84c-9dd52124994c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "env = Monitor(env) \n",
    "env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"policy\": 'MultiInputPolicy',\n",
    "    \"total_timesteps\": 697\n",
    "}\n",
    "\n",
    "# The noise objects for DDPG\n",
    "n_actions = env.action_space.shape[-1]\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "wandb.init(\n",
    "    config=config,\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Training\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"new_action_high_price\", \"new_action_high_size\", \"delivery_simulation_4.0\", \"TD3\",\"training\", \"wind_config\", \"single vpp obs\", \"vpp config\", \"13MW config\", \"4kprice\", \"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"], \n",
    "    job_type=\"training\"\n",
    ")\n",
    "\n",
    "model = TD3(config['policy'], env, action_noise=action_noise, verbose=1, tensorboard_log=f\"runs/ddpg\")\n",
    "\n",
    "model.learn(total_timesteps=config['total_timesteps'],\n",
    "            log_interval=1,\n",
    "            callback=WandbCallback(\n",
    "                gradient_save_freq=1,\n",
    "                verbose=2))\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05bbeb-906b-4f7c-9473-52a5bce82c43",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3694d5-33a3-4c75-94e8-5d2f0869a038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"RL-VPP-Evaluation\",\n",
    "    #monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"TD3, \"\"wind_config\",\"eval\", \"single vpp obs\", \"vpp config\", \"price plots\",\"updated_reward\" , \"delivery_against_FCR\", \"pred_market_prices\"],\n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "\n",
    "tbl = wandb.Table(columns=[\"episode\", \"bid_submission_time\"])\n",
    "\n",
    "episodes = 697\n",
    "\n",
    "for i_episode in range(episodes):\n",
    "    observation = eval_env.reset()\n",
    "    for t in range(1):\n",
    "        eval_env.render(mode=\"human\")\n",
    "        logging.debug(\"observation : \" + str(observation))\n",
    "        action, _states = model.predict(observation)\n",
    "        observation, reward, done, info = eval_env.step(action)\n",
    "        if done:\n",
    "            print('Episode: {} Info: {}'.format(i_episode, info))\n",
    "            tbl.add_data(i_episode, info[\"bid_submission_time\"])\n",
    "            wandb.log({\"episode_reward\": reward,\n",
    "                       \"episode\": i_episode\n",
    "                      })\n",
    "            \n",
    "            break\n",
    "wandb.log({\"bid_submission_time\" : tbl})\n",
    "eval_env.close()\n",
    "mean_ep_rew = info[\"total_reward\"] / episodes\n",
    "\n",
    "wandb.run.summary[\"mean_ep_rew\"] = mean_ep_rew\n",
    "print(\"Mean Episode Reward: \" + str(mean_ep_rew))\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fe36f-dbe5-419c-9957-393ec2effb92",
   "metadata": {},
   "source": [
    "### Tuning TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41979c6d-ce66-4507-beff-762e6be633dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "\n",
    "def sample_td3_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for TD3 hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 200])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "    # Polyak coeff\n",
    "    tau = trial.suggest_categorical(\"tau\", [0.001, 0.005, 0.01, 0.02, 0.05, 0.08])\n",
    "\n",
    "    #train_freq = trial.suggest_categorical(\"train_freq\", [1, 4, 8, 16, 32, 64, 128, 256, 512])\n",
    "    #gradient_steps = train_freq\n",
    "\n",
    "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
    "    noise_std = trial.suggest_uniform(\"noise_std\", 0, 1)\n",
    "\n",
    "    # NOTE: Add \"verybig\" to net_arch when tuning HER\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    # activation_fn = trial.suggest_categorical('activation_fn', [nn.Tanh, nn.ReLU, nn.ELU, nn.LeakyReLU])\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [64, 64],\n",
    "        \"medium\": [256, 256],\n",
    "        \"big\": [400, 300],\n",
    "        # Uncomment for tuning HER\n",
    "        # \"verybig\": [256, 256, 256],\n",
    "    }[net_arch]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"gamma\": gamma,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        #\"train_freq\": train_freq,\n",
    "        #\"gradient_steps\": gradient_steps,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "        \"tau\": tau,\n",
    "    }\n",
    "    \n",
    "    n_actions = 12      \n",
    "    if noise_type == \"normal\":\n",
    "        hyperparams[\"action_noise\"] = NormalActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "    elif noise_type == \"ornstein-uhlenbeck\":\n",
    "        hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def optimize_agent_td3(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    model_params = sample_td3_params(trial)\n",
    "    \n",
    "    # init tracking experiment.\n",
    "    # hyper-parameters, trial id are stored.\n",
    "    config = dict(trial.params)\n",
    "    config[\"trial.number\"] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"RL-optuna\",\n",
    "        entity=\"jlu237\", \n",
    "        sync_tensorboard=True,\n",
    "        config=config,\n",
    "        tags=[\"TD3\"],\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "    \n",
    "    \n",
    "    model = TD3('MultiInputPolicy', env, verbose=0, tensorboard_log=f\"runs/td3\", seed = 1, **model_params)\n",
    "\n",
    "    model.learn(total_timesteps=697,\n",
    "                log_interval=1,\n",
    "                callback=WandbCallback(\n",
    "                    gradient_save_freq=1,\n",
    "                    verbose=0))\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "study = optuna.create_study()\n",
    "try:\n",
    "    study.optimize(optimize_agent_td3, n_trials=20)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d56d1b-4450-432c-a34f-57b0ab710879",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PPO - Proximal Policy Optimization algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def512a-0674-4c59-b45d-c32d9df7827b",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90b7e1e2-f72e-4762-ac56-59e7a3c4ee96",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MultiInputPolicy\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "env = make('VPPBiddingEnv-TRAIN-v1')\n",
    "\n",
    "callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=363, verbose=1)\n",
    "\n",
    "model = PPO(MultiInputPolicy, env, verbose=1) # the verbosity level: 0 no output, 1 info, 2 debug\n",
    "model.learn(total_timesteps=1, callback=callback_max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc36d3e-c566-425d-b680-9a608a06be13",
   "metadata": {},
   "source": [
    "#### Evaluate Agent"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3f2fd1a-b5d0-4419-8f53-e0e62e16cbc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "\n",
    "eval_env_monitor = Monitor(eval_env)   # won't work with vectorized enviroments, will throw cryptic errors\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env_monitor, n_eval_episodes=363)\n",
    "\n",
    "logging.warning(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "print(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04c634-c34b-4604-aca6-d43c32e95f24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## A2C - synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531abe3a-8e9d-4bd3-9526-7629687c1d89",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec11ef48-04c2-43c9-b826-cc4c51dd4dea",
   "metadata": {},
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env(\"VPPBiddingEnv-TRAIN-v1\", n_envs=1)\n",
    "\n",
    "model = A2C(policy = \"MultiInputPolicy\", env = env, verbose=2, n_steps=363)\n",
    "\n",
    "#callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=363, verbose=1)\n",
    "#model.learn(total_timesteps=1, callback=callback_max_episodes)\n",
    "model.learn(total_timesteps=1)\n",
    "\n",
    "#model.save(\"a2c_cartpole\")\n",
    "#del model # remove to demonstrate saving and loading\n",
    "#model = A2C.load(\"a2c_cartpole\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd37fc1f-f379-47e2-9995-3969d1bb3a95",
   "metadata": {},
   "source": [
    "#### Eval"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e16c1115-3274-4e2e-ac87-f38d284c558f",
   "metadata": {},
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "\n",
    "eval_env_monitor = Monitor(eval_env)   # won't work with vectorized enviroments, will throw cryptic errors\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env_monitor, n_eval_episodes=363)\n",
    "\n",
    "logging.warning(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "print(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095db4e-0406-480b-82b9-1f4253184e9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Other Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480d1cb-1706-4675-b832-91fc49eb38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee09cce-ebb4-43ae-9df3-43a20ecac7a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DQN -- needs Discrete Action Space. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d884313c-0069-457a-b11d-77ede91244bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=1, exploration_final_eps=0.1, target_update_interval=250)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=int(1e5))\n",
    "# Save the agent\n",
    "model.save(\"dqn_vpp\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "model = DQN.load(\"dqn_vpp\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf2c9ce1-90d8-446f-974a-51095ef6be00",
   "metadata": {},
   "source": [
    "\n",
    "# Use a separate environement for evaluation\n",
    "eval_env = VPPBiddingEnv(renewables_df = renewables_df,\n",
    "                    bids_df = bids_df,\n",
    "                    tenders_df = tenders_df,\n",
    "                    time_features_df = time_features_df,\n",
    "                    hist_window_size = hist_window_size,\n",
    "                    forecast_window_size = forecast_window_size,\n",
    "                    frame_bound = frame_bound,\n",
    "                    log_level = \"INFO\" # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "                   )\n",
    "\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a20097f-0144-4b82-8633-89798dc61bde",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b488227-3204-4d3a-b6b4-83c1cb2aa347",
   "metadata": {},
   "source": [
    "#### Run Episodes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "610385a0-1265-4713-b43f-173a9cd11e43",
   "metadata": {
    "tags": []
   },
   "source": [
    "env = make('VPPBiddingEnv-TEST-v1')\n",
    "\n",
    "\n",
    "episodes = 2\n",
    "\n",
    "for episode in range(episodes):\n",
    "    logging.info('Start of Episode:{} '.format(episode))\n",
    "    observation = env.reset()\n",
    "\n",
    "    \n",
    "    # timestep defined as: 1 step = 1 day.\n",
    "    for timestep in range(1):\n",
    "        #env.render()\n",
    "        #logging.info(observation)\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            logging.warning('Episode: {} Info: {}'.format(episode, info))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad8086-eeba-4912-8ba7-e545b3c7e2c0",
   "metadata": {},
   "source": [
    "### Check the Environment"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7c159c3-b39f-459e-9c6d-c2b04ba6d233",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "\n",
    "check_env(env)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62a69846-b26e-4b97-b7e5-3aad41c582a3",
   "metadata": {},
   "source": [
    "eval_env = VPPBiddingEnv(renewables_df = renewables_df,\n",
    "                    bids_df = bids_df,\n",
    "                    tenders_df = tenders_df,\n",
    "                    time_features_df = time_features_df,\n",
    "                    hist_window_size = hist_window_size,\n",
    "                    forecast_window_size = forecast_window_size,\n",
    "                    frame_bound = frame_bound,\n",
    "                    log_level = \"INFO\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "                    env_type = \"eval\"\n",
    "                   )\n",
    "\n",
    "obs = env_vec.reset()\n",
    "for i in range(0,10): \n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env_vec.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4ef8a-3bd0-41a0-b94b-fc063a62c1f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. Create a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "567f265b-9bf7-4bb6-bddc-649a320bd443",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6863b554-621c-46bf-b4bc-71c8a916ee85",
   "metadata": {},
   "source": [
    "states_hydro_historic = env.observation_space[\"historic_data\"][\"hydro_historic\"].shape\n",
    "states_wind_historic = env.observation_space[\"historic_data\"][\"wind_historic\"].shape\n",
    "\n",
    "actions = env.action_space[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b41edbd-ed15-49ad-9e38-9bb68522648d",
   "metadata": {},
   "source": [
    "display(states_hydro_historic)\n",
    "display(states_wind_historic)\n",
    "display(actions)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1ae7712-99d1-4f7e-8b4a-5d0b7cee19fa",
   "metadata": {},
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    # flatten? \n",
    "    model.add(Dense(24, activation='relu', input_shape=states))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba57d484-d2b2-4b08-af86-73db026a41e9",
   "metadata": {},
   "source": [
    "del model \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4fb5422-4f7e-4f0f-a0f7-88279749aa1e",
   "metadata": {},
   "source": [
    "model = build_model(states, actions)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19775f65-1db4-4429-8f32-8d876fb04526",
   "metadata": {},
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f603624e-f942-49b1-b963-679c80199088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. Build Agent with Keras-RL\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1e63485-59b3-49a4-9226-f766281e29fa",
   "metadata": {},
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "803f070f-f1d8-4277-9415-c9e1c4de5770",
   "metadata": {},
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d138070-3db1-4e7b-8e1b-389de9690c53",
   "metadata": {},
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59fcd11f-24bb-4ba2-b8bb-8e5e9c9fde40",
   "metadata": {},
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c03ec672-bef7-4294-a077-f1b5e7869758",
   "metadata": {},
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5cd21-515a-4c46-a642-11db503e2174",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4. Reloading Agent from Memory\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55adf8b9-702d-4332-872e-cbb87f6bf6c0",
   "metadata": {},
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21192001-f0e4-4115-885e-09da17843ce9",
   "metadata": {},
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e730c10-1662-482b-888e-ca877c5b144d",
   "metadata": {},
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='vpp-v0',\n",
    "    entry_point='gym_foo.envs:FooEnv',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "51cf3131-40c6-4931-a0c1-419c3790e743",
   "metadata": {
    "tags": []
   },
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "27080d1b-5c5f-4cf3-9d7b-12306188205e",
   "metadata": {},
   "source": [
    "dqn.load_weights('dqn_weights.h5f')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1504afef-d7aa-481b-8dba-f1dd022b727d",
   "metadata": {},
   "source": [
    "_ = dqn.test(env, nb_episodes=5, visualize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7941b248-1fc0-4aa2-8343-cabeb98dd490",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Archive\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a986ff9b-b88d-4dfe-a83a-00d8d723d8e5",
   "metadata": {},
   "source": [
    "flatdim(env.observation_space)\n",
    "flatten_space(env.complex_action_space)\n",
    "\n",
    "\n",
    "flattened_datapoint = flatten(env.complex_action_space, env.complex_action_space.sample())\n",
    "display(flattened_datapoint)\n",
    "\n",
    "unflattened_datapoint = unflatten(env.complex_action_space, env.action_space.sample())\n",
    "display(unflattened_datapoint)\n",
    "\n",
    "# check if flattened data point is in space\n",
    "\n",
    "flatten(env.observation_space, env.observation_space.sample())  in flatten_space(env.observation_space)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a5343c6-645b-4db9-8462-bc065ba6b2c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "    def _simulate_market(self, action):\n",
    "        \n",
    "        # market clearing algorithm:\n",
    "        \n",
    "        # for each slot \n",
    "        # get all bids\n",
    "        # bids to dict\n",
    "        # add bid from action \n",
    "        # bring in order by price \n",
    "        # accumulate capacities until demand is filled \n",
    "        # check if bid is in bid list \n",
    "            # if yes, set auciton_won = True and get SETTLEMENTCAPACITY_PRICE\n",
    "            # if no, set auciton_won = False\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "        \n",
    "        \n",
    "        # TODO: Market clearing algorithmus neu schreiben, Angebote aller Länder (ausser DÄNEMARK ?? ) müssen berücksichtigt werden, um gesatm demand zu füllen , erst dann steht preis für slot fest. \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for each slot \n",
    "        # get all bids\n",
    "        # bids to dict\n",
    "        # add bid from action \n",
    "        # bring in order by price\n",
    "        # accumulate capacities until demand is filled  FOR ALL COUNTRIES\n",
    "            # indivisible flag needs to be included and checked: indivisible offer needs to be fully included\n",
    "            # check for every country, if Core Portion and export limit are satisfied \n",
    "                # for each country \n",
    "                    # at least the core portion needs to be satisfied\n",
    "                    # if capacity is less than total demand, the settlement price for an underfilled country is the price of the last accepted bid\n",
    "                # at most the export limit needs to be satisfied (all bids for Country - demand of country)\n",
    "        \n",
    "        # set prices for all countries\n",
    "        \n",
    "        \n",
    "        ################################################\n",
    "        \n",
    "        # The optimisation algorithm calculates the optimal combination of FCR bids to be awarded under consideration of core shares and the maximum exchangeable FCR volumes (export limits of a country) with the goal to reduce total procurement cost of the cooperation. \n",
    "        \n",
    "        # 1. If no export limits or core share constraint are hit, one cross-border marginal price (CBMP) will be determined equalling the most expensive awarded bid in the overall cooperation. \n",
    "        # Exceptions from having one CBMP may occur once export limits or core share constraint of one or more countries of the cooperation are hit. In this case, an LMP will be determined based on the local awarded bids within a country.\n",
    "        \n",
    "        \n",
    "\n",
    "        def add_bid_to_acceppted_bids(bid,\n",
    "                                     accepted_bids,\n",
    "                                     sorted_bids_list_by_price,\n",
    "                                     accumulated_capacities,\n",
    "                                     LMPi=False):\n",
    "            country_prefix = bid[\"country\"]\n",
    "            # add the capacity of the bid to the accumulated capacity of a single country\n",
    "            accumulated_capacities[country_prefix + \"_capacity\"]  += bid[\"offered\"]\n",
    "\n",
    "            # add the capacity of each bid to the accumulated capacity\n",
    "            accumulated_capacities[\"total_capacity\"] += bid[\"offered\"]\n",
    "            if LMPi: \n",
    "                # if bid is evaluated for an LMPi\n",
    "                accumulated_capacities[country_prefix + \"_LMPi\"] = bid[\"price\"]\n",
    "            else:\n",
    "                # if it is a normal bid, the bids price is the new CBMP \n",
    "                accumulated_capacities[\"CBMP\"] = bid[\"price\"]\n",
    "            # add the bid to the accepted bids list\n",
    "            bid[\"allocated\"] = bid[\"offered\"]\n",
    "            accepted_bids.append(bid)\n",
    "            #print('bid[\"index\"] = %s ' % (bid[\"index\"]))\n",
    "            #print(\"agents_bid_index = %s\" % (agents_bid_index))\n",
    "            #print(\"accumulated_capacities[\"total_capacity\"] = %s\" % (accumulated_capacities[\"total_capacity\"]))\n",
    "\n",
    "            # remove bid from list to not iterate over it again when searching for limit constraint replacement bids\n",
    "            sorted_bids_list_by_price.remove(bid)\n",
    "\n",
    "            return accepted_bids, sorted_bids_list_by_price, accumulated_capacities\n",
    "            \n",
    "        \n",
    "        auction_bids = self.bids_df[self.market_start : self.market_end]\n",
    "        country_constraints = self.tenders_df[self.market_start : self.market_end]\n",
    "\n",
    "        \n",
    "        \n",
    "        for slot in range(0, len(self.slot_date_list)):\n",
    "            slot_date = self.slot_date_list[slot]\n",
    "            print(\"slot_date = %s\" % (slot_date))\n",
    "            slot_bids = auction_bids[slot_date : slot_date].reset_index(drop=True).reset_index(drop=False)\n",
    "            bids_list = slot_bids.to_dict('records')\n",
    "            \n",
    "            slot_constraints = country_constraints[slot_date : slot_date].reset_index(drop=True).reset_index(drop=False)\n",
    "            slot_constraints = slot_constraints.to_dict('records')[0]            \n",
    "\n",
    "            #print(\"bids_list = \")\n",
    "            #print(\"\\n\".join(\" \\t{}\".format(k) for k in bids_list))\n",
    "            \n",
    "            # get the lenght of the list ot create an index fo the agents bid that now will be added\n",
    "            agents_bid_index = len(bids_list)\n",
    "            \n",
    "            \n",
    "            # extract the bid size out of the agents action\n",
    "            # agents_bid_size = action[0][slot]\n",
    "            agents_bid_size = 10\n",
    "\n",
    "            \n",
    "            # extract the bid price out of the agents action\n",
    "            #agents_bid_price = action[1][slot]\n",
    "            agents_bid_price = 0\n",
    "            \n",
    "            \n",
    "            print(\"agents_bid_size = %s\" % (agents_bid_size))\n",
    "            print(\"agents_bid_price = %s\" % (agents_bid_price))\n",
    "            print(\"agents_bid_index = %s\" % (agents_bid_index))\n",
    "            # add the selected bid from the agent to the list of all bids\n",
    "            bids_list.append({'index': agents_bid_index, 'offered': agents_bid_size, 'price': agents_bid_price, \"country\": \"DE\", \"indivisible\": False})\n",
    "            # sort the list based on the price to later accumulate all bids' capacity (but ordered on price)\n",
    "            sorted_bids_list_by_price = sorted(bids_list, key=lambda x: x['price'])\n",
    "            \n",
    "            #print(\"sorted_bids_list_by_price = \")\n",
    "            #print(\"\\n\".join(\" \\t{}\".format(k) for k in sorted_bids_list_by_price))\n",
    "            \n",
    "            country_list = list(set([x['country'] for x in sorted_bids_list_by_price]))\n",
    "            LMPi_list = []\n",
    "            accepted_bids = []\n",
    "            slot_finished = False\n",
    "            \n",
    "            # CBMP = cross-border marginal price\n",
    "            # LMPi = Local Marginal Price of importing country\n",
    "            accumulated_capacities = {\n",
    "                 'total_capacity': 0,\n",
    "                 'CBMP': 0,\n",
    "                 'DE_capacity': 0,\n",
    "                 'DE_export': 0,\n",
    "                 'DE_core': 0,\n",
    "                 'DE_LMPi': 0,\n",
    "                 'BE_capacity': 0,\n",
    "                 'BE_export': 0,\n",
    "                 'BE_core': 0,\n",
    "                 'BE_LMPi': 0,\n",
    "                 'FR_capacity': 0,\n",
    "                 'FR_export': 0,\n",
    "                 'FR_core': 0,\n",
    "                 'FR_LMPi': 0,\n",
    "                 'NL_capacity': 0,\n",
    "                 'NL_export': 0,\n",
    "                 'NL_core': 0,\n",
    "                 'NL_LMPi': 0,\n",
    "                 'AT_capacity': 0,\n",
    "                 'AT_export': 0,\n",
    "                 'AT_core': 0,\n",
    "                 'AT_LMPi': 0,\n",
    "                 'CH_capacity': 0,\n",
    "                 'CH_export': 0,\n",
    "                 'CH_core': 0,\n",
    "                 'CH_LMPi': 0,\n",
    "                 'SI_capacity': 0,\n",
    "                 'SI_export': 0,\n",
    "                 'SI_core': 0,\n",
    "                 'SI_LMPi': 0,\n",
    "                 'DK_capacity': 0,\n",
    "                 'DK_export': 0,\n",
    "                 'DK_core': 0,\n",
    "                 'DK_LMPi': 0,\n",
    "            }\n",
    "\n",
    "            for bid in sorted_bids_list_by_price[:]:\n",
    "                # check if LMPi_list containts countries that need to be checked\n",
    "                if LMPi_list:\n",
    "                    # check if current bid is from country\n",
    "                    if bid[\"country\"] != LMPi_list[0]:\n",
    "                        # if not, go to next bid \n",
    "                        continue\n",
    "                    # if bid is from country\n",
    "                    else:\n",
    "                        # add bid to accepted bids\n",
    "                        accepted_bids, sorted_bids_list_by_price, accumulated_capacities = add_bid_to_acceppted_bids(bid,\n",
    "                                                                                                                     accepted_bids,\n",
    "                                                                                                                     sorted_bids_list_by_price,\n",
    "                                                                                                                     accumulated_capacities,\n",
    "                                                                                                                     LMPi = True\n",
    "                                                                                                                    )\n",
    "                        # after bid was added, remove country from LMPi list \n",
    "                        LMPi_list.pop(0)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                # add bid to accepted bids\n",
    "                accepted_bids, sorted_bids_list_by_price, accumulated_capacities = add_bid_to_acceppted_bids(bid,\n",
    "                                                                                                            accepted_bids,\n",
    "                                                                                                            sorted_bids_list_by_price,\n",
    "                                                                                                            accumulated_capacities,\n",
    "                                                                                                            LMPi = False)\n",
    "                    \n",
    "                # 2.1 Case of hitting a limit constraint\n",
    "                # It is important to understand that an export limit or core share constraint is hit whenever it influences the solution and not only when the quantity awarded in a country is exactly equal to the respective limit quantity of that country.\n",
    "                \n",
    "            \n",
    "                if accumulated_capacities[\"total_capacity\"] >= self.total_slot_FCR_demand:\n",
    "                    # if accumulated_capacities[\"total_capacity\"] is bigger than the demand, the last indvisible offer(s) need to be reduced\n",
    "                    \n",
    "                    # 2.1.2 Check if core share of every country is hit\n",
    "                    for country in country_list: \n",
    "                        print(\"accumulated_capacities for \" + country + \": \" +  str(accumulated_capacities[country + \"_capacity\"]))\n",
    "                        print(\"core constraint for \" + country + \": \" +  str(slot_constraints[country + \"_core\"]))\n",
    "                        if accumulated_capacities[country + \"_capacity\"] < slot_constraints[country + \"_core\"]: \n",
    "                            print(\"CORE SHARE TOO SMALL FOR COUNTRY: \" + country)\n",
    "                            \n",
    "                            del accepted_bids[-1]\n",
    "                            accumulated_capacities[\"total_capacity\"] -= bid[\"offered\"]\n",
    "                            LMPi_list.append(country)\n",
    "                            \n",
    "                            # TODO: set CBMP for all other countries. \n",
    "                            \n",
    "                            # continue step continues for loop \n",
    "                            continue\n",
    "                            \n",
    "                            # TODO: for every core-underfilled country the capacity needs to be filled and the LMPi has to be found \n",
    "                        \n",
    "                    # TODO: set allocated price of all bids :   bid[\"allocated\"\n",
    "                    \n",
    "                    # TODO: Check Over Procurement \n",
    "                    if accumulated_capacities[\"total_capacity\"] > self.total_slot_FCR_demand: \n",
    "                        # get the overfilled capacity (difference)\n",
    "                        overfilled_capacity = accumulated_capacities[\"total_capacity\"] - self.total_slot_FCR_demand\n",
    "                        # get list of accepted bids that are divisible (= that is not indivisible)\n",
    "                        accepted_bids_divisible = [bid for bid in accepted_bids if not bid['indivisible']]\n",
    "                        # get last accepted bid that is divisible\n",
    "                        accepted_bids_divisible[-1]\n",
    "                        \n",
    "                        # TODO: proceed with over procurement \n",
    "                        \n",
    "                        # TODO: place slot_finished boolean somewhere\n",
    "                        slot_finished = True\n",
    "                    \n",
    "                    # TODO: check if a country has a CBMP or LMPi \n",
    "                    \n",
    "                    # last accepted bid sets settlement price of auction\n",
    "                    settlement_price = bid[\"price\"]\n",
    "                    # set settlement price for the current auctioned slot in slot_prices list\n",
    "                    self.slot_prices[slot] = settlement_price\n",
    "                    \n",
    "                    if agents_bid_index in [x['index'] for x in accepted_bids]:\n",
    "                        # set boolean for auction win\n",
    "                        self.delivery_results[\"slots_won\"][slot] = 1\n",
    "                    \n",
    "                    print(\"accumulated_capacities['total_capacity'] = %s\" % (accumulated_capacities[\"total_capacity\"]))\n",
    "                    print(\"self.delivery_results[\"slots_won\"] = \")\n",
    "                    print(\"\\n\".join(\"won: \\t{}\".format(k) for k in self.delivery_results[\"slots_won\"]))\n",
    "                    print(\"self.slot_prices = \")\n",
    "                    print(\"\\n\".join(\"price: \\t{}\".format(k) for k in self.slot_prices))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                if slot_finished: \n",
    "                    break\n",
    "                    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9a8e9eb-20b9-4876-89ed-c007b5944012",
   "metadata": {},
   "source": [
    " '''\n",
    "                # add the selected bid from the agent to the list of all bids\n",
    "                bids_list.append({'index': agents_bid_index, 'offered': agents_bid_size, 'price': agents_bid_price, \"country\": \"DE\", \"indivisible\": False})\n",
    "                # sort the list based on the price to later accumulate all bids' capacity (but ordered on price)\n",
    "                sorted_bids_list_by_price = sorted(bids_list, key=lambda x: x['price'])\n",
    "\n",
    "\n",
    "                # as we dont habe enought data to simulate a realistic market clearing algorithm,\n",
    "                # we calcualte a new settlement price with\n",
    "                #prices = [x['price'] for x in accepted_bids]\n",
    "                #diffs = np.diff(np.array(list_a))\n",
    "                #mean_diff = sum(diffs) / len(diffs)\n",
    "                '''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24a17ef1-cc08-40e7-a443-c443fb43fe60",
   "metadata": {},
   "source": [
    "# ----------------------\n",
    "# other way of representing observation\n",
    "\n",
    "'''\n",
    "next_observation = Dict({\n",
    "    'historic_data': Dict({\n",
    "        \"hydro_historic\": Box(low, high, dtype=np.float32)\n",
    "        \"wind_historic\":  Box(low, high, dtype=np.float32)\n",
    "    }),\n",
    "    'forecast_data':  Dict({\n",
    "        \"hydro_forecast\": Box(low, high, dtype=np.float32),\n",
    "        \"wind_forecast\": Box(low, high, dtype=np.float32),\n",
    "        \"soc_forecast\": Box(low, high, dtype=np.float32)\n",
    "        # TODO should I keep the Battery state of charge? \n",
    "    }),\n",
    "    'market_data':  Dict({\n",
    "        \"market_demand\": Discrete(3), # for the demands 573, 562 and 555 MW\n",
    "        # TODO for 2021 its always 562, how to handle differetn years? maybe set it as a global constant? \n",
    "\n",
    "        \"predicted_market_prices\":  Box(low=0.0, high=1634.52, shape=(6, 1), dtype=np.float32), # for each slot, can be prices of same day last week \n",
    "    }),\n",
    "    'time_features':  Dict({\n",
    "        \"weekday\": Discrete(7), # for the days of the week\n",
    "        \"holiday\": Discrete(2), # holiday = 1, no holiday = 0\n",
    "        \"month\": Discrete(12), # for the month\n",
    "    })\n",
    "})\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "597d7512-37e6-4890-a54d-be709d93da09",
   "metadata": {},
   "source": [
    "demand = [{'index': 0, 'total': 1409.0, 'DE_demand': 562.0, 'DE_export': 168.0, 'DE_core': 169.0, 'BE_demand': 87.0, 'BE_export': 100.0, 'BE_core': 27.0, 'FR_demand': 508.0, 'FR_export': 152.0, 'FR_core': 153.0, 'NL_demand': 114.0, 'NL_export': 100.0, 'NL_core': 35.0, 'AT_demand': 71.0, 'AT_export': 100.0, 'AT_core': 22.0, 'CH_demand': 67.0, 'CH_export': 100.0, 'CH_core': 21.0, 'SI_demand': \"nan\", 'SI_export': \"nan\", 'SI_core': \"nan\", 'DK_demand': \"nan\", 'DK_export': \"nan\", 'DK_core': \"nan\"}]\n",
    "demand = demand[0]\n",
    "demand[\"DE_demand\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0815e140-fb16-4c0d-8151-e34a8fb715db",
   "metadata": {},
   "source": [
    "demand"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e6a7161f-351b-486b-a97a-aae5911281b0",
   "metadata": {},
   "source": [
    "accepted_bids = [{'index': 0, 'offered': 11, \"allocated\" : 10, 'price': 0.0, 'country': 'DE', 'indivisible': False}\n",
    ",{'index': 1, 'offered': 1, \"allocated\" : 10,'price': 0.0, 'country': 'DE', 'indivisible': True}\n",
    ",{'index': 2, 'offered': 1,\"allocated\" : 10, 'price': 0.0, 'country': 'FR', 'indivisible': True}\n",
    ",{'index': 3, 'offered': 4,\"allocated\" : 10, 'price': 0.0, 'country': 'DE', 'indivisible': False}]\n",
    "\n",
    "country_tenders = [{\"DE\": \n",
    "                    {\"total\": 0, \"export\": 11, \"core\" : 10},\n",
    "                    \"BE\": \n",
    "                    {\"total\": 0, \"export\": 11, \"core\" : 10}}]\n",
    "\n",
    "country_capacity =  {\"DE\": 555, \"BE\": 200}        \n",
    "\n",
    "# go through all accepted bids in reversed order sorted by price\n",
    "for bid in reversed(accepted_bids):\n",
    "    # if an order is divisible and can be divided...\n",
    "    if not bid[\"indivisible\"]:\n",
    "        # check if the offered capacity of the bid is a minimum of 1 bigger than the overfilled_capacity (so it can be substracted)\n",
    "        if bid[\"offered\"] > overfilled_capacity: \n",
    "            difference_to_core = country_tenders[bid[\"country\"]][\"core\"]\n",
    "            overfilled_capacity\n",
    "        bid[\"allocated\"] = bid[\"allocated\"]-1\n",
    "        \n",
    "        break\n",
    "\n",
    "print(accepted_bids)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9058fdfa-5cff-4def-8ca9-b27b005ea8d0",
   "metadata": {},
   "source": [
    "accepted_bids = [{'index': 0, 'offered': 11, \"allocated\" : 10, 'price': 1.0, 'country': 'DE', 'indivisible': False}\n",
    ",{'index': 1, 'offered': 1, \"allocated\" : 10,'price': 1.0, 'country': 'BE', 'indivisible': True}\n",
    ",{'index': 2, 'offered': 1,\"allocated\" : 10, 'price': 2.0, 'country': 'FR', 'indivisible': True}\n",
    ",{'index': 3, 'offered': 4,\"allocated\" : 10, 'price': 1.0, 'country': 'DE', 'indivisible': False}]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e8168ef-e48f-4350-a153-a9f65def5309",
   "metadata": {},
   "source": [
    "accepted_bids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "231dd6f0-6cd6-485d-8c13-dd58997bc96a",
   "metadata": {},
   "source": [
    "display([bid['price'] for bid in accepted_bids if bid['country']== \"DE\"])\n",
    "display([bid['price'] for bid in accepted_bids if bid['country']== \"DE\"][-1])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3babaed-33b8-46c5-9496-a50ffccdf957",
   "metadata": {},
   "source": [
    "set(([x['country'] for x in accepted_bids]))\n",
    "(([x['country'] for x in accepted_bids]))\n",
    "(([x['price'] for x in accepted_bids]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de4c949f-81c4-40f1-98c3-7fc80234e9af",
   "metadata": {},
   "source": [
    "unique_country_bids = list({v['country']:v for v in accepted_bids}.values())\n",
    "display(unique_country_bids)\n",
    "all_prices = [x['price'] for x in unique_country_bids]\n",
    "display(all_prices)\n",
    "\n",
    "cbmp = max(set(all_prices), key = all_prices.count)\n",
    "display(cbmp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e42f4a58-e40b-4827-bbee-3641d32dee92",
   "metadata": {},
   "source": [
    "unique_country_bids = [{'index': 665, 'offered': 1, 'price': 11.0, 'country': 'DE', 'settlement_price': 13.3, 'indivisible': False}, {'index': 598, 'offered': 1, 'price': 4.48, 'country': 'FR', 'settlement_price': 4.48, 'indivisible': False}, {'index': 677, 'offered': 23, 'price': 217.76, 'country': 'BE', 'settlement_price': 217.76, 'indivisible': True}, {'index': 671, 'offered': 1, 'price': 12.5, 'country': 'NL', 'settlement_price': 13.3, 'indivisible': False}, {'index': 673, 'offered': 5, 'price': 13.3, 'country': 'CH', 'settlement_price': 13.3, 'indivisible': False}, {'index': 670, 'offered': 2, 'price': 12.4, 'country': 'AT', 'settlement_price': 13.3, 'indivisible': False}, {'index': 675, 'offered': 1, 'price': 50.0, 'country': 'DK', 'settlement_price': 50.0, 'indivisible': True}]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39e532b3-6f0e-4223-829a-e529edcd90b0",
   "metadata": {},
   "source": [
    "display(unique_country_bids)\n",
    "all_prices = [d['settlement_price'] for d in unique_country_bids]\n",
    "display(all_prices)\n",
    "\n",
    "cbmp = max(set(all_prices), key = all_prices.count)\n",
    "display(cbmp)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "683c3e55-ad50-4b39-84e7-75b9a420d323",
   "metadata": {},
   "source": [
    "[bid['settlement_price'] for bid in accepted_bids if bid['country']== \"DE\"][0]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1826df77-bf0d-4c52-8a8c-414a24c19d59",
   "metadata": {},
   "source": [
    "#### test 2- self written eval - wont work "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5d838b5-1b67-404a-96b9-12f9b282da49",
   "metadata": {},
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"masterthesis\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"delivery_simulation\", \"delivery_plots\", \"episode_error\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "obs = eval_env.reset()\n",
    "for ep in range(0,363):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = eval_env.step(action)\n",
    "    eval_env.render(mode=\"human\")\n",
    "    \n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d816a671-b8bf-43c5-89e7-def7bc645972",
   "metadata": {},
   "source": [
    "#### test 1 - mean works, but no lgging is done. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b561b20-5794-4ce8-a224-f044099b7885",
   "metadata": {
    "tags": []
   },
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "eval_env = make('VPPBiddingEnv-EVAL-v1')\n",
    "\n",
    "eval_env_monitor = Monitor(eval_env)   # won't work with vectorized enviroments, will throw cryptic errors\n",
    "\n",
    "wandb.init(\n",
    "    sync_tensorboard=True,  # automatically upload SB3's tensorboard metrics to W&B\n",
    "    project=\"masterthesis\",\n",
    "    monitor_gym=True,       # automatically upload gym environements' videos\n",
    "    save_code=True,\n",
    "    entity=\"jlu237\", \n",
    "    tags=[\"delivery_simulation\", \"delivery_plots\", \"episode_error\"], \n",
    "    job_type=\"eval\"\n",
    ")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, \n",
    "                                          eval_env_monitor, \n",
    "                                          n_eval_episodes=363,\n",
    "                                          render = True\n",
    "                                         )\n",
    "\n",
    "logging.warning(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "print(\"Mean reward: {} , +/-  {} \".format(mean_reward , std_reward))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
