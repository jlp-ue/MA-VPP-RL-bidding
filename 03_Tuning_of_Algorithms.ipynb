{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e323e6-755a-4476-a70f-88b07c86dd07",
   "metadata": {},
   "source": [
    "# Tuning of all Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03607031-f4e8-46d0-928c-39b1a1cff8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "from stable_baselines3.common.noise import NormalActionNoise,OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import RecordEpisodeStatistics\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "from gym.envs.registration import register\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import subprocess\n",
    "from rl_zoo3 import linear_schedule\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from gym import make\n",
    "\n",
    "import warnings\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ba063-764f-4d1d-a340-2ad6d1d85a89",
   "metadata": {},
   "source": [
    "## Register the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c07d26c-fe13-450b-8c14-3e9382f969f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "   \n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TRAIN-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"training\",\n",
    "            'render_mode' :\"human\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TUNING-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"WARNING\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"training\",\n",
    "            'render_mode' :\"fast_training\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")\n",
    "\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-EVAL-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"DEBUG\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"eval\",\n",
    "            'render_mode' :\"human\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TUNING-EVAL-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"WARNING\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"eval\",\n",
    "            'render_mode' :\"fast_training\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"VPPBiddingEnv-TEST-v1\",\n",
    "    entry_point='vpp-gym.vpp_gym.envs.vpp_env:VPPBiddingEnv',\n",
    "    max_episode_steps=1,\n",
    "    kwargs={'config_path': \"vpp_config_4.json\",\n",
    "            'log_level' : \"INFO\", # \"DEBUG\" , \"INFO\" or  \"WARNING\"\n",
    "            'env_type' :\"test\",\n",
    "            'render_mode' :\"human\", # \"human\", \"fast_training\" or None\n",
    "           }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a018703-2b98-4312-a007-dabf896f8556",
   "metadata": {},
   "source": [
    "## Test the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e734a41-5452-4654-ac11-a2737749eff6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log_step: initial // slot: initial  log level = info\n",
      "log_step: initial // slot: initial  log level = warning\n",
      "log_step: 1 slot: None logging_step: 1\n",
      "log_step: 1 slot: None Bid Submission time (D-1) = 2020-07-02 05:00:00+00:00\n",
      "log_step: 1 slot: None Gate Closure time (D-1) = 2020-07-02 06:00:00+00:00\n",
      "log_step: 1 slot: None Historic Data Window: from 2020-07-01 05:00:00+00:00 to 2020-07-02 04:45:00+00:00 \n",
      "log_step: 1 slot: None Forecast Data Window: from 2020-07-02 22:00:00+00:00 to 2020-07-03 21:45:00+00:00 \n",
      "log_step: 1 slot: 0 Current Slot Time: (D) = 2020-07-02 22:00:00+00:00\n",
      "log_step: 1 slot: 0 agents_bid_size = 14\n",
      "log_step: 1 slot: 0 agents_bid_price = 1867.2778\n",
      "log_step: 1 slot: 0 settlement_price_DE : 16.67\n",
      "log_step: 1 slot: 0 self.activation_results['slots_won'] = \n",
      "log_step: 1 slot: 0\n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 1 slot: 0      agents bid_size = \n",
      "log_step: 1 slot: 0\n",
      "size: \t14 \n",
      "size: \t62 \n",
      "size: \t60 \n",
      "size: \t126 \n",
      "size: \t73 \n",
      "size: \t80\n",
      "log_step: 1 slot: 0 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 1 slot: 0\n",
      "price: \t16.67 \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 1 slot: 1 Current Slot Time: (D) = 2020-07-03 02:00:00+00:00\n",
      "log_step: 1 slot: 1 agents_bid_size = 62\n",
      "log_step: 1 slot: 1 agents_bid_price = 1816.881\n",
      "log_step: 1 slot: 1 settlement_price_DE : 19.06\n",
      "log_step: 1 slot: 1 self.activation_results['slots_won'] = \n",
      "log_step: 1 slot: 1\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 1 slot: 1      agents bid_size = \n",
      "log_step: 1 slot: 1\n",
      "size: \t14 \n",
      "size: \t62 \n",
      "size: \t60 \n",
      "size: \t126 \n",
      "size: \t73 \n",
      "size: \t80\n",
      "log_step: 1 slot: 1 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 1 slot: 1\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 1 slot: 2 Current Slot Time: (D) = 2020-07-03 06:00:00+00:00\n",
      "log_step: 1 slot: 2 agents_bid_size = 60\n",
      "log_step: 1 slot: 2 agents_bid_price = 1990.042\n",
      "log_step: 1 slot: 2 settlement_price_DE : 17.72\n",
      "log_step: 1 slot: 2 self.activation_results['slots_won'] = \n",
      "log_step: 1 slot: 2\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 1 slot: 2      agents bid_size = \n",
      "log_step: 1 slot: 2\n",
      "size: \t14 \n",
      "size: \t62 \n",
      "size: \t60 \n",
      "size: \t126 \n",
      "size: \t73 \n",
      "size: \t80\n",
      "log_step: 1 slot: 2 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 1 slot: 2\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \tNone \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 1 slot: 3 Current Slot Time: (D) = 2020-07-03 10:00:00+00:00\n",
      "log_step: 1 slot: 3 agents_bid_size = 126\n",
      "log_step: 1 slot: 3 agents_bid_price = 119.37836\n",
      "log_step: 1 slot: 3 settlement_price_DE : 16.67\n",
      "log_step: 1 slot: 3 self.activation_results['slots_won'] = \n",
      "log_step: 1 slot: 3\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone \n",
      "slot won: \tNone\n",
      "log_step: 1 slot: 3      agents bid_size = \n",
      "log_step: 1 slot: 3\n",
      "size: \t14 \n",
      "size: \t62 \n",
      "size: \t60 \n",
      "size: \t126 \n",
      "size: \t73 \n",
      "size: \t80\n",
      "log_step: 1 slot: 3 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 1 slot: 3\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \t16.67 \n",
      "price: \tNone \n",
      "price: \tNone\n",
      "log_step: 1 slot: 4 Current Slot Time: (D) = 2020-07-03 14:00:00+00:00\n",
      "log_step: 1 slot: 4 agents_bid_size = 73\n",
      "log_step: 1 slot: 4 agents_bid_price = 87.573425\n",
      "log_step: 1 slot: 4 settlement_price_DE : 18.0\n",
      "log_step: 1 slot: 4 self.activation_results['slots_won'] = \n",
      "log_step: 1 slot: 4\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \tNone\n",
      "log_step: 1 slot: 4      agents bid_size = \n",
      "log_step: 1 slot: 4\n",
      "size: \t14 \n",
      "size: \t62 \n",
      "size: \t60 \n",
      "size: \t126 \n",
      "size: \t73 \n",
      "size: \t80\n",
      "log_step: 1 slot: 4 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 1 slot: 4\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \t16.67 \n",
      "price: \t18.0 \n",
      "price: \tNone\n",
      "log_step: 1 slot: 5 Current Slot Time: (D) = 2020-07-03 18:00:00+00:00\n",
      "log_step: 1 slot: 5 agents_bid_size = 80\n",
      "log_step: 1 slot: 5 agents_bid_price = 4199.31\n",
      "log_step: 1 slot: 5 settlement_price_DE : 16.67\n",
      "log_step: 1 slot: 5 self.activation_results['slots_won'] = \n",
      "log_step: 1 slot: 5\n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1 \n",
      "slot won: \t-1\n",
      "log_step: 1 slot: 5      agents bid_size = \n",
      "log_step: 1 slot: 5\n",
      "size: \t14 \n",
      "size: \t62 \n",
      "size: \t60 \n",
      "size: \t126 \n",
      "size: \t73 \n",
      "size: \t80\n",
      "log_step: 1 slot: 5 self.activation_results['slot_settlement_prices_DE'] = \n",
      "log_step: 1 slot: 5\n",
      "price: \t16.67 \n",
      "price: \t19.06 \n",
      "price: \t17.72 \n",
      "price: \t16.67 \n",
      "price: \t18.0 \n",
      "price: \t16.67\n",
      "log_step: 1 slot: None Reward Overview:\n",
      "log_step: 1 slot: None self.activation_results['slots_won']: [-1, -1, -1, -1, -1, -1]\n",
      "log_step: 1 slot: None len(self.activation_results['slots_won']) : 6\n",
      "log_step: 1 slot: 0 slot no. 0\n",
      "log_step: 1 slot: 0 slot no 0 was lost\n",
      "log_step: 1 slot: 0 distance_to_settlement_price = 1850.60783203125\n",
      "log_step: 1 slot: 0 auction_reward = 0.28339234723174145\n",
      "log_step: 1 slot: 0 for slot no : 0\n",
      "log_step: 1 slot: 0 self.activation_results['slot_settlement_prices_DE'][slot]: 16.67\n",
      "log_step: 1 slot: 0 auction_reward: 0.28339234723174145\n",
      "log_step: 1 slot: 0 reservation_reward: 0\n",
      "log_step: 1 slot: 0 activation_reward: 0\n",
      "log_step: 1 slot: 0 slot_reward: 0.28339234723174145\n",
      "log_step: 1 slot: 0 weighted_slot_reward (slot_reward/3) = 0.09446411574391382\n",
      "log_step: 1 slot: 0 slot_profit: 0\n",
      "log_step: 1 slot: 1 slot no. 1\n",
      "log_step: 1 slot: 1 slot no 1 was lost\n",
      "log_step: 1 slot: 1 distance_to_settlement_price = 1797.8209814453126\n",
      "log_step: 1 slot: 1 auction_reward = 0.2916396216365251\n",
      "log_step: 1 slot: 1 for slot no : 1\n",
      "log_step: 1 slot: 1 self.activation_results['slot_settlement_prices_DE'][slot]: 19.06\n",
      "log_step: 1 slot: 1 auction_reward: 0.2916396216365251\n",
      "log_step: 1 slot: 1 reservation_reward: 0\n",
      "log_step: 1 slot: 1 activation_reward: 0\n",
      "log_step: 1 slot: 1 slot_reward: 0.2916396216365251\n",
      "log_step: 1 slot: 1 weighted_slot_reward (slot_reward/3) = 0.09721320721217504\n",
      "log_step: 1 slot: 1 slot_profit: 0\n",
      "log_step: 1 slot: 2 slot no. 2\n",
      "log_step: 1 slot: 2 slot no 2 was lost\n",
      "log_step: 1 slot: 2 distance_to_settlement_price = 1972.3219921875\n",
      "log_step: 1 slot: 2 auction_reward = 0.26489934394256254\n",
      "log_step: 1 slot: 2 for slot no : 2\n",
      "log_step: 1 slot: 2 self.activation_results['slot_settlement_prices_DE'][slot]: 17.72\n",
      "log_step: 1 slot: 2 auction_reward: 0.26489934394256254\n",
      "log_step: 1 slot: 2 reservation_reward: 0\n",
      "log_step: 1 slot: 2 activation_reward: 0\n",
      "log_step: 1 slot: 2 slot_reward: 0.26489934394256254\n",
      "log_step: 1 slot: 2 weighted_slot_reward (slot_reward/3) = 0.08829978131418752\n",
      "log_step: 1 slot: 2 slot_profit: 0\n",
      "log_step: 1 slot: 3 slot no. 3\n",
      "log_step: 1 slot: 3 slot no 3 was lost\n",
      "log_step: 1 slot: 3 distance_to_settlement_price = 102.70835693359375\n",
      "log_step: 1 slot: 3 auction_reward = 0.7745771189218438\n",
      "log_step: 1 slot: 3 for slot no : 3\n",
      "log_step: 1 slot: 3 self.activation_results['slot_settlement_prices_DE'][slot]: 16.67\n",
      "log_step: 1 slot: 3 auction_reward: 0.7745771189218438\n",
      "log_step: 1 slot: 3 reservation_reward: 0\n",
      "log_step: 1 slot: 3 activation_reward: 0\n",
      "log_step: 1 slot: 3 slot_reward: 0.7745771189218438\n",
      "log_step: 1 slot: 3 weighted_slot_reward (slot_reward/3) = 0.2581923729739479\n",
      "log_step: 1 slot: 3 slot_profit: 0\n",
      "log_step: 1 slot: 4 slot no. 4\n",
      "log_step: 1 slot: 4 slot no 4 was lost\n",
      "log_step: 1 slot: 4 distance_to_settlement_price = 69.57342529296875\n",
      "log_step: 1 slot: 4 auction_reward = 0.807099640248618\n",
      "log_step: 1 slot: 4 for slot no : 4\n",
      "log_step: 1 slot: 4 self.activation_results['slot_settlement_prices_DE'][slot]: 18.0\n",
      "log_step: 1 slot: 4 auction_reward: 0.807099640248618\n",
      "log_step: 1 slot: 4 reservation_reward: 0\n",
      "log_step: 1 slot: 4 activation_reward: 0\n",
      "log_step: 1 slot: 4 slot_reward: 0.807099640248618\n",
      "log_step: 1 slot: 4 weighted_slot_reward (slot_reward/3) = 0.269033213416206\n",
      "log_step: 1 slot: 4 slot_profit: 0\n",
      "log_step: 1 slot: 5 slot no. 5\n",
      "log_step: 1 slot: 5 slot no 5 was lost\n",
      "log_step: 1 slot: 5 distance_to_settlement_price = 4182.64005859375\n",
      "log_step: 1 slot: 5 auction_reward = 0.007030565158881563\n",
      "log_step: 1 slot: 5 for slot no : 5\n",
      "log_step: 1 slot: 5 self.activation_results['slot_settlement_prices_DE'][slot]: 16.67\n",
      "log_step: 1 slot: 5 auction_reward: 0.007030565158881563\n",
      "log_step: 1 slot: 5 reservation_reward: 0\n",
      "log_step: 1 slot: 5 activation_reward: 0\n",
      "log_step: 1 slot: 5 slot_reward: 0.007030565158881563\n",
      "log_step: 1 slot: 5 weighted_slot_reward (slot_reward/3) = 0.0023435217196271876\n",
      "log_step: 1 slot: 5 slot_profit: 0\n",
      "log_step: 1 total_step_reward (sum of all weighted_slot_reward ) = 0.8095462123800574\n",
      "log_step: 1 total_weighted_step_reward (= total_step_reward / 6) for all 6 slots : 0.13492436873000957\n",
      "log_step: 1 step_profit (sum of all slot_profit) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# It will check your custom environment and output additional warnings if needed\n",
    "env_to_check = make('VPPBiddingEnv-TEST-v1', render_mode=None)\n",
    "check_env(env_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759c221-88c2-477e-81b5-919a3d708720",
   "metadata": {},
   "source": [
    "## Globals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67303de9-3fd4-482a-9218-47b5df3876ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_TAGS = [\"tuning_3\"]\n",
    "EXPERIMENT_TIMESTEPS = 2785 #2785 #how many episodes to train\n",
    "N_TRIALS = 2 #20 #how many experiments to run\n",
    "N_STARTUP_TRIALS = 2 # 20 #how long to use random sampling before using TPESampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b272c9e-93ab-45a7-b31c-d4ba47539849",
   "metadata": {},
   "source": [
    "## Offline Training and later sync logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c53c9a-c971-4545-887b-c6197dc93182",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"0cea1eee5f42654eca0de365f0acca116367c9b4\"\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893eb40b-3616-49c7-b0e6-661297ace9f2",
   "metadata": {},
   "source": [
    "## Tuning of Algorithms "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54884c31-cd68-4643-bc7a-fb2032095518",
   "metadata": {},
   "source": [
    "### HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76342922-378e-427a-8389-1be3c84aa5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_her_params(trial,hyperparams):\n",
    "    \"\"\"\n",
    "    Sampler for HerReplayBuffer hyperparams.\n",
    "    :param trial:\n",
    "    :parma hyperparams:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    her_kwargs = trial.her_kwargs.copy()\n",
    "    her_kwargs[\"n_sampled_goal\"] = trial.suggest_int(\"n_sampled_goal\", 1, 5)\n",
    "    her_kwargs[\"goal_selection_strategy\"] = trial.suggest_categorical(\n",
    "        \"goal_selection_strategy\", [\"final\", \"episode\", \"future\"]\n",
    "    )\n",
    "    her_kwargs[\"online_sampling\"] = trial.suggest_categorical(\"online_sampling\", [True, False])\n",
    "    hyperparams[\"replay_buffer_kwargs\"] = her_kwargs\n",
    "    return hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d998b65-14c2-4013-9e1d-3f3145e4aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS_SAMPLER = {\n",
    "    \"A2C\": sample_a2c_params,\n",
    "    #\"ARS\": sample_ars_params,\n",
    "    \"DDPG\": sample_ddpg_params,\n",
    "    #\"DQN\": sample_dqn_params,\n",
    "    #\"QRDQN\": sample_qrdqn_params,\n",
    "    \"SAC\": sample_sac_params,\n",
    "    \"TQC\": sample_tqc_params,\n",
    "    \"PPO\": sample_ppo_params,\n",
    "    \"R_PPO\": sample_rppo_params,\n",
    "    \"TD3\": sample_td3_params,\n",
    "    \"TRPO\": sample_trpo_params,\n",
    "}\n",
    "\n",
    "ALGOS = {\n",
    "    \"A2C\": A2C,\n",
    "    \"DDPG\": DDPG,\n",
    "    #\"DQN\": DQN,\n",
    "    \"PPO\": PPO,\n",
    "    \"SAC\": SAC,\n",
    "    \"TD3\": TD3,\n",
    "    # SB3 Contrib,\n",
    "    #\"ARS\": ARS,\n",
    "    #\"QRDQN\": QRDQN,\n",
    "    \"TQC\": TQC,\n",
    "    \"TRPO\": TRPO,\n",
    "    \"R_PPO\": RecurrentPPO,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c44c8a0-79e6-445b-9c12-4f0266186940",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=EXPERIMENT_TIMESTEPS // 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdd76255-8847-4cf7-bce4-b0cc7556088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    algo = study.user_attrs[\"algo\"]\n",
    "    nan_encountered = False\n",
    "    try: \n",
    "        \n",
    "\n",
    "        model_params = HYPERPARAMS_SAMPLER[algo](trial)\n",
    "\n",
    "        # init tracking experiment.\n",
    "        # hyper-parameters, trial id are stored.\n",
    "        config = dict(trial.params)\n",
    "        config[\"trial.number\"] = trial.number\n",
    "        wandb.init(\n",
    "            project=\"RL-optuna\",\n",
    "            entity=\"jlu237\", \n",
    "            sync_tensorboard=True,\n",
    "            config=config,\n",
    "            tags=[algo] + EXPERIMENT_TAGS,\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "        env = make('VPPBiddingEnv-TUNING-v1')\n",
    "        env = Monitor(env) \n",
    "        env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "        \n",
    "        if algo == \"R_PPO\": \n",
    "            model = ALGOS[algo]('MultiInputLstmPolicy', env, verbose=0, seed = 1, **model_params)\n",
    "        else:\n",
    "            model = ALGOS[algo]('MultiInputPolicy', env, verbose=0,  seed = 1, **model_params)\n",
    "            \n",
    "        print(model_params)\n",
    "    \n",
    "        # -------------- TRAINING -----------------\n",
    "        model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                    log_interval=1,\n",
    "                    progress_bar = True,\n",
    "                    callback=WandbCallback(\n",
    "                        gradient_save_freq=1,\n",
    "                        verbose=0))\n",
    "        \n",
    "        # -------------- EVALUATION -----------------\n",
    "        eval_env = make('VPPBiddingEnv-TUNING-EVAL-v1')\n",
    "        eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "        episodes = 140\n",
    "        for i_episode in range(episodes):\n",
    "            observation = eval_env.reset()\n",
    "            if algo == \"R_PPO\":\n",
    "                lstm_states = None\n",
    "                num_envs = 1\n",
    "                # Episode start signals are used to reset the lstm states\n",
    "                episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "                for t in range(1):\n",
    "                    eval_env.render()\n",
    "                    #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "                    action, lstm_states = model.predict(observation, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "                    observation, reward, dones, info = eval_env.step(action)\n",
    "                    episode_starts = dones\n",
    "                    if dones:\n",
    "                        break\n",
    "            else: \n",
    "                for t in range(1):\n",
    "                    eval_env.render()\n",
    "                    #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "                    action, _states = model.predict(observation, deterministic = True)\n",
    "                    observation, reward, done, info = eval_env.step(action)\n",
    "                    if done:\n",
    "                        break\n",
    "        total_reward_test = info[\"total_reward\"]\n",
    "        total_profit_test = info[\"total_profit\"]\n",
    "\n",
    "        mean_episode_reward_test = info[\"total_reward\"] / episodes\n",
    "        mean_episode_profit_test = info[\"total_profit\"] / episodes\n",
    "\n",
    "        print(\"Total Reward on Test Set: \" + str(total_reward_test))\n",
    "        print(\"Total Profit on Test Set: \" + str(total_profit_test))\n",
    "        print(\"Mean Episode Reward: \" + str(mean_episode_reward_test))\n",
    "        print(\"Mean Episode Profit: \" + str(mean_episode_profit_test))\n",
    "\n",
    "        wandb.log({\"total_reward_test\": total_reward_test, \n",
    "                   \"total_profit_test\": total_profit_test, \n",
    "                   \"mean_episode_reward_test\": mean_episode_reward_test,\n",
    "                   \"mean_episode_profit_test\": mean_episode_profit_test,\n",
    "                })\n",
    "        wandb.finish()\n",
    "        eval_env.close()\n",
    "\n",
    "        return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "        \n",
    "    finally:        \n",
    "        # Free memory\n",
    "        env.close()\n",
    "        \n",
    "    if nan_encountered: \n",
    "        return float(\"nan\")\n",
    "\n",
    "    return total_reward_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa370148-126b-499c-9ec4-ac20b7b4bda9",
   "metadata": {},
   "source": [
    "### A2C "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31eb215d-d102-4efc-a74a-3c727649b843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_a2c_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for a2c hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    trial.using_her_replay_buffer = False\n",
    "    \n",
    "    n_steps_list = []\n",
    "    n_step = round(EXPERIMENT_TIMESTEPS/3)\n",
    "\n",
    "    while n_step > 1:\n",
    "        n_steps_list.append(n_step)\n",
    "        n_step = round(n_step/3)\n",
    "    \n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    normalize_advantage = trial.suggest_categorical(\"normalize_advantage\", [False, True])\n",
    "    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5])\n",
    "    # Toggle PyTorch RMS Prop (different from TF one, cf doc)\n",
    "    use_rms_prop = trial.suggest_categorical(\"use_rms_prop\", [False, True])\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n",
    "    #n_steps = trial.suggest_categorical(\"n_steps\", n_steps_list)\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [2,3,4,5,6,7,8,9,10])\n",
    "    #n_steps = 2\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    ent_coef = trial.suggest_categorical('ent_coef', [0.1, 0.05, 0.025, 0.01, 0.001, 0.0001,  0.00000001])\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0.3, 1)\n",
    "    \n",
    "    # ------- policy_kwargs --------\n",
    "    lr_schedule = trial.suggest_categorical(\"lr_schedule\", [\"linear\", \"constant\"])\n",
    "    use_sde = trial.suggest_categorical(\"use_sde\", [False, True])\n",
    "    if use_sde is True:\n",
    "        sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 0, 1, 2, 3])\n",
    "        log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "    sde_net_arch = trial.suggest_categorical(\"sde_net_arch\", [None, \"tiny\", \"small\"])\n",
    "    full_std = trial.suggest_categorical(\"full_std\", [False, True])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "\n",
    "    # NOTE: Add \"verybig\" to net_arch when tuning HER\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    # ------- policy_kwargs --------\n",
    "\n",
    "    if lr_schedule == \"linear\":\n",
    "        learning_rate = linear_schedule(learning_rate)\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [dict(pi=[64, 64], vf=[64, 64])],\n",
    "        \"medium\": [dict(pi=[256, 256], vf=[256, 256])],\n",
    "        \"big\": [dict(pi=[400, 400], vf=[400, 400])],\n",
    "    }[net_arch]\n",
    "    \n",
    "    sde_net_arch = {\n",
    "         None: None,\n",
    "         \"tiny\": [64],\n",
    "         \"small\": [64, 64],\n",
    "    }[sde_net_arch]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "    \n",
    "    hyperparams = {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"normalize_advantage\": normalize_advantage,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"use_rms_prop\": use_rms_prop,\n",
    "        \"use_sde\": use_sde,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"policy_kwargs\": dict(\n",
    "            net_arch=net_arch,\n",
    "            full_std=full_std,\n",
    "            activation_fn=activation_fn,\n",
    "            sde_net_arch=sde_net_arch,\n",
    "            ortho_init=ortho_init,\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "        \n",
    "    if use_sde is True:\n",
    "        hyperparams[\"sde_sample_freq\"] = sde_sample_freq\n",
    "        hyperparams[\"policy_kwargs\"][\"log_std_init\"] = log_std_init\n",
    " \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d5565d2-9d61-4fda-a98d-ab395aa2e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMS_SAMPLER = {\n",
    "    \"A2C\": sample_a2c_params,\n",
    "    #\"ARS\": sample_ars_params,\n",
    "    ##\"DDPG\": sample_ddpg_params,\n",
    "    #\"DQN\": sample_dqn_params,\n",
    "    #\"QRDQN\": sample_qrdqn_params,\n",
    "    ##\"SAC\": sample_sac_params,\n",
    "    ##\"TQC\": sample_tqc_params,\n",
    "    ##\"PPO\": sample_ppo_params,\n",
    "    ##\"R_PPO\": sample_rppo_params,\n",
    "    ##\"TD3\": sample_td3_params,\n",
    "    ##\"TRPO\": sample_trpo_params,\n",
    "}\n",
    "\n",
    "ALGOS = {\n",
    "    \"A2C\": A2C,\n",
    "    ##\"DDPG\": DDPG,\n",
    "    #\"DQN\": DQN,\n",
    "    ##\"PPO\": PPO,\n",
    "    ##\"SAC\": SAC,\n",
    "    ##\"TD3\": TD3,\n",
    "    # SB3 Contrib,\n",
    "    #\"ARS\": ARS,\n",
    "    #\"QRDQN\": QRDQN,\n",
    "    ##\"TQC\": TQC,\n",
    "    ##\"TRPO\": TRPO,\n",
    "    ##\"R_PPO\": RecurrentPPO,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e32b090b-a9b2-40d0-be66-ebb95c44d3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-15 15:44:57,605]\u001b[0m A new study created in memory with name: no-name-033c319d-9379-40ff-bac6-7d191b13cdbb\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f237d115579444e19b090c0b38b8c70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 3, 'gamma': 0.995, 'gae_lambda': 0.95, 'learning_rate': 0.9802493533201759, 'ent_coef': 0.001, 'normalize_advantage': False, 'max_grad_norm': 0.9, 'use_rms_prop': False, 'use_sde': True, 'vf_coef': 0.7502462221465176, 'policy_kwargs': {'net_arch': [{'pi': [256, 256], 'vf': [256, 256]}], 'full_std': False, 'activation_fn': <class 'torch.nn.modules.activation.ReLU'>, 'sde_net_arch': None, 'ortho_init': True, 'log_std_init': -2.7341152711700327}, 'sde_sample_freq': -1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2022-11-15 15:46:12,764]\u001b[0m Trial 0 failed because of the following error: ValueError('Expected parameter scale (Tensor of shape (256, 12)) of distribution Normal(loc: torch.Size([256, 12]), scale: torch.Size([256, 12])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\\n        [nan, nan, nan,  ..., nan, nan, nan],\\n        [nan, nan, nan,  ..., nan, nan, nan],\\n        ...,\\n        [nan, nan, nan,  ..., nan, nan, nan],\\n        [nan, nan, nan,  ..., nan, nan, nan],\\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MulBackward0>)')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/8y/13f6tls56kz7j9jbb8j_dy7r0000gq/T/ipykernel_10274/1375961096.py\", line 38, in optimize_agent\n",
      "    model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py\", line 203, in learn\n",
      "    return super().learn(\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 262, in learn\n",
      "    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py\", line 160, in collect_rollouts\n",
      "    self.policy.reset_noise(env.num_envs)\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/policies.py\", line 516, in reset_noise\n",
      "    self.action_dist.sample_weights(self.log_std, batch_size=n_envs)\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/distributions.py\", line 489, in sample_weights\n",
      "    self.weights_dist = Normal(th.zeros_like(std), std)\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/torch/distributions/normal.py\", line 50, in __init__\n",
      "    super(Normal, self).__init__(batch_shape, validate_args=validate_args)\n",
      "  File \"/Users/Jan-Lukas.Pflaum/.virtualenvs/thesis/lib/python3.8/site-packages/torch/distributions/distribution.py\", line 55, in __init__\n",
      "    raise ValueError(\n",
      "ValueError: Expected parameter scale (Tensor of shape (256, 12)) of distribution Normal(loc: torch.Size([256, 12]), scale: torch.Size([256, 12])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter scale (Tensor of shape (256, 12)) of distribution Normal(loc: torch.Size([256, 12]), scale: torch.Size([256, 12])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MulBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m study\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA2C\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(optimize_agent, n_trials\u001b[38;5;241m=\u001b[39mN_TRIALS, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10800\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInterrupted by keyboard.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis feature will be removed in v4.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/optuna/study/_optimize.py:264\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36moptimize_agent\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_params)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# -------------- TRAINING -----------------\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXPERIMENT_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWandbCallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgradient_save_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# -------------- EVALUATION -----------------\u001b[39;00m\n\u001b[1;32m     46\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m make(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVPPBiddingEnv-TUNING-EVAL-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/a2c/a2c.py:203\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m: A2CSelf,\n\u001b[1;32m    191\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    201\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m A2CSelf:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:262\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    258\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 262\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continue_training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/on_policy_algorithm.py:160\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Sample new weights for the state dependent exploration\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde:\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_rollout_start()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_steps \u001b[38;5;241m<\u001b[39m n_rollout_steps:\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/policies.py:516\u001b[0m, in \u001b[0;36mActorCriticPolicy.reset_noise\u001b[0;34m(self, n_envs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03mSample new weights for the exploration matrix.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m:param n_envs:\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, StateDependentNoiseDistribution), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset_noise() is only available when using gSDE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 516\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:489\u001b[0m, in \u001b[0;36mStateDependentNoiseDistribution.sample_weights\u001b[0;34m(self, log_std, batch_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03mSample weights for the noise exploration matrix,\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03musing a centered Gaussian distribution.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;124;03m:param batch_size:\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    488\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_std(log_std)\n\u001b[0;32m--> 489\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_dist \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# Reparametrization trick to pass gradients\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexploration_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_dist\u001b[38;5;241m.\u001b[39mrsample()\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/torch/distributions/normal.py:50\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNormal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/thesis/lib/python3.8/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m(Distribution, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter scale (Tensor of shape (256, 12)) of distribution Normal(loc: torch.Size([256, 12]), scale: torch.Size([256, 12])) to satisfy the constraint GreaterThan(lower_bound=0.0), but found invalid values:\ntensor([[nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        ...,\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan],\n        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<MulBackward0>)"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "study.set_user_attr(\"algo\", \"A2C\")\n",
    "\n",
    "try:\n",
    "    study.optimize(optimize_agent, n_trials=N_TRIALS, timeout=10800)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5bf214-6ccc-49b9-90fb-b015a67e64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup \n",
    "\n",
    "%%time\n",
    "\n",
    "def sample_a2c_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for a2c hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    trial.using_her_replay_buffer = False\n",
    "    \n",
    "    n_steps_list = []\n",
    "    n_step = round(EXPERIMENT_TIMESTEPS/3)\n",
    "\n",
    "    while n_step > 1:\n",
    "        n_steps_list.append(n_step)\n",
    "        n_step = round(n_step/3)\n",
    "    \n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    normalize_advantage = trial.suggest_categorical(\"normalize_advantage\", [False, True])\n",
    "    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5])\n",
    "    # Toggle PyTorch RMS Prop (different from TF one, cf doc)\n",
    "    use_rms_prop = trial.suggest_categorical(\"use_rms_prop\", [False, True])\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n",
    "    #n_steps = trial.suggest_categorical(\"n_steps\", n_steps_list)\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [2,3,4,5,6,7,8,9,10])\n",
    "    #n_steps = 2\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    ent_coef = trial.suggest_categorical('ent_coef', [0.1, 0.05, 0.025, 0.01, 0.001, 0.0001,  0.00000001])\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0.3, 1)\n",
    "    \n",
    "    # ------- policy_kwargs --------\n",
    "    lr_schedule = trial.suggest_categorical(\"lr_schedule\", [\"linear\", \"constant\"])\n",
    "    use_sde = trial.suggest_categorical(\"use_sde\", [False, True])\n",
    "    if use_sde is True:\n",
    "        sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 0, 1, 2, 3])\n",
    "        log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "    sde_net_arch = trial.suggest_categorical(\"sde_net_arch\", [None, \"tiny\", \"small\"])\n",
    "    full_std = trial.suggest_categorical(\"full_std\", [False, True])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "\n",
    "    # NOTE: Add \"verybig\" to net_arch when tuning HER\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    # ------- policy_kwargs --------\n",
    "\n",
    "    if lr_schedule == \"linear\":\n",
    "        learning_rate = linear_schedule(learning_rate)\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [dict(pi=[64, 64], vf=[64, 64])],\n",
    "        \"medium\": [dict(pi=[256, 256], vf=[256, 256])],\n",
    "        \"big\": [dict(pi=[400, 400], vf=[400, 400])],\n",
    "    }[net_arch]\n",
    "    \n",
    "    sde_net_arch = {\n",
    "         None: None,\n",
    "         \"tiny\": [64],\n",
    "         \"small\": [64, 64],\n",
    "    }[sde_net_arch]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "    \n",
    "    hyperparams = {\n",
    "        \"n_steps\": n_steps,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"normalize_advantage\": normalize_advantage,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"use_rms_prop\": use_rms_prop,\n",
    "        \"use_sde\": use_sde,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"policy_kwargs\": dict(\n",
    "            net_arch=net_arch,\n",
    "            full_std=full_std,\n",
    "            activation_fn=activation_fn,\n",
    "            sde_net_arch=sde_net_arch,\n",
    "            ortho_init=ortho_init,\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "        \n",
    "    if use_sde is True:\n",
    "        hyperparams[\"sde_sample_freq\"] = sde_sample_freq\n",
    "        hyperparams[\"policy_kwargs\"][\"log_std_init\"] = log_std_init\n",
    " \n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "    \n",
    "def optimize_agent_a2c(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try: \n",
    "        model_params = sample_a2c_params(trial)\n",
    "\n",
    "        # init tracking experiment.\n",
    "        # hyper-parameters, trial id are stored.\n",
    "        config = dict(trial.params)\n",
    "        config[\"trial.number\"] = trial.number\n",
    "        wandb.init(\n",
    "            project=\"RL-optuna\",\n",
    "            entity=\"jlu237\", \n",
    "            sync_tensorboard=True,\n",
    "            config=config,\n",
    "            tags=[\"A2C\"] + EXPERIMENT_TAGS,\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "        env = make('VPPBiddingEnv-TUNING-v1')\n",
    "        env = Monitor(env) \n",
    "        env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "        model = A2C('MultiInputPolicy', env, verbose=0,  seed = 1, **model_params)\n",
    "        print(model_params)\n",
    "    \n",
    "        # -------------- TRAINING -----------------\n",
    "        model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                    log_interval=1,\n",
    "                    progress_bar = True,\n",
    "                    callback=WandbCallback(\n",
    "                        gradient_save_freq=1,\n",
    "                        verbose=0))\n",
    "        \n",
    "        # -------------- EVALUATION -----------------\n",
    "        eval_env = make('VPPBiddingEnv-TUNING-EVAL-v1')\n",
    "        eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "        episodes = 140\n",
    "        for i_episode in range(episodes):\n",
    "            observation = eval_env.reset()\n",
    "            for t in range(1):\n",
    "                eval_env.render()\n",
    "                #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "                action, _states = model.predict(observation, deterministic = True)\n",
    "                observation, reward, done, info = eval_env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "        total_reward_test = info[\"total_reward\"]\n",
    "        total_profit_test = info[\"total_profit\"]\n",
    "        total_monthly_profit_test = info[\"total_profit_monthly\"]\n",
    "\n",
    "        mean_episode_reward_test = info[\"total_reward\"] / episodes\n",
    "        mean_episode_profit_test = info[\"total_profit\"] / episodes\n",
    "\n",
    "        print(\"Total Reward on Test Set: \" + str(total_reward_test))\n",
    "        print(\"Total Profit on Test Set: \" + str(total_profit_test))\n",
    "        print(\"Total Monthly Profit on Test Set: \" + str(total_monthly_profit_test))\n",
    "        print(\"Mean Episode Reward: \" + str(mean_episode_reward_test))\n",
    "        print(\"Mean Episode Profit: \" + str(mean_episode_profit_test))\n",
    "\n",
    "        wandb.log({\"total_reward_test\": total_reward_test, \n",
    "                   \"total_profit_test\": total_profit_test, \n",
    "                   \"total_monthly_profit_test\": total_monthly_profit_test, \n",
    "                   \"mean_episode_reward_test\": mean_episode_reward_test,\n",
    "                   \"mean_episode_profit_test\": mean_episode_profit_test,\n",
    "                })\n",
    "        \n",
    "        wandb.finish()\n",
    "        eval_env.close()\n",
    "\n",
    "        return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "        \n",
    "    finally:        \n",
    "        # Free memory\n",
    "        env.close()\n",
    "        \n",
    "    if nan_encountered: \n",
    "        return float(\"nan\")\n",
    "\n",
    "    return total_reward_test\n",
    "\n",
    "    \n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=EXPERIMENT_TIMESTEPS // 3)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(optimize_agent(algo=\"A2C\"), n_trials=N_TRIALS, timeout=10800)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a41d5-ecd4-4cef-827e-97d9f2dd071a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tuning TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6effaa8-4364-4935-b8df-65e559f918b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_td3_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for TD3 hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    trial.using_her_replay_buffer = False\n",
    "\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 200])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "    tau = trial.suggest_categorical(\"tau\", [0.001, 0.005, 0.01, 0.02, 0.05, 0.08])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 2, 8, 32])\n",
    "    #gradient_steps = train_freq \n",
    "    gradient_steps = trial.suggest_categorical(\"gradient_steps\", [-1, 1, 2, 8, 32, 256])\n",
    "    learning_starts = trial.suggest_categorical('learning_starts', [0, 1, 10, 20, 100, 200]) \n",
    "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
    "    noise_std = trial.suggest_uniform(\"noise_std\", 0, 1)\n",
    "    \n",
    "    policy_delay = trial.suggest_categorical(\"policy_delay\", [ 1, 2, 5])\n",
    "    target_policy_noise = trial.suggest_categorical(\"target_policy_noise\", [0.1, 0.2, 0.3])\n",
    "\n",
    "    if trial.using_her_replay_buffer: \n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\", \"verybig\"])\n",
    "    else:\n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    \n",
    "    net_arch = {\n",
    "        \"small\": [64, 64],\n",
    "        \"medium\": [256, 256],\n",
    "        \"big\": [400, 300],\n",
    "        \"verybig\": [256, 256, 256],\n",
    "    }[net_arch]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"tau\": tau,\n",
    "        \"gamma\": gamma,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"policy_delay\" : policy_delay,\n",
    "        \"target_policy_noise\": target_policy_noise,\n",
    "        \"policy_kwargs\": dict(\n",
    "            net_arch=net_arch, \n",
    "            activation_fn=activation_fn\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    n_actions = 12      \n",
    "    if noise_type == \"normal\":\n",
    "        hyperparams[\"action_noise\"] = NormalActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "    elif noise_type == \"ornstein-uhlenbeck\":\n",
    "        hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "def optimize_agent_td3(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    model_params = sample_td3_params(trial)\n",
    "    \n",
    "    config = dict(trial.params)\n",
    "    config[\"trial.number\"] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"RL-optuna\",\n",
    "        entity=\"jlu237\", \n",
    "        sync_tensorboard=True,\n",
    "        config=config,\n",
    "        tags=[\"TD3\"]+EXPERIMENT_TAGS,\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    env = make('VPPBiddingEnv-TUNING-v1')\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "    model = TD3('MultiInputPolicy', env, verbose=0, seed = 1, **model_params)\n",
    "    print(model_params)\n",
    "    model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                log_interval=1,\n",
    "                progress_bar = True,\n",
    "                callback=WandbCallback(\n",
    "                    gradient_save_freq=1,\n",
    "                    verbose=0))\n",
    "    \n",
    "    # -------------- EVALUATION -----------------\n",
    "    eval_env = make('VPPBiddingEnv-TUNING-EVAL-v1')\n",
    "    eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "    episodes = 140\n",
    "    for i_episode in range(episodes):\n",
    "        observation = eval_env.reset()\n",
    "        for t in range(1):\n",
    "            eval_env.render()\n",
    "            #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "            action, _states = model.predict(observation, deterministic = True)\n",
    "            observation, reward, done, info = eval_env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "    eval_env.close()\n",
    "    \n",
    "    total_reward_test = info[\"total_reward\"]\n",
    "    total_profit_test = info[\"total_profit\"]\n",
    "\n",
    "    mean_episode_reward_test = info[\"total_reward\"] / episodes\n",
    "    mean_episode_profit_test = info[\"total_profit\"] / episodes\n",
    "\n",
    "    print(\"Total Reward on Test Set: \" + str(total_reward_test))\n",
    "    print(\"Total Profit on Test Set: \" + str(total_profit_test))\n",
    "    print(\"Mean Episode Reward: \" + str(mean_episode_reward_test))\n",
    "    print(\"Mean Episode Profit: \" + str(mean_episode_profit_test))\n",
    "    \n",
    "    wandb.log({\"total_reward_test\": total_reward_test, \n",
    "               \"total_profit_test\": total_profit_test, \n",
    "               \"mean_episode_reward_test\": mean_episode_reward_test,\n",
    "               \"mean_episode_profit_test\": mean_episode_profit_test,\n",
    "            })\n",
    "    wandb.finish()\n",
    "    \n",
    "    return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "    \n",
    "    return total_reward_test\n",
    "    \n",
    "    \n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=EXPERIMENT_TIMESTEPS // 3)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(optimize_agent_td3, n_trials=N_TRIALS, timeout=10800)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d4b7e-e6a7-4e9d-ab5c-c79f3545f429",
   "metadata": {},
   "source": [
    "### Tuning SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33b71c4-de8b-4cf4-8d79-fe1ca8dd41c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_sac_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for SAC hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    trial.using_her_replay_buffer = False\n",
    "\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 200])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "    tau = trial.suggest_categorical(\"tau\", [0.001, 0.005, 0.01, 0.02, 0.05, 0.08])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 2, 8, 32])\n",
    "    # gradient_steps takes too much time\n",
    "    gradient_steps = trial.suggest_categorical(\"gradient_steps\", [-1, 1, 2, 8, 32, 256])\n",
    "    learning_starts = trial.suggest_categorical('learning_starts', [0, 1, 10, 20, 100, 200]) \n",
    "\n",
    "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
    "    noise_std = trial.suggest_uniform(\"noise_std\", 0, 1)\n",
    "    policy_delay = trial.suggest_categorical(\"policy_delay\", [ 1, 2, 5])\n",
    "    target_policy_noise = trial.suggest_categorical(\"target_policy_noise\", [0.1, 0.2, 0.3])\n",
    "       \n",
    "    ent_coef = trial.suggest_categorical('ent_coef', ['auto',  'auto_0.1', 0.5, 0.1, 0.05, 0.01, 0.0001])\n",
    "\n",
    "    if ent_coef == 'auto' or 'auto_0.1':\n",
    "        target_entropy = trial.suggest_categorical('target_entropy', ['auto', 10 , 5, 1, 0, -1, -5, -10])\n",
    "\n",
    "    if trial.using_her_replay_buffer: \n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\", \"verybig\"])\n",
    "    else:\n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    \n",
    "    \n",
    "    net_arch = {\n",
    "        \"small\": [64, 64],\n",
    "        \"medium\": [256, 256],\n",
    "        \"big\": [400, 300],\n",
    "        \"verybig\": [256, 256, 256],\n",
    "    }[net_arch]\n",
    "\n",
    "\n",
    "    # ------- policy_kwargs --------\n",
    "    use_sde = trial.suggest_categorical(\"use_sde\", [False, True])\n",
    "    if use_sde is True:\n",
    "        sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 0, 1, 2, 3])\n",
    "        log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "    \n",
    "    #activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "    # --------------------\n",
    " \n",
    "    hyperparams = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"tau\": tau,\n",
    "        \"gamma\": gamma,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch, \n",
    "                              activation_fn=activation_fn\n",
    "                             ),\n",
    "    }\n",
    "    \n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "        \n",
    "    if use_sde is True:\n",
    "        hyperparams[\"sde_sample_freq\"] = sde_sample_freq\n",
    "        hyperparams[\"policy_kwargs\"][\"log_std_init\"] = log_std_init\n",
    "\n",
    "        \n",
    "    if ent_coef == 'auto' or '‘auto_0.1':\n",
    "        hyperparams[\"target_entropy\"] = target_entropy\n",
    "\n",
    "    n_actions = 12      \n",
    "    if noise_type == \"normal\":\n",
    "        hyperparams[\"action_noise\"] = NormalActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "    elif noise_type == \"ornstein-uhlenbeck\":\n",
    "        \n",
    "        hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "def optimize_agent_sac(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try: \n",
    "        model_params = sample_sac_params(trial)\n",
    "\n",
    "        # init tracking experiment.\n",
    "        # hyper-parameters, trial id are stored.\n",
    "        config = dict(trial.params)\n",
    "        config[\"trial.number\"] = trial.number\n",
    "        wandb.init(\n",
    "            project=\"RL-optuna\",\n",
    "            entity=\"jlu237\", \n",
    "            sync_tensorboard=True,\n",
    "            config=config,\n",
    "            tags=[\"SAC\"]+EXPERIMENT_TAGS,\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "        env = make('VPPBiddingEnv-TUNING-v1')\n",
    "        env = Monitor(env) \n",
    "        env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "        model = SAC('MultiInputPolicy', env, verbose=0, seed = 1, **model_params)\n",
    "        print(model_params)\n",
    "    \n",
    "        # -------------- TRAINING -----------------\n",
    "        model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                    log_interval=1,\n",
    "                    progress_bar = True,\n",
    "                    callback=WandbCallback(\n",
    "                        gradient_save_freq=1,\n",
    "                        verbose=0))\n",
    "        \n",
    "        # -------------- EVALUATION -----------------\n",
    "        eval_env = make('VPPBiddingEnv-TUNING-EVAL-v1')\n",
    "        eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "        episodes = 140\n",
    "        for i_episode in range(episodes):\n",
    "            observation = eval_env.reset()\n",
    "            for t in range(1):\n",
    "                eval_env.render()\n",
    "                #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "                action, _states = model.predict(observation, deterministic = True)\n",
    "                observation, reward, done, info = eval_env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "        total_reward_test = info[\"total_reward\"]\n",
    "        total_profit_test = info[\"total_profit\"]\n",
    "\n",
    "        mean_episode_reward_test = info[\"total_reward\"] / episodes\n",
    "        mean_episode_profit_test = info[\"total_profit\"] / episodes\n",
    "\n",
    "        print(\"Total Reward on Test Set: \" + str(total_reward_test))\n",
    "        print(\"Total Profit on Test Set: \" + str(total_profit_test))\n",
    "        print(\"Mean Episode Reward: \" + str(mean_episode_reward_test))\n",
    "        print(\"Mean Episode Profit: \" + str(mean_episode_profit_test))\n",
    "\n",
    "        wandb.log({\"total_reward_test\": total_reward_test, \n",
    "                   \"total_profit_test\": total_profit_test, \n",
    "                   \"mean_episode_reward_test\": mean_episode_reward_test,\n",
    "                   \"mean_episode_profit_test\": mean_episode_profit_test,\n",
    "                })\n",
    "        wandb.finish()\n",
    "        eval_env.close()\n",
    "\n",
    "        return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "        \n",
    "    finally:        \n",
    "        # Free memory\n",
    "        env.close()\n",
    "        \n",
    "    if nan_encountered: \n",
    "        return float(\"nan\")\n",
    "\n",
    "    return total_reward_test\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=EXPERIMENT_TIMESTEPS // 3)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(optimize_agent_sac, n_trials=N_TRIALS, timeout=10800)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a673c-6b9a-4eda-9fe4-43afefcdd0f3",
   "metadata": {},
   "source": [
    "### Tuning DDPG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd0699-c01c-42a2-b39b-147a8a0d7036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_ddpg_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for DDPG hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    trial.using_her_replay_buffer = False\n",
    "\n",
    "    \n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [int(1e4), int(1e5), int(1e6)])\n",
    "    learning_starts = trial.suggest_categorical('learning_starts', [0, 1, 10, 20, 100, 200]) \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 200])\n",
    "    tau = trial.suggest_categorical(\"tau\", [0.001, 0.005, 0.01, 0.02, 0.05, 0.08])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    train_freq = trial.suggest_categorical(\"train_freq\", [1, 2, 8, 32])\n",
    "    # gradient_steps takes too much time\n",
    "    gradient_steps = trial.suggest_categorical(\"gradient_steps\", [-1, 1, 2, 8, 32])\n",
    "    noise_type = trial.suggest_categorical(\"noise_type\", [\"ornstein-uhlenbeck\", \"normal\", None])\n",
    "    noise_std = trial.suggest_uniform(\"noise_std\", 0, 1)\n",
    "    \n",
    "    if trial.using_her_replay_buffer: \n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\", \"verybig\"])\n",
    "    else:\n",
    "        net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "        \n",
    "    net_arch = {\n",
    "        \"small\": [64, 64],\n",
    "        \"medium\": [256, 256],\n",
    "        \"big\": [400, 300],\n",
    "        \"verybig\": [256, 256, 256],\n",
    "    }[net_arch]\n",
    "    \n",
    "    # ------- policy_kwargs --------\n",
    "    #activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "    # --------------------\n",
    "\n",
    "    hyperparams = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"learning_starts\": learning_starts,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"tau\": tau,\n",
    "        \"gamma\": gamma,\n",
    "        \"train_freq\": train_freq,\n",
    "        \"gradient_steps\": gradient_steps,\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch, \n",
    "                              activation_fn=activation_fn\n",
    "                             ),\n",
    "    }\n",
    "\n",
    "    n_actions = 12      \n",
    "    if noise_type == \"normal\":\n",
    "        hyperparams[\"action_noise\"] = NormalActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "    elif noise_type == \"ornstein-uhlenbeck\":\n",
    "        \n",
    "        hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(\n",
    "            mean=np.zeros(n_actions), sigma=noise_std * np.ones(n_actions)\n",
    "        )\n",
    "\n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "def optimize_agent_ddpg(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try: \n",
    "        model_params = sample_ddpg_params(trial)\n",
    "\n",
    "        # init tracking experiment.\n",
    "        # hyper-parameters, trial id are stored.\n",
    "        config = dict(trial.params)\n",
    "        config[\"trial.number\"] = trial.number\n",
    "        wandb.init(\n",
    "            project=\"RL-optuna\",\n",
    "            entity=\"jlu237\", \n",
    "            sync_tensorboard=True,\n",
    "            config=config,\n",
    "            tags=[\"DDPG\"] + EXPERIMENT_TAGS,\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "        env = make('VPPBiddingEnv-TUNING-v1')\n",
    "        env = Monitor(env) \n",
    "        env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "        model = DDPG('MultiInputPolicy', env, verbose=0,  seed = 1, **model_params)\n",
    "        print(model_params)\n",
    "    \n",
    "        # -------------- TRAINING -----------------\n",
    "        model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                    log_interval=1,\n",
    "                    progress_bar = True,\n",
    "                    callback=WandbCallback(\n",
    "                        gradient_save_freq=1,\n",
    "                        verbose=0))\n",
    "        \n",
    "        # -------------- EVALUATION -----------------\n",
    "        eval_env = make('VPPBiddingEnv-TUNING-EVAL-v1')\n",
    "        eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "        episodes = 140\n",
    "        for i_episode in range(episodes):\n",
    "            observation = eval_env.reset()\n",
    "            for t in range(1):\n",
    "                eval_env.render()\n",
    "                #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "                action, _states = model.predict(observation, deterministic = True)\n",
    "                observation, reward, done, info = eval_env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "        total_reward_test = info[\"total_reward\"]\n",
    "        total_profit_test = info[\"total_profit\"]\n",
    "\n",
    "        mean_episode_reward_test = info[\"total_reward\"] / episodes\n",
    "        mean_episode_profit_test = info[\"total_profit\"] / episodes\n",
    "\n",
    "        print(\"Total Reward on Test Set: \" + str(total_reward_test))\n",
    "        print(\"Total Profit on Test Set: \" + str(total_profit_test))\n",
    "        print(\"Mean Episode Reward: \" + str(mean_episode_reward_test))\n",
    "        print(\"Mean Episode Profit: \" + str(mean_episode_profit_test))\n",
    "\n",
    "        wandb.log({\"total_reward_test\": total_reward_test, \n",
    "                   \"total_profit_test\": total_profit_test, \n",
    "                   \"mean_episode_reward_test\": mean_episode_reward_test,\n",
    "                   \"mean_episode_profit_test\": mean_episode_profit_test,\n",
    "                })\n",
    "        wandb.finish()\n",
    "        eval_env.close()\n",
    "\n",
    "        return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "        \n",
    "    finally:        \n",
    "        # Free memory\n",
    "        env.close()\n",
    "        \n",
    "    if nan_encountered: \n",
    "        return float(\"nan\")\n",
    "\n",
    "    return total_reward_test\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=EXPERIMENT_TIMESTEPS // 3)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(optimize_agent_ddpg, n_trials=N_TRIALS, timeout=10800)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ccc60-f8ce-4590-857c-c994997f3584",
   "metadata": {},
   "source": [
    "### Tuning RecurrentPPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cc5aac-bd2a-453b-b52c-0e994a3aebf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_rppo_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for RecurrentPPO hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    trial.using_her_replay_buffer = False\n",
    "    \n",
    "    n_steps_list = []\n",
    "    n_step = round(EXPERIMENT_TIMESTEPS/3)\n",
    "\n",
    "    while n_step > 1:\n",
    "        n_steps_list.append(n_step)\n",
    "        n_step = round(n_step/3)\n",
    "        \n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    #n_steps = trial.suggest_categorical(\"n_steps\", n_steps_list)\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [2,3,4,5,6,7,8,9,10])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 200])\n",
    "    n_epochs = trial.suggest_categorical(\"n_epochs\", [1, 5, 10, 20])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.7, 0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n",
    "    clip_range = trial.suggest_categorical(\"clip_range\", [0.1, 0.2, 0.3, 0.4])\n",
    "    normalize_advantage = trial.suggest_categorical(\"normalize_advantage\", [False, True])\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0, 1)\n",
    "    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5])\n",
    "    target_kl = trial.suggest_categorical(\"target_kl\", [0.1, 0.05, 0.03, 0.02, 0.01, 0.005, 0.001])\n",
    "    \n",
    "    # ------- policy_kwargs --------\n",
    "    lr_schedule = trial.suggest_categorical(\"lr_schedule\", [\"linear\", \"constant\"])\n",
    "    use_sde = trial.suggest_categorical(\"use_sde\", [False, True])\n",
    "    if use_sde is True:\n",
    "        sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 0, 1, 2, 3])\n",
    "        log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "    full_std = trial.suggest_categorical(\"full_std\", [False, True])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    ortho_init = trial.suggest_categorical(\"ortho_init\", [False, True])\n",
    "\n",
    "    # NOTE: Add \"verybig\" to net_arch when tuning HER\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    # ------- policy_kwargs --------\n",
    "\n",
    "    if lr_schedule == \"linear\":\n",
    "        learning_rate = linear_schedule(learning_rate)\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [dict(pi=[64, 64], vf=[64, 64])],\n",
    "        \"medium\": [dict(pi=[256, 256], vf=[256, 256])],\n",
    "        \"big\": [dict(pi=[400, 400], vf=[400, 400])],\n",
    "    }[net_arch]\n",
    "    \n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "    \n",
    "    hyperparams = {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"n_steps\": n_steps,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"clip_range\": clip_range,\n",
    "        \"normalize_advantage\": normalize_advantage,\n",
    "        \"ent_coef\": ent_coef,\n",
    "        \"vf_coef\": vf_coef,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"target_kl\": target_kl,\n",
    "        \"policy_kwargs\": dict(\n",
    "            net_arch=net_arch,\n",
    "            full_std=full_std,\n",
    "            activation_fn=activation_fn,\n",
    "            ortho_init=ortho_init,\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "     \n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "        \n",
    "    if use_sde is True:\n",
    "        hyperparams[\"sde_sample_freq\"] = sde_sample_freq\n",
    "        hyperparams[\"policy_kwargs\"][\"log_std_init\"] = log_std_init\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "def optimize_agent_rppo(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try: \n",
    "        model_params = sample_rppo_params(trial)\n",
    "\n",
    "        # init tracking experiment.\n",
    "        # hyper-parameters, trial id are stored.\n",
    "        config = dict(trial.params)\n",
    "        config[\"trial.number\"] = trial.number\n",
    "        wandb.init(\n",
    "            project=\"RL-optuna\",\n",
    "            entity=\"jlu237\", \n",
    "            sync_tensorboard=True,\n",
    "            config=config,\n",
    "            tags=[\"R_PPO\"]+EXPERIMENT_TAGS,\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "        env = make('VPPBiddingEnv-TUNING-v1')\n",
    "        env = Monitor(env) \n",
    "        env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "        model = RecurrentPPO('MultiInputLstmPolicy', env, verbose=0, seed = 1, **model_params)\n",
    "        print(model_params)\n",
    "    \n",
    "        # -------------- TRAINING -----------------\n",
    "        model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                    log_interval=1,\n",
    "                    progress_bar = True,\n",
    "                    callback=WandbCallback(\n",
    "                        gradient_save_freq=1,\n",
    "                        verbose=0))\n",
    "        \n",
    "        # -------------- EVALUATION -----------------\n",
    "        eval_env = make('VPPBiddingEnv-TUNING-EVAL-v1')\n",
    "        eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "        episodes = 140\n",
    "        for i_episode in range(episodes):\n",
    "            observation = eval_env.reset()\n",
    "            lstm_states = None\n",
    "            num_envs = 1\n",
    "            # Episode start signals are used to reset the lstm states\n",
    "            episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "            for t in range(1):\n",
    "                eval_env.render()\n",
    "                #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "                action, lstm_states = model.predict(observation, state=lstm_states, episode_start=episode_starts, deterministic=True)\n",
    "                observation, reward, dones, info = eval_env.step(action)\n",
    "                episode_starts = dones\n",
    "                if dones:\n",
    "                    break\n",
    "        \n",
    "        total_reward_test = info[\"total_reward\"]\n",
    "        total_profit_test = info[\"total_profit\"]\n",
    "\n",
    "        mean_episode_reward_test = info[\"total_reward\"] / episodes\n",
    "        mean_episode_profit_test = info[\"total_profit\"] / episodes\n",
    "\n",
    "        print(\"Total Reward on Test Set: \" + str(total_reward_test))\n",
    "        print(\"Total Profit on Test Set: \" + str(total_profit_test))\n",
    "        print(\"Mean Episode Reward: \" + str(mean_episode_reward_test))\n",
    "        print(\"Mean Episode Profit: \" + str(mean_episode_profit_test))\n",
    "\n",
    "        wandb.log({\"total_reward_test\": total_reward_test, \n",
    "                   \"total_profit_test\": total_profit_test, \n",
    "                   \"mean_episode_reward_test\": mean_episode_reward_test,\n",
    "                   \"mean_episode_profit_test\": mean_episode_profit_test,\n",
    "                })\n",
    "        wandb.finish()\n",
    "        eval_env.close()\n",
    "\n",
    "        return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "        \n",
    "    finally:        \n",
    "        # Free memory\n",
    "        env.close()\n",
    "        \n",
    "    if nan_encountered: \n",
    "        return float(\"nan\")\n",
    "\n",
    "    return total_reward_test\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=EXPERIMENT_TIMESTEPS // 3)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(optimize_agent_rppo, n_trials=N_TRIALS, timeout=10800)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ae0ad-0b1f-43e3-b690-9e31348d4558",
   "metadata": {},
   "source": [
    "### Tuning TRPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1f24dc-25b2-44de-afe5-9c3a6ae9a910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_trpo_params(trial):\n",
    "    \"\"\"\n",
    "    Sampler for TRPO hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    trial.using_her_replay_buffer = False\n",
    "    \n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [2,3,4,5,6,7,8,9,10])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 100, 128, 200])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    cg_max_steps = trial.suggest_categorical(\"cg_max_steps\", [5, 10, 15, 20, 25, 30])\n",
    "    cg_damping = trial.suggest_categorical(\"cg_damping\", [0.5, 0.2, 0.1, 0.05, 0.01])\n",
    "    line_search_shrinking_factor = trial.suggest_categorical(\"line_search_shrinking_factor\", [0.6, 0.7, 0.8, 0.9])\n",
    "    line_search_max_iter = trial.suggest_categorical(\"line_search_max_iter\", [1, 5, 10, 15, 20])\n",
    "    n_critic_updates = trial.suggest_categorical(\"n_critic_updates\", [1, 5, 10, 20, 25])\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n",
    "    normalize_advantage = trial.suggest_categorical(\"normalize_advantage\", [False, True])\n",
    "    use_sde = trial.suggest_categorical(\"use_sde\", [False, True])\n",
    "    if use_sde is True:\n",
    "        sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 0, 1, 2, 3])    \n",
    "    target_kl = trial.suggest_categorical(\"target_kl\", [0.1, 0.05, 0.03, 0.02, 0.01, 0.005, 0.001])\n",
    "    \n",
    "    # ------- policy_kwargs --------\n",
    "    if use_sde is True:\n",
    "        log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "        full_std = trial.suggest_categorical('full_std', [False, True])\n",
    "    sde_net_arch = trial.suggest_categorical(\"sde_net_arch\", [None, \"tiny\", \"small\"])\n",
    "    lr_schedule = trial.suggest_categorical(\"lr_schedule\", [\"linear\", \"constant\"])\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\", \"big\"])\n",
    "    ortho_init = trial.suggest_categorical('ortho_init', [False, True])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "\n",
    "    if batch_size > n_steps:\n",
    "        batch_size = n_steps\n",
    "\n",
    "    if lr_schedule == \"linear\":\n",
    "        learning_rate = linear_schedule(learning_rate)\n",
    "\n",
    "    net_arch = {\n",
    "        \"small\": [dict(pi=[64, 64], vf=[64, 64])],\n",
    "        \"medium\": [dict(pi=[256, 256], vf=[256, 256])],\n",
    "        \"big\": [dict(pi=[400, 400], vf=[400, 400])],\n",
    "    }[net_arch]\n",
    "    \n",
    "    sde_net_arch = {\n",
    "         None: None,\n",
    "         \"tiny\": [64],\n",
    "         \"small\": [64, 64],\n",
    "    }[sde_net_arch]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "\n",
    "    hyperparams = {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_steps\": n_steps,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"gamma\": gamma,\n",
    "            \"cg_max_steps\": cg_max_steps,\n",
    "            \"cg_damping\": cg_damping,\n",
    "            \"line_search_shrinking_factor\": line_search_shrinking_factor,\n",
    "            \"line_search_max_iter\": line_search_max_iter,\n",
    "            \"n_critic_updates\": n_critic_updates,\n",
    "            \"gae_lambda\": gae_lambda,\n",
    "            \"normalize_advantage\": normalize_advantage,\n",
    "            \"use_sde\": use_sde,\n",
    "            \"target_kl\": target_kl,\n",
    "            \"policy_kwargs\": dict(\n",
    "                net_arch=net_arch,\n",
    "                ortho_init=ortho_init,\n",
    "                activation_fn=activation_fn,\n",
    "            ),\n",
    "        }\n",
    "    \n",
    "     \n",
    "    if trial.using_her_replay_buffer:\n",
    "        hyperparams = sample_her_params(trial, hyperparams)\n",
    "        \n",
    "    if use_sde is True:\n",
    "        hyperparams[\"sde_sample_freq\"] = sde_sample_freq\n",
    "        hyperparams[\"policy_kwargs\"][\"log_std_init\"] = log_std_init\n",
    "        hyperparams[\"policy_kwargs\"][\"full_std\"] = full_std\n",
    "        hyperparams[\"policy_kwargs\"][\"sde_net_arch\"] = sde_net_arch       \n",
    "        \n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "def optimize_agent_trpo(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "    \n",
    "    nan_encountered = False\n",
    "    try: \n",
    "        model_params = sample_trpo_params(trial)\n",
    "\n",
    "        # init tracking experiment.\n",
    "        # hyper-parameters, trial id are stored.\n",
    "        config = dict(trial.params)\n",
    "        config[\"trial.number\"] = trial.number\n",
    "        wandb.init(\n",
    "            project=\"RL-optuna\",\n",
    "            entity=\"jlu237\", \n",
    "            sync_tensorboard=True,\n",
    "            config=config,\n",
    "            tags=[\"TRPO\"] + EXPERIMENT_TAGS,\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "        env = make('VPPBiddingEnv-TUNING-v1')\n",
    "        env = Monitor(env) \n",
    "        env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "        model = TRPO('MultiInputPolicy', env, verbose=0,  seed = 1, **model_params)\n",
    "        print(model_params)\n",
    "    \n",
    "        # -------------- TRAINING -----------------\n",
    "        model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                    log_interval=1,\n",
    "                    progress_bar = True,\n",
    "                    callback=WandbCallback(\n",
    "                        gradient_save_freq=1,\n",
    "                        verbose=0))\n",
    "        \n",
    "        # -------------- EVALUATION -----------------\n",
    "        eval_env = make('VPPBiddingEnv-TUNING-EVAL-v1')\n",
    "        eval_env = RecordEpisodeStatistics(eval_env) # record stats such as returns\n",
    "        episodes = 140\n",
    "        for i_episode in range(episodes):\n",
    "            observation = eval_env.reset()\n",
    "            for t in range(1):\n",
    "                eval_env.render()\n",
    "                #logging.debug(\"observation : \" + str(observation), extra={'log_step': str(i_episode), 'slot': 'test'})\n",
    "                action, _states = model.predict(observation, deterministic = True)\n",
    "                observation, reward, done, info = eval_env.step(action)\n",
    "                if done:\n",
    "                    break\n",
    "        total_reward_test = info[\"total_reward\"]\n",
    "        total_profit_test = info[\"total_profit\"]\n",
    "\n",
    "        mean_episode_reward_test = info[\"total_reward\"] / episodes\n",
    "        mean_episode_profit_test = info[\"total_profit\"] / episodes\n",
    "\n",
    "        print(\"Total Reward on Test Set: \" + str(total_reward_test))\n",
    "        print(\"Total Profit on Test Set: \" + str(total_profit_test))\n",
    "        print(\"Mean Episode Reward: \" + str(mean_episode_reward_test))\n",
    "        print(\"Mean Episode Profit: \" + str(mean_episode_profit_test))\n",
    "\n",
    "        wandb.log({\"total_reward_test\": total_reward_test, \n",
    "                   \"total_profit_test\": total_profit_test, \n",
    "                   \"mean_episode_reward_test\": mean_episode_reward_test,\n",
    "                   \"mean_episode_profit_test\": mean_episode_profit_test,\n",
    "                })\n",
    "        wandb.finish()\n",
    "        eval_env.close()\n",
    "\n",
    "        return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "        \n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "        \n",
    "    finally:        \n",
    "        # Free memory\n",
    "        env.close()\n",
    "        \n",
    "    if nan_encountered: \n",
    "        return float(\"nan\")\n",
    "\n",
    "    return total_reward_test\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=EXPERIMENT_TIMESTEPS // 3)\n",
    "\n",
    "#study = optuna.create_study()\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(optimize_agent_trpo, n_trials=N_TRIALS, timeout=10800)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16756f87-57f3-4b4d-9c68-bb2c39f7ff12",
   "metadata": {},
   "source": [
    "### Tuning PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c060a4-89a7-460e-83bb-f6aa32a830a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for PPO hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    trial.using_her_replay_buffer = False\n",
    "    \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128, 256, 512])\n",
    "    n_steps = trial.suggest_categorical(\"n_steps\", [8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "    gamma = trial.suggest_categorical(\"gamma\", [0.9, 0.95, 0.98, 0.99, 0.995, 0.999, 0.9999])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
    "    lr_schedule = \"constant\"\n",
    "    # Uncomment to enable learning rate schedule\n",
    "    # lr_schedule = trial.suggest_categorical('lr_schedule', ['linear', 'constant'])\n",
    "    ent_coef = trial.suggest_loguniform(\"ent_coef\", 0.00000001, 0.1)\n",
    "    clip_range = trial.suggest_categorical(\"clip_range\", [0.1, 0.2, 0.3, 0.4])\n",
    "    n_epochs = trial.suggest_categorical(\"n_epochs\", [1, 5, 10, 20])\n",
    "    gae_lambda = trial.suggest_categorical(\"gae_lambda\", [0.8, 0.9, 0.92, 0.95, 0.98, 0.99, 1.0])\n",
    "    max_grad_norm = trial.suggest_categorical(\"max_grad_norm\", [0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 5])\n",
    "    vf_coef = trial.suggest_uniform(\"vf_coef\", 0, 1)\n",
    "    net_arch = trial.suggest_categorical(\"net_arch\", [\"small\", \"medium\"])\n",
    "    # Uncomment for gSDE (continuous actions)\n",
    "    # log_std_init = trial.suggest_uniform(\"log_std_init\", -4, 1)\n",
    "    # Uncomment for gSDE (continuous action)\n",
    "    # sde_sample_freq = trial.suggest_categorical(\"sde_sample_freq\", [-1, 8, 16, 32, 64, 128, 256])\n",
    "    # Orthogonal initialization\n",
    "    ortho_init = False\n",
    "    # ortho_init = trial.suggest_categorical('ortho_init', [False, True])\n",
    "    # activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu', 'elu', 'leaky_relu'])\n",
    "    activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n",
    "\n",
    "    # TODO: account when using multiple envs\n",
    "    if batch_size > n_steps:\n",
    "        batch_size = n_steps\n",
    "\n",
    "    if lr_schedule == \"linear\":\n",
    "        learning_rate = linear_schedule(learning_rate)\n",
    "\n",
    "    # Independent networks usually work best\n",
    "    # when not working with images\n",
    "    net_arch = {\n",
    "        \"small\": [dict(pi=[64, 64], vf=[64, 64])],\n",
    "        \"medium\": [dict(pi=[256, 256], vf=[256, 256])],\n",
    "    }[net_arch]\n",
    "\n",
    "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[activation_fn]\n",
    "\n",
    "    hyperparams = {\n",
    "            \"n_steps\": n_steps,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"gamma\": gamma,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"ent_coef\": ent_coef,\n",
    "            \"clip_range\": clip_range,\n",
    "            \"n_epochs\": n_epochs,\n",
    "            \"gae_lambda\": gae_lambda,\n",
    "            \"max_grad_norm\": max_grad_norm,\n",
    "            \"vf_coef\": vf_coef,\n",
    "            # \"sde_sample_freq\": sde_sample_freq,\n",
    "            \"policy_kwargs\": dict(\n",
    "                # log_std_init=log_std_init,\n",
    "                net_arch=net_arch,\n",
    "                activation_fn=activation_fn,\n",
    "                ortho_init=ortho_init,\n",
    "            ),\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "def optimize_agent_ppo(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    model_params = sample_ppo_params(trial)\n",
    "    \n",
    "    # init tracking experiment.\n",
    "    # hyper-parameters, trial id are stored.\n",
    "    config = dict(trial.params)\n",
    "    config[\"trial.number\"] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"RL-optuna\",\n",
    "        entity=\"jlu237\", \n",
    "        sync_tensorboard=True,\n",
    "        config=config,\n",
    "        tags=[\"PPO\"] + EXPERIMENT_TAGS,\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "    \n",
    "    \n",
    "    model = PPO('MultiInputPolicy', env, verbose=0,  seed = 1, **model_params)\n",
    "\n",
    "    model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                log_interval=1,\n",
    "                progress_bar = True,\n",
    "                callback=WandbCallback(\n",
    "                    gradient_save_freq=1,\n",
    "                    verbose=0))\n",
    "\n",
    "    wandb.finish()\n",
    "    return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "    print(return_code)\n",
    "    \n",
    "study = optuna.create_study()\n",
    "try:\n",
    "    study.optimize(optimize_agent_ppo, n_trials=N_TRIALS)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c3f6b2-d483-4e90-bd69-fdaf3c3ce643",
   "metadata": {},
   "source": [
    "### Tuning TQC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f448ef-015b-477c-852a-f50ad3ca11fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tqc_params(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Sampler for TQC hyperparams.\n",
    "    :param trial:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TQC is SAC + Distributional RL\n",
    "    hyperparams = sample_sac_params(trial)\n",
    "\n",
    "    n_quantiles = trial.suggest_int(\"n_quantiles\", 5, 50)\n",
    "    top_quantiles_to_drop_per_net = trial.suggest_int(\"top_quantiles_to_drop_per_net\", 0, n_quantiles - 1)\n",
    "\n",
    "    hyperparams[\"policy_kwargs\"].update({\"n_quantiles\": n_quantiles})\n",
    "    hyperparams[\"top_quantiles_to_drop_per_net\"] = top_quantiles_to_drop_per_net\n",
    "\n",
    "    return hyperparams\n",
    "\n",
    "\n",
    "\n",
    "def optimize_agent_tqc(trial):\n",
    "    \"\"\" Train the model and optimize\n",
    "        Optuna maximises the negative log likelihood, so we\n",
    "        need to negate the reward here\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    model_params = sample_tqc_params(trial)\n",
    "    \n",
    "    # init tracking experiment.\n",
    "    # hyper-parameters, trial id are stored.\n",
    "    config = dict(trial.params)\n",
    "    config[\"trial.number\"] = trial.number\n",
    "    wandb.init(\n",
    "        project=\"RL-optuna\",\n",
    "        entity=\"jlu237\", \n",
    "        sync_tensorboard=True,\n",
    "        config=config,\n",
    "        tags=[\"TQC\"] + EXPERIMENT_TAGS,\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    env = make('VPPBiddingEnv-TRAIN-v1', render_mode=\"human\")\n",
    "    env = Monitor(env) \n",
    "    env = RecordEpisodeStatistics(env) # record stats such as returns\n",
    "    \n",
    "    \n",
    "    model = DDPG('MultiInputPolicy', env, verbose=0,  seed = 1, **model_params)\n",
    "\n",
    "    model.learn(total_timesteps=EXPERIMENT_TIMESTEPS,\n",
    "                log_interval=1,\n",
    "                progress_bar = True,\n",
    "                callback=WandbCallback(\n",
    "                    gradient_save_freq=1,\n",
    "                    verbose=0))\n",
    "\n",
    "    wandb.finish()\n",
    "    return_code = subprocess.run(\"wandb sync wandb/latest-run\", shell=True)\n",
    "    print(return_code)\n",
    "    \n",
    "study = optuna.create_study()\n",
    "try:\n",
    "    study.optimize(optimize_agent_tqc, n_trials=N_TRIALS)\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by keyboard.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd771f-2dd6-4986-8d72-fab16886c795",
   "metadata": {},
   "source": [
    "### ARS  ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ba520-1bee-45a3-a7d6-0db853d44c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "slot_settlement_prices_DE : List[np.float32] = []\n",
    "print(slot_settlement_prices_DE)\n",
    "print(type(slot_settlement_prices_DE))\n",
    "\n",
    "slot_settlement_prices_DE = [0.,0.,0.,0.,0.,0.]\n",
    "print(slot_settlement_prices_DE)\n",
    "print(type(slot_settlement_prices_DE))\n",
    "\n",
    "slot_settlement_prices_DE = np.array(slot_settlement_prices_DE, dtype=np.float32)\n",
    "print(slot_settlement_prices_DE)\n",
    "print(type(slot_settlement_prices_DE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b0931-6738-4026-ad5a-1cd09a723192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "maximum_possible_VPP_capacity = 131.32\n",
    "mean: float = 0. # symmetrical normal distribution at 0 \n",
    "sd: float = maximum_possible_VPP_capacity/7\n",
    "max_at_10_percent: float = norm.pdf(maximum_possible_VPP_capacity*0.1,mean,sd)\n",
    "\n",
    "max_at_10_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fac75-4dee-4b9d-a1ab-68f4ab15d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_false: List[int] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0ddc8-a438-452c-9ccf-2c08aa52a8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
